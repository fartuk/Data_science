{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fao3864/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn import manifold, decomposition, linear_model, ensemble, neighbors, cross_validation\n",
    "import sklearn\n",
    "from sklearn import pipeline, preprocessing, feature_extraction\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, auc, roc_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgbm\n",
    "import scipy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"data/train.csv\")\n",
    "X_test = pd.read_csv(\"data/test_with_ids.csv\")\n",
    "\n",
    "y_train = pd.DataFrame(X_train['is_duplicate'])\n",
    "y_train.columns = ['y']\n",
    "del X_train['is_duplicate']\n",
    "\n",
    "skf = StratifiedKFold(5, shuffle=True, random_state=0)\n",
    "folds = []\n",
    "for itr, ite in skf.split(X_train, y_train.y):\n",
    "    folds += [[itr, ite]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"data/train_inter.csv\")\n",
    "\n",
    "X_train['question1'] = X_train['question1'].apply(lambda x: str(x))\n",
    "X_train['question2'] = X_train['question2'].apply(lambda x: str(x))\n",
    "\n",
    "X_train['question1'] = X_train['question1'].apply(lambda x: x.lower().split())\n",
    "X_train['question2'] = X_train['question2'].apply(lambda x: x.lower().split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"data/test_inter.csv\")\n",
    "\n",
    "X_test['question1'] = X_test['question1'].apply(lambda x: str(x))\n",
    "X_test['question2'] = X_test['question2'].apply(lambda x: str(x))\n",
    "\n",
    "X_test['question1'] = X_test['question1'].apply(lambda x: x.lower().split())\n",
    "X_test['question2'] = X_test['question2'].apply(lambda x: x.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = map(lambda x: ' '.join(x), X_train['question1'])\n",
    "X1 = map(lambda x: ' '.join(x), X_train['question2'])\n",
    "Y = map(lambda x: ' '.join(x), X_test['question1'])\n",
    "Y1 = map(lambda x: ' '.join(x), X_test['question2'])\n",
    "hw = HashingVectorizer(n_features=200).fit(X_train['question1'] + X_train['question2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(hw.transform(X).todense())\n",
    "X1 = pd.DataFrame(hw.transform(X1).todense())\n",
    "Y = pd.DataFrame(hw.transform(Y).todense())\n",
    "Y1 = pd.DataFrame(hw.transform(Y1).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = feature_extraction.text.TfidfTransformer()\n",
    "tfidf.fit(X+X1)\n",
    "\n",
    "X = pd.DataFrame(tfidf.transform(X).todense())\n",
    "X1 = pd.DataFrame(tfidf.transform(X1).todense())\n",
    "Y = pd.DataFrame(tfidf.transform(Y).todense())\n",
    "Y1 = pd.DataFrame(tfidf.transform(Y1).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train, X], axis=1, join_axes=[X_train.index])\n",
    "del X\n",
    "X_train = pd.concat([X_train, X1], axis=1, join_axes=[X_train.index])\n",
    "del X1\n",
    "X_test = pd.concat([X_test, Y], axis=1, join_axes=[X_test.index])\n",
    "del Y\n",
    "X_test = pd.concat([X_test, Y1], axis=1, join_axes=[X_test.index])\n",
    "del Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cores_dict = pd.read_csv(\"data/question_max_kcores.csv\", index_col=\"qid\").to_dict()[\"max_kcore\"]\n",
    "def gen_qid1_max_kcore(row):\n",
    "    return cores_dict[row[\"qid1\"]]\n",
    "def gen_qid2_max_kcore(row):\n",
    "    return cores_dict[row[\"qid2\"]]\n",
    "\n",
    "#def gen_max_kcore(row):\n",
    "#    return max(row[\"qid1_max_kcore\"], row[\"qid2_max_kcore\"])\n",
    "\n",
    "X_train[\"qid1_max_kcore\"] = X_train.apply(gen_qid1_max_kcore, axis=1)\n",
    "#X_test[\"qid1_max_kcore\"] = X_test.apply(gen_qid1_max_kcore, axis=1)\n",
    "X_train[\"qid2_max_kcore\"] = X_train.apply(gen_qid2_max_kcore, axis=1)\n",
    "#X_test[\"qid2_max_kcore\"] = X_test.apply(gen_qid2_max_kcore, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_train = pd.read_csv(\"data/kcore_test.csv\")\n",
    "X_test = pd.concat([X_test, q_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>word_match</th>\n",
       "      <th>tfidf_word_match</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>qid1_max_kcore</th>\n",
       "      <th>qid2_max_kcore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[what, is, the, step, by, step, guide, to, inv...</td>\n",
       "      <td>[what, is, the, step, by, step, guide, to, inv...</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.772164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[what, is, the, story, of, kohinoor, (koh-i-no...</td>\n",
       "      <td>[what, would, happen, if, the, indian, governm...</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.361758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 409 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  [what, is, the, step, by, step, guide, to, inv...   \n",
       "1   1     3     4  [what, is, the, story, of, kohinoor, (koh-i-no...   \n",
       "\n",
       "                                           question2  word_match  \\\n",
       "0  [what, is, the, step, by, step, guide, to, inv...    0.727273   \n",
       "1  [what, would, happen, if, the, indian, governm...    0.307692   \n",
       "\n",
       "   tfidf_word_match    0    1    2       ...             192  193  194  195  \\\n",
       "0          0.772164  0.0  0.0  0.0       ...        0.325385  0.0  0.0  0.0   \n",
       "1          0.361758  0.0  0.0  0.0       ...        0.000000  0.0  0.0  0.0   \n",
       "\n",
       "   196  197  198  199  qid1_max_kcore  qid2_max_kcore  \n",
       "0  0.0  0.0  0.0  0.0               0               0  \n",
       "1  0.0  0.0  0.0  0.0               0               0  \n",
       "\n",
       "[2 rows x 409 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del X_train['question1']\n",
    "del X_train['question2']\n",
    "del X_train['qid1']\n",
    "del X_train['qid2']\n",
    "del X_train['id']\n",
    "\n",
    "del X_test['question1']\n",
    "del X_test['question2']\n",
    "del X_test['test_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train = pd.read_csv(\"data/train_inter.csv\")\n",
    "q_train = pd.read_csv(\"data/freq_train.csv\")\n",
    "X_train = pd.concat([X_train, q_train], axis=1)\n",
    "\n",
    "f_train = pd.read_csv(\"data/train_features.csv\", encoding = \"ISO-8859-1\")\n",
    "f_train = f_train.ix[:, 2:]\n",
    "X_train = pd.concat([X_train, f_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_train = pd.read_csv(\"data/f1_train.csv\")\n",
    "X_train = pd.concat([X_train, q_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_train = pd.read_csv(\"data/f2_train.csv\")\n",
    "X_train = pd.concat([X_train, q_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_train = pd.read_csv(\"data/train_lda_feat.csv\", encoding = \"ISO-8859-1\")\n",
    "q_train = q_train.ix[:, 6:]\n",
    "X_train = pd.concat([X_train, q_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_match</th>\n",
       "      <th>tfidf_word_match</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>kur_q1vec</th>\n",
       "      <th>kur_q2vec</th>\n",
       "      <th>q1_q2_intersect</th>\n",
       "      <th>cosine_distance_lda</th>\n",
       "      <th>cityblock_distance_lda</th>\n",
       "      <th>jaccard_distance_lda</th>\n",
       "      <th>canberra_distance_lda</th>\n",
       "      <th>euclidean_distance_lda</th>\n",
       "      <th>minkowski_distance_lda</th>\n",
       "      <th>braycurtis_distance_lda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.772164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050416</td>\n",
       "      <td>0.337301</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046653</td>\n",
       "      <td>0.232798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>0.13759</td>\n",
       "      <td>0.127564</td>\n",
       "      <td>0.13409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 442 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_match  tfidf_word_match    0    1    2    3    4    5    6    7  \\\n",
       "0    0.727273          0.772164  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "            ...             kur_q1vec  kur_q2vec  q1_q2_intersect  \\\n",
       "0           ...              0.050416   0.337301                0   \n",
       "\n",
       "   cosine_distance_lda  cityblock_distance_lda  jaccard_distance_lda  \\\n",
       "0             0.046653                0.232798                   1.0   \n",
       "\n",
       "   canberra_distance_lda  euclidean_distance_lda  minkowski_distance_lda  \\\n",
       "0               1.266667                 0.13759                0.127564   \n",
       "\n",
       "   braycurtis_distance_lda  \n",
       "0                  0.13409  \n",
       "\n",
       "[1 rows x 442 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-16217c75fe8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mq_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'q_test' is not defined"
     ]
    }
   ],
   "source": [
    "del q_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_test = pd.read_csv(\"data/freq_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_test, q_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_test = pd.read_csv(\"data/test_features.csv\", encoding = \"ISO-8859-1\")\n",
    "f_test = f_test.ix[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_test, f_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_test = pd.read_csv(\"data/f1_test.csv\")\n",
    "X_test = pd.concat([X_test, q_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_test = pd.read_csv(\"data/f2_test.csv\")\n",
    "X_test = pd.concat([X_test, q_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_test = pd.read_csv(\"data/test_lda_feat.csv\", encoding = \"ISO-8859-1\")\n",
    "q_test = q_test.ix[:, 3:]\n",
    "X_test = pd.concat([X_test, q_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_match</th>\n",
       "      <th>tfidf_word_match</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>kur_q1vec</th>\n",
       "      <th>kur_q2vec</th>\n",
       "      <th>q1_q2_intersect</th>\n",
       "      <th>cosine_distance_lda</th>\n",
       "      <th>cityblock_distance_lda</th>\n",
       "      <th>jaccard_distance_lda</th>\n",
       "      <th>canberra_distance_lda</th>\n",
       "      <th>euclidean_distance_lda</th>\n",
       "      <th>minkowski_distance_lda</th>\n",
       "      <th>braycurtis_distance_lda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.772164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050416</td>\n",
       "      <td>0.337301</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046653</td>\n",
       "      <td>0.232798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>0.13759</td>\n",
       "      <td>0.127564</td>\n",
       "      <td>0.13409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 442 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_match  tfidf_word_match    0    1    2    3    4    5    6    7  \\\n",
       "0    0.727273          0.772164  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "            ...             kur_q1vec  kur_q2vec  q1_q2_intersect  \\\n",
       "0           ...              0.050416   0.337301                0   \n",
       "\n",
       "   cosine_distance_lda  cityblock_distance_lda  jaccard_distance_lda  \\\n",
       "0             0.046653                0.232798                   1.0   \n",
       "\n",
       "   canberra_distance_lda  euclidean_distance_lda  minkowski_distance_lda  \\\n",
       "0               1.266667                 0.13759                0.127564   \n",
       "\n",
       "   braycurtis_distance_lda  \n",
       "0                  0.13409  \n",
       "\n",
       "[1 rows x 442 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_match</th>\n",
       "      <th>tfidf_word_match</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>kur_q1vec</th>\n",
       "      <th>kur_q2vec</th>\n",
       "      <th>q1_q2_intersect</th>\n",
       "      <th>cosine_distance_lda</th>\n",
       "      <th>cityblock_distance_lda</th>\n",
       "      <th>jaccard_distance_lda</th>\n",
       "      <th>canberra_distance_lda</th>\n",
       "      <th>euclidean_distance_lda</th>\n",
       "      <th>minkowski_distance_lda</th>\n",
       "      <th>braycurtis_distance_lda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.274019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.156312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039938</td>\n",
       "      <td>-0.144866</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573341</td>\n",
       "      <td>1.16019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.833784</td>\n",
       "      <td>0.399663</td>\n",
       "      <td>0.290631</td>\n",
       "      <td>0.65838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 442 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_match  tfidf_word_match    0    1    2    3    4         5    6    7  \\\n",
       "0    0.266667          0.274019  0.0  0.0  0.0  0.0  0.0 -0.156312  0.0  0.0   \n",
       "\n",
       "            ...             kur_q1vec  kur_q2vec  q1_q2_intersect  \\\n",
       "0           ...              0.039938  -0.144866                0   \n",
       "\n",
       "   cosine_distance_lda  cityblock_distance_lda  jaccard_distance_lda  \\\n",
       "0             0.573341                 1.16019                   1.0   \n",
       "\n",
       "   canberra_distance_lda  euclidean_distance_lda  minkowski_distance_lda  \\\n",
       "0               7.833784                0.399663                0.290631   \n",
       "\n",
       "   braycurtis_distance_lda  \n",
       "0                  0.65838  \n",
       "\n",
       "[1 rows x 442 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.633353\teval-logloss:0.633926\n",
      "[15]\ttrain-logloss:0.321747\teval-logloss:0.325977\n",
      "[30]\ttrain-logloss:0.268889\teval-logloss:0.274951\n",
      "[45]\ttrain-logloss:0.25428\teval-logloss:0.261443\n",
      "[60]\ttrain-logloss:0.245561\teval-logloss:0.254675\n",
      "[75]\ttrain-logloss:0.238781\teval-logloss:0.24994\n",
      "[90]\ttrain-logloss:0.234125\teval-logloss:0.247252\n",
      "[105]\ttrain-logloss:0.229668\teval-logloss:0.244957\n",
      "[120]\ttrain-logloss:0.226156\teval-logloss:0.243386\n",
      "[135]\ttrain-logloss:0.223186\teval-logloss:0.242062\n",
      "[150]\ttrain-logloss:0.22066\teval-logloss:0.241184\n",
      "[165]\ttrain-logloss:0.218442\teval-logloss:0.240467\n",
      "[180]\ttrain-logloss:0.216348\teval-logloss:0.239777\n",
      "[195]\ttrain-logloss:0.214042\teval-logloss:0.238993\n",
      "[210]\ttrain-logloss:0.212182\teval-logloss:0.238393\n",
      "[225]\ttrain-logloss:0.209928\teval-logloss:0.237558\n",
      "[240]\ttrain-logloss:0.208525\teval-logloss:0.237265\n",
      "[255]\ttrain-logloss:0.207248\teval-logloss:0.237003\n",
      "[270]\ttrain-logloss:0.205663\teval-logloss:0.236472\n",
      "[285]\ttrain-logloss:0.204185\teval-logloss:0.236171\n",
      "[300]\ttrain-logloss:0.20309\teval-logloss:0.23596\n",
      "[315]\ttrain-logloss:0.201931\teval-logloss:0.235731\n",
      "[330]\ttrain-logloss:0.200763\teval-logloss:0.235602\n",
      "[345]\ttrain-logloss:0.199685\teval-logloss:0.235389\n",
      "[360]\ttrain-logloss:0.198615\teval-logloss:0.235255\n",
      "[375]\ttrain-logloss:0.197712\teval-logloss:0.235097\n",
      "[390]\ttrain-logloss:0.196114\teval-logloss:0.234649\n",
      "[405]\ttrain-logloss:0.194953\teval-logloss:0.234496\n",
      "[420]\ttrain-logloss:0.193948\teval-logloss:0.234401\n",
      "[435]\ttrain-logloss:0.192828\teval-logloss:0.23418\n",
      "[450]\ttrain-logloss:0.191902\teval-logloss:0.234045\n",
      "[465]\ttrain-logloss:0.191027\teval-logloss:0.233962\n",
      "[480]\ttrain-logloss:0.190154\teval-logloss:0.233816\n",
      "[495]\ttrain-logloss:0.189316\teval-logloss:0.233714\n",
      "[510]\ttrain-logloss:0.188494\teval-logloss:0.233564\n",
      "[525]\ttrain-logloss:0.187255\teval-logloss:0.233317\n",
      "[540]\ttrain-logloss:0.186286\teval-logloss:0.233152\n",
      "[555]\ttrain-logloss:0.185444\teval-logloss:0.23307\n",
      "[570]\ttrain-logloss:0.184721\teval-logloss:0.233054\n",
      "[585]\ttrain-logloss:0.18393\teval-logloss:0.232974\n",
      "[600]\ttrain-logloss:0.182097\teval-logloss:0.232534\n",
      "[615]\ttrain-logloss:0.181325\teval-logloss:0.232464\n",
      "[630]\ttrain-logloss:0.180387\teval-logloss:0.232273\n",
      "[645]\ttrain-logloss:0.179786\teval-logloss:0.232283\n",
      "[660]\ttrain-logloss:0.178908\teval-logloss:0.232107\n",
      "[675]\ttrain-logloss:0.178285\teval-logloss:0.232015\n",
      "[690]\ttrain-logloss:0.177697\teval-logloss:0.231972\n",
      "[705]\ttrain-logloss:0.176779\teval-logloss:0.231813\n",
      "[720]\ttrain-logloss:0.176237\teval-logloss:0.231788\n",
      "[735]\ttrain-logloss:0.175566\teval-logloss:0.23178\n",
      "[750]\ttrain-logloss:0.174896\teval-logloss:0.231731\n",
      "[765]\ttrain-logloss:0.173509\teval-logloss:0.231388\n",
      "[780]\ttrain-logloss:0.172826\teval-logloss:0.23136\n",
      "[795]\ttrain-logloss:0.171816\teval-logloss:0.231261\n",
      "[810]\ttrain-logloss:0.171254\teval-logloss:0.231242\n",
      "[825]\ttrain-logloss:0.170546\teval-logloss:0.231174\n",
      "[840]\ttrain-logloss:0.169222\teval-logloss:0.230896\n",
      "[855]\ttrain-logloss:0.168439\teval-logloss:0.230854\n",
      "[870]\ttrain-logloss:0.167496\teval-logloss:0.230696\n",
      "[885]\ttrain-logloss:0.166747\teval-logloss:0.230641\n",
      "[900]\ttrain-logloss:0.166058\teval-logloss:0.230556\n",
      "[915]\ttrain-logloss:0.165421\teval-logloss:0.23055\n",
      "[930]\ttrain-logloss:0.164909\teval-logloss:0.23053\n",
      "[945]\ttrain-logloss:0.164334\teval-logloss:0.230532\n",
      "[960]\ttrain-logloss:0.163887\teval-logloss:0.230505\n",
      "[975]\ttrain-logloss:0.162843\teval-logloss:0.230399\n",
      "[990]\ttrain-logloss:0.162272\teval-logloss:0.230351\n",
      "[1005]\ttrain-logloss:0.161669\teval-logloss:0.230301\n",
      "[1020]\ttrain-logloss:0.16106\teval-logloss:0.230225\n",
      "[1035]\ttrain-logloss:0.160268\teval-logloss:0.230132\n",
      "[1050]\ttrain-logloss:0.1591\teval-logloss:0.229882\n",
      "[1065]\ttrain-logloss:0.158346\teval-logloss:0.2298\n",
      "[1080]\ttrain-logloss:0.157777\teval-logloss:0.229777\n",
      "[1095]\ttrain-logloss:0.157268\teval-logloss:0.229757\n",
      "[1110]\ttrain-logloss:0.156808\teval-logloss:0.229722\n",
      "[1125]\ttrain-logloss:0.15627\teval-logloss:0.229708\n",
      "[1140]\ttrain-logloss:0.155797\teval-logloss:0.229675\n",
      "[1155]\ttrain-logloss:0.155217\teval-logloss:0.229642\n",
      "[1170]\ttrain-logloss:0.15465\teval-logloss:0.229592\n",
      "[1185]\ttrain-logloss:0.154075\teval-logloss:0.229563\n",
      "[1200]\ttrain-logloss:0.153556\teval-logloss:0.229569\n",
      "[1215]\ttrain-logloss:0.152888\teval-logloss:0.229492\n",
      "[1230]\ttrain-logloss:0.152508\teval-logloss:0.229465\n",
      "[1245]\ttrain-logloss:0.152063\teval-logloss:0.229463\n",
      "[1260]\ttrain-logloss:0.150989\teval-logloss:0.229345\n",
      "[1275]\ttrain-logloss:0.150177\teval-logloss:0.22925\n",
      "[1290]\ttrain-logloss:0.149709\teval-logloss:0.229173\n",
      "[1305]\ttrain-logloss:0.149281\teval-logloss:0.229202\n",
      "[1320]\ttrain-logloss:0.148684\teval-logloss:0.229212\n",
      "[1335]\ttrain-logloss:0.148301\teval-logloss:0.229219\n",
      "[1350]\ttrain-logloss:0.147669\teval-logloss:0.229164\n",
      "[1365]\ttrain-logloss:0.146635\teval-logloss:0.229039\n",
      "[1380]\ttrain-logloss:0.146223\teval-logloss:0.229017\n",
      "[1395]\ttrain-logloss:0.145648\teval-logloss:0.228912\n",
      "[1410]\ttrain-logloss:0.145176\teval-logloss:0.228867\n",
      "[1425]\ttrain-logloss:0.144607\teval-logloss:0.228854\n",
      "[1440]\ttrain-logloss:0.144012\teval-logloss:0.228831\n",
      "[1455]\ttrain-logloss:0.143473\teval-logloss:0.22883\n",
      "[1470]\ttrain-logloss:0.142942\teval-logloss:0.228757\n",
      "[1485]\ttrain-logloss:0.141923\teval-logloss:0.228693\n",
      "[1500]\ttrain-logloss:0.141318\teval-logloss:0.228632\n",
      "[1515]\ttrain-logloss:0.140859\teval-logloss:0.22861\n",
      "[1530]\ttrain-logloss:0.140274\teval-logloss:0.228554\n",
      "[1545]\ttrain-logloss:0.139872\teval-logloss:0.228539\n",
      "[1560]\ttrain-logloss:0.139459\teval-logloss:0.228539\n",
      "[1575]\ttrain-logloss:0.138919\teval-logloss:0.228457\n",
      "[1590]\ttrain-logloss:0.138602\teval-logloss:0.228471\n",
      "[1605]\ttrain-logloss:0.137899\teval-logloss:0.228404\n",
      "[1620]\ttrain-logloss:0.137505\teval-logloss:0.228442\n",
      "[1635]\ttrain-logloss:0.136707\teval-logloss:0.228412\n",
      "[1650]\ttrain-logloss:0.136216\teval-logloss:0.22839\n",
      "[1665]\ttrain-logloss:0.135833\teval-logloss:0.228379\n",
      "[1680]\ttrain-logloss:0.135405\teval-logloss:0.228358\n",
      "[1695]\ttrain-logloss:0.135005\teval-logloss:0.228345\n",
      "[1710]\ttrain-logloss:0.134717\teval-logloss:0.228347\n",
      "[1725]\ttrain-logloss:0.13429\teval-logloss:0.228342\n",
      "[1740]\ttrain-logloss:0.13384\teval-logloss:0.228316\n",
      "[1755]\ttrain-logloss:0.13318\teval-logloss:0.228177\n",
      "[1770]\ttrain-logloss:0.132553\teval-logloss:0.22814\n",
      "[1785]\ttrain-logloss:0.132018\teval-logloss:0.228154\n",
      "[1800]\ttrain-logloss:0.131534\teval-logloss:0.228118\n",
      "[1815]\ttrain-logloss:0.131006\teval-logloss:0.228104\n",
      "[1830]\ttrain-logloss:0.130368\teval-logloss:0.228041\n",
      "[1845]\ttrain-logloss:0.129635\teval-logloss:0.227945\n",
      "[1860]\ttrain-logloss:0.129147\teval-logloss:0.227974\n",
      "[1875]\ttrain-logloss:0.128763\teval-logloss:0.227998\n",
      "[1890]\ttrain-logloss:0.128287\teval-logloss:0.228009\n",
      "[1905]\ttrain-logloss:0.127988\teval-logloss:0.228018\n",
      "[1920]\ttrain-logloss:0.127435\teval-logloss:0.228061\n",
      "[1935]\ttrain-logloss:0.12709\teval-logloss:0.228069\n",
      "[1950]\ttrain-logloss:0.126791\teval-logloss:0.228068\n",
      "[1965]\ttrain-logloss:0.126336\teval-logloss:0.228065\n",
      "[1980]\ttrain-logloss:0.126045\teval-logloss:0.228076\n",
      "[1995]\ttrain-logloss:0.125488\teval-logloss:0.228023\n",
      "[0]\ttrain-logloss:0.633412\teval-logloss:0.633731\n",
      "[15]\ttrain-logloss:0.322367\teval-logloss:0.325774\n",
      "[30]\ttrain-logloss:0.269548\teval-logloss:0.275006\n",
      "[45]\ttrain-logloss:0.254609\teval-logloss:0.261659\n",
      "[60]\ttrain-logloss:0.24544\teval-logloss:0.254671\n",
      "[75]\ttrain-logloss:0.238461\teval-logloss:0.249904\n",
      "[90]\ttrain-logloss:0.233653\teval-logloss:0.247333\n",
      "[105]\ttrain-logloss:0.229617\teval-logloss:0.245229\n",
      "[120]\ttrain-logloss:0.225925\teval-logloss:0.243502\n",
      "[135]\ttrain-logloss:0.222724\teval-logloss:0.242192\n",
      "[150]\ttrain-logloss:0.220034\teval-logloss:0.24112\n",
      "[165]\ttrain-logloss:0.216887\teval-logloss:0.239897\n",
      "[180]\ttrain-logloss:0.214755\teval-logloss:0.239258\n",
      "[195]\ttrain-logloss:0.213\teval-logloss:0.238723\n",
      "[210]\ttrain-logloss:0.211243\teval-logloss:0.238191\n",
      "[225]\ttrain-logloss:0.209231\teval-logloss:0.23751\n",
      "[240]\ttrain-logloss:0.207392\teval-logloss:0.237011\n",
      "[255]\ttrain-logloss:0.205839\teval-logloss:0.236645\n",
      "[270]\ttrain-logloss:0.204467\teval-logloss:0.236399\n",
      "[285]\ttrain-logloss:0.203343\teval-logloss:0.236209\n",
      "[300]\ttrain-logloss:0.202096\teval-logloss:0.235936\n",
      "[315]\ttrain-logloss:0.200674\teval-logloss:0.235664\n",
      "[330]\ttrain-logloss:0.199372\teval-logloss:0.235368\n",
      "[345]\ttrain-logloss:0.198248\teval-logloss:0.235205\n",
      "[360]\ttrain-logloss:0.197403\teval-logloss:0.235086\n",
      "[375]\ttrain-logloss:0.195795\teval-logloss:0.234707\n",
      "[390]\ttrain-logloss:0.194584\teval-logloss:0.234522\n",
      "[405]\ttrain-logloss:0.193855\teval-logloss:0.234544\n",
      "[420]\ttrain-logloss:0.192391\teval-logloss:0.234222\n",
      "[435]\ttrain-logloss:0.191346\teval-logloss:0.234061\n",
      "[450]\ttrain-logloss:0.190124\teval-logloss:0.233946\n",
      "[465]\ttrain-logloss:0.189067\teval-logloss:0.23367\n",
      "[480]\ttrain-logloss:0.188411\teval-logloss:0.233635\n",
      "[495]\ttrain-logloss:0.187531\teval-logloss:0.233511\n",
      "[510]\ttrain-logloss:0.186445\teval-logloss:0.233369\n",
      "[525]\ttrain-logloss:0.18568\teval-logloss:0.233267\n",
      "[540]\ttrain-logloss:0.184949\teval-logloss:0.233207\n",
      "[555]\ttrain-logloss:0.184136\teval-logloss:0.233127\n",
      "[570]\ttrain-logloss:0.183139\teval-logloss:0.233007\n",
      "[585]\ttrain-logloss:0.182133\teval-logloss:0.232934\n",
      "[600]\ttrain-logloss:0.180975\teval-logloss:0.232742\n",
      "[615]\ttrain-logloss:0.180363\teval-logloss:0.232724\n",
      "[630]\ttrain-logloss:0.179641\teval-logloss:0.232687\n",
      "[645]\ttrain-logloss:0.178201\teval-logloss:0.232374\n",
      "[660]\ttrain-logloss:0.177501\teval-logloss:0.23234\n",
      "[675]\ttrain-logloss:0.176465\teval-logloss:0.232169\n",
      "[690]\ttrain-logloss:0.17583\teval-logloss:0.232131\n",
      "[705]\ttrain-logloss:0.175038\teval-logloss:0.232031\n",
      "[720]\ttrain-logloss:0.174474\teval-logloss:0.232059\n",
      "[735]\ttrain-logloss:0.173681\teval-logloss:0.231977\n",
      "[750]\ttrain-logloss:0.172596\teval-logloss:0.231732\n",
      "[765]\ttrain-logloss:0.17193\teval-logloss:0.23166\n",
      "[780]\ttrain-logloss:0.171332\teval-logloss:0.231655\n",
      "[795]\ttrain-logloss:0.170632\teval-logloss:0.231572\n",
      "[810]\ttrain-logloss:0.170094\teval-logloss:0.231583\n",
      "[825]\ttrain-logloss:0.169383\teval-logloss:0.231549\n",
      "[840]\ttrain-logloss:0.168953\teval-logloss:0.231564\n",
      "[855]\ttrain-logloss:0.168183\teval-logloss:0.231468\n",
      "[870]\ttrain-logloss:0.167468\teval-logloss:0.231445\n",
      "[885]\ttrain-logloss:0.166539\teval-logloss:0.231312\n",
      "[900]\ttrain-logloss:0.165692\teval-logloss:0.231154\n",
      "[915]\ttrain-logloss:0.16489\teval-logloss:0.231\n",
      "[930]\ttrain-logloss:0.164178\teval-logloss:0.230956\n",
      "[945]\ttrain-logloss:0.162989\teval-logloss:0.230691\n",
      "[960]\ttrain-logloss:0.162326\teval-logloss:0.230661\n",
      "[975]\ttrain-logloss:0.161599\teval-logloss:0.230605\n",
      "[990]\ttrain-logloss:0.161046\teval-logloss:0.230568\n",
      "[1005]\ttrain-logloss:0.16045\teval-logloss:0.230557\n",
      "[1020]\ttrain-logloss:0.159998\teval-logloss:0.230526\n",
      "[1035]\ttrain-logloss:0.159161\teval-logloss:0.230409\n",
      "[1050]\ttrain-logloss:0.158655\teval-logloss:0.230373\n",
      "[1065]\ttrain-logloss:0.157631\teval-logloss:0.230186\n",
      "[1080]\ttrain-logloss:0.15686\teval-logloss:0.230155\n",
      "[1095]\ttrain-logloss:0.156221\teval-logloss:0.230153\n",
      "[1110]\ttrain-logloss:0.155672\teval-logloss:0.230144\n",
      "[1125]\ttrain-logloss:0.155115\teval-logloss:0.230093\n",
      "[1140]\ttrain-logloss:0.154346\teval-logloss:0.230032\n",
      "[1155]\ttrain-logloss:0.153867\teval-logloss:0.230011\n",
      "[1170]\ttrain-logloss:0.153451\teval-logloss:0.229998\n",
      "[1185]\ttrain-logloss:0.152857\teval-logloss:0.229961\n",
      "[1200]\ttrain-logloss:0.152392\teval-logloss:0.229974\n",
      "[1215]\ttrain-logloss:0.151526\teval-logloss:0.229885\n",
      "[1230]\ttrain-logloss:0.151063\teval-logloss:0.229859\n",
      "[1245]\ttrain-logloss:0.150539\teval-logloss:0.229848\n",
      "[1260]\ttrain-logloss:0.149928\teval-logloss:0.229839\n",
      "[1275]\ttrain-logloss:0.149582\teval-logloss:0.229849\n",
      "[1290]\ttrain-logloss:0.14901\teval-logloss:0.229807\n",
      "[1305]\ttrain-logloss:0.14841\teval-logloss:0.229817\n",
      "[1320]\ttrain-logloss:0.147517\teval-logloss:0.229754\n",
      "[1335]\ttrain-logloss:0.147057\teval-logloss:0.229769\n",
      "[1350]\ttrain-logloss:0.14666\teval-logloss:0.2298\n",
      "[1365]\ttrain-logloss:0.146155\teval-logloss:0.229793\n",
      "[1380]\ttrain-logloss:0.145014\teval-logloss:0.229601\n",
      "[1395]\ttrain-logloss:0.144243\teval-logloss:0.229547\n",
      "[1410]\ttrain-logloss:0.143681\teval-logloss:0.229506\n",
      "[1425]\ttrain-logloss:0.143254\teval-logloss:0.229467\n",
      "[1440]\ttrain-logloss:0.142688\teval-logloss:0.229479\n",
      "[1455]\ttrain-logloss:0.142267\teval-logloss:0.229467\n",
      "[1470]\ttrain-logloss:0.141437\teval-logloss:0.22946\n",
      "[1485]\ttrain-logloss:0.14076\teval-logloss:0.229414\n",
      "[1500]\ttrain-logloss:0.139849\teval-logloss:0.229271\n",
      "[1515]\ttrain-logloss:0.13925\teval-logloss:0.22924\n",
      "[1530]\ttrain-logloss:0.138945\teval-logloss:0.229253\n",
      "[1545]\ttrain-logloss:0.138611\teval-logloss:0.229287\n",
      "[1560]\ttrain-logloss:0.138149\teval-logloss:0.22932\n",
      "[1575]\ttrain-logloss:0.137777\teval-logloss:0.229312\n",
      "[1590]\ttrain-logloss:0.13711\teval-logloss:0.229343\n",
      "[1605]\ttrain-logloss:0.136435\teval-logloss:0.229273\n",
      "[1620]\ttrain-logloss:0.136007\teval-logloss:0.229259\n",
      "[1635]\ttrain-logloss:0.13529\teval-logloss:0.229212\n",
      "[1650]\ttrain-logloss:0.134611\teval-logloss:0.229121\n",
      "[1665]\ttrain-logloss:0.134199\teval-logloss:0.229109\n",
      "[1680]\ttrain-logloss:0.133662\teval-logloss:0.229073\n",
      "[1695]\ttrain-logloss:0.133224\teval-logloss:0.229098\n",
      "[1710]\ttrain-logloss:0.132843\teval-logloss:0.229118\n",
      "[1725]\ttrain-logloss:0.13251\teval-logloss:0.22915\n",
      "[1740]\ttrain-logloss:0.132176\teval-logloss:0.229169\n",
      "[1755]\ttrain-logloss:0.131652\teval-logloss:0.229164\n",
      "[1770]\ttrain-logloss:0.131381\teval-logloss:0.229199\n",
      "[1785]\ttrain-logloss:0.130926\teval-logloss:0.229163\n",
      "[1800]\ttrain-logloss:0.130503\teval-logloss:0.229163\n",
      "[1815]\ttrain-logloss:0.130072\teval-logloss:0.229215\n",
      "[1830]\ttrain-logloss:0.129472\teval-logloss:0.229236\n",
      "[1845]\ttrain-logloss:0.129013\teval-logloss:0.229222\n",
      "[1860]\ttrain-logloss:0.12856\teval-logloss:0.229192\n",
      "[1875]\ttrain-logloss:0.128229\teval-logloss:0.229222\n",
      "[1890]\ttrain-logloss:0.127758\teval-logloss:0.229215\n",
      "[1905]\ttrain-logloss:0.12726\teval-logloss:0.229218\n",
      "[1920]\ttrain-logloss:0.126599\teval-logloss:0.229179\n",
      "[1935]\ttrain-logloss:0.126121\teval-logloss:0.229175\n",
      "[1950]\ttrain-logloss:0.125634\teval-logloss:0.229213\n",
      "[1965]\ttrain-logloss:0.125099\teval-logloss:0.229243\n",
      "[1980]\ttrain-logloss:0.124663\teval-logloss:0.229245\n",
      "[1995]\ttrain-logloss:0.124336\teval-logloss:0.229216\n",
      "[0]\ttrain-logloss:0.633403\teval-logloss:0.633755\n",
      "[15]\ttrain-logloss:0.3219\teval-logloss:0.325131\n",
      "[30]\ttrain-logloss:0.269595\teval-logloss:0.274644\n",
      "[45]\ttrain-logloss:0.255041\teval-logloss:0.261498\n",
      "[60]\ttrain-logloss:0.245157\teval-logloss:0.253851\n",
      "[75]\ttrain-logloss:0.238482\teval-logloss:0.249165\n",
      "[90]\ttrain-logloss:0.233485\teval-logloss:0.246373\n",
      "[105]\ttrain-logloss:0.229455\teval-logloss:0.244384\n",
      "[120]\ttrain-logloss:0.225952\teval-logloss:0.242767\n",
      "[135]\ttrain-logloss:0.222712\teval-logloss:0.241357\n",
      "[150]\ttrain-logloss:0.220025\teval-logloss:0.24043\n",
      "[165]\ttrain-logloss:0.217651\teval-logloss:0.239423\n",
      "[180]\ttrain-logloss:0.215454\teval-logloss:0.238786\n",
      "[195]\ttrain-logloss:0.213368\teval-logloss:0.238206\n",
      "[210]\ttrain-logloss:0.211722\teval-logloss:0.23769\n",
      "[225]\ttrain-logloss:0.209631\teval-logloss:0.237027\n",
      "[240]\ttrain-logloss:0.208001\teval-logloss:0.23663\n",
      "[255]\ttrain-logloss:0.206414\teval-logloss:0.236232\n",
      "[270]\ttrain-logloss:0.204787\teval-logloss:0.235791\n",
      "[285]\ttrain-logloss:0.203691\teval-logloss:0.235578\n",
      "[300]\ttrain-logloss:0.202302\teval-logloss:0.235288\n",
      "[315]\ttrain-logloss:0.201293\teval-logloss:0.235178\n",
      "[330]\ttrain-logloss:0.200139\teval-logloss:0.234942\n",
      "[345]\ttrain-logloss:0.199291\teval-logloss:0.23486\n",
      "[360]\ttrain-logloss:0.197783\teval-logloss:0.234533\n",
      "[375]\ttrain-logloss:0.196662\teval-logloss:0.234367\n",
      "[390]\ttrain-logloss:0.195342\teval-logloss:0.234068\n",
      "[405]\ttrain-logloss:0.194224\teval-logloss:0.233777\n",
      "[420]\ttrain-logloss:0.193169\teval-logloss:0.233652\n",
      "[435]\ttrain-logloss:0.192271\teval-logloss:0.233441\n",
      "[450]\ttrain-logloss:0.191527\teval-logloss:0.233354\n",
      "[465]\ttrain-logloss:0.190702\teval-logloss:0.2333\n",
      "[480]\ttrain-logloss:0.190008\teval-logloss:0.233281\n",
      "[495]\ttrain-logloss:0.189261\teval-logloss:0.233249\n",
      "[510]\ttrain-logloss:0.188171\teval-logloss:0.233059\n",
      "[525]\ttrain-logloss:0.187391\teval-logloss:0.232998\n",
      "[540]\ttrain-logloss:0.186627\teval-logloss:0.232898\n",
      "[555]\ttrain-logloss:0.185085\teval-logloss:0.232575\n",
      "[570]\ttrain-logloss:0.184099\teval-logloss:0.232429\n",
      "[585]\ttrain-logloss:0.183107\teval-logloss:0.232233\n",
      "[600]\ttrain-logloss:0.182444\teval-logloss:0.232185\n",
      "[615]\ttrain-logloss:0.181735\teval-logloss:0.232196\n",
      "[630]\ttrain-logloss:0.181003\teval-logloss:0.232214\n",
      "[645]\ttrain-logloss:0.180222\teval-logloss:0.23208\n",
      "[660]\ttrain-logloss:0.179258\teval-logloss:0.231985\n",
      "[675]\ttrain-logloss:0.17851\teval-logloss:0.23187\n",
      "[690]\ttrain-logloss:0.177632\teval-logloss:0.231746\n",
      "[705]\ttrain-logloss:0.176798\teval-logloss:0.231626\n",
      "[720]\ttrain-logloss:0.176144\teval-logloss:0.231603\n",
      "[735]\ttrain-logloss:0.175431\teval-logloss:0.231537\n",
      "[750]\ttrain-logloss:0.174756\teval-logloss:0.231467\n",
      "[765]\ttrain-logloss:0.174114\teval-logloss:0.23143\n",
      "[780]\ttrain-logloss:0.173176\teval-logloss:0.231303\n",
      "[795]\ttrain-logloss:0.172077\teval-logloss:0.231099\n",
      "[810]\ttrain-logloss:0.170984\teval-logloss:0.230987\n",
      "[825]\ttrain-logloss:0.170381\teval-logloss:0.231012\n",
      "[840]\ttrain-logloss:0.16971\teval-logloss:0.231033\n",
      "[855]\ttrain-logloss:0.169198\teval-logloss:0.230996\n",
      "[870]\ttrain-logloss:0.168595\teval-logloss:0.230875\n",
      "[885]\ttrain-logloss:0.16746\teval-logloss:0.230673\n",
      "[900]\ttrain-logloss:0.166733\teval-logloss:0.230643\n",
      "[915]\ttrain-logloss:0.165964\teval-logloss:0.230519\n",
      "[930]\ttrain-logloss:0.165353\teval-logloss:0.230559\n",
      "[945]\ttrain-logloss:0.164899\teval-logloss:0.230528\n",
      "[960]\ttrain-logloss:0.164395\teval-logloss:0.230527\n",
      "[975]\ttrain-logloss:0.163659\teval-logloss:0.230489\n",
      "[990]\ttrain-logloss:0.163025\teval-logloss:0.230456\n",
      "[1005]\ttrain-logloss:0.162198\teval-logloss:0.230431\n",
      "[1020]\ttrain-logloss:0.161343\teval-logloss:0.23031\n",
      "[1035]\ttrain-logloss:0.160768\teval-logloss:0.230269\n",
      "[1050]\ttrain-logloss:0.16006\teval-logloss:0.230228\n",
      "[1065]\ttrain-logloss:0.159314\teval-logloss:0.230214\n",
      "[1080]\ttrain-logloss:0.158666\teval-logloss:0.230207\n",
      "[1095]\ttrain-logloss:0.158088\teval-logloss:0.230158\n",
      "[1110]\ttrain-logloss:0.157641\teval-logloss:0.230128\n",
      "[1125]\ttrain-logloss:0.157027\teval-logloss:0.23008\n",
      "[1140]\ttrain-logloss:0.156478\teval-logloss:0.230058\n",
      "[1155]\ttrain-logloss:0.155931\teval-logloss:0.230036\n",
      "[1170]\ttrain-logloss:0.154916\teval-logloss:0.229925\n",
      "[1185]\ttrain-logloss:0.154376\teval-logloss:0.229915\n",
      "[1200]\ttrain-logloss:0.153714\teval-logloss:0.229931\n",
      "[1215]\ttrain-logloss:0.153393\teval-logloss:0.229911\n",
      "[1230]\ttrain-logloss:0.152663\teval-logloss:0.229804\n",
      "[1245]\ttrain-logloss:0.152161\teval-logloss:0.229826\n",
      "[1260]\ttrain-logloss:0.151621\teval-logloss:0.229815\n",
      "[1275]\ttrain-logloss:0.151149\teval-logloss:0.229846\n",
      "[1290]\ttrain-logloss:0.150806\teval-logloss:0.229861\n",
      "[1305]\ttrain-logloss:0.150173\teval-logloss:0.229758\n",
      "[1320]\ttrain-logloss:0.149494\teval-logloss:0.229726\n",
      "[1335]\ttrain-logloss:0.148498\teval-logloss:0.22959\n",
      "[1350]\ttrain-logloss:0.14806\teval-logloss:0.229592\n",
      "[1365]\ttrain-logloss:0.147711\teval-logloss:0.229633\n",
      "[1380]\ttrain-logloss:0.147243\teval-logloss:0.229562\n",
      "[1395]\ttrain-logloss:0.146389\teval-logloss:0.229426\n",
      "[1410]\ttrain-logloss:0.145883\teval-logloss:0.229397\n",
      "[1425]\ttrain-logloss:0.145266\teval-logloss:0.229326\n",
      "[1440]\ttrain-logloss:0.144647\teval-logloss:0.22921\n",
      "[1455]\ttrain-logloss:0.144047\teval-logloss:0.229214\n",
      "[1470]\ttrain-logloss:0.143737\teval-logloss:0.229224\n",
      "[1485]\ttrain-logloss:0.143033\teval-logloss:0.229115\n",
      "[1500]\ttrain-logloss:0.142599\teval-logloss:0.229118\n",
      "[1515]\ttrain-logloss:0.142146\teval-logloss:0.229119\n",
      "[1530]\ttrain-logloss:0.141578\teval-logloss:0.229087\n",
      "[1545]\ttrain-logloss:0.141187\teval-logloss:0.229055\n",
      "[1560]\ttrain-logloss:0.140624\teval-logloss:0.22899\n",
      "[1575]\ttrain-logloss:0.139754\teval-logloss:0.228857\n",
      "[1590]\ttrain-logloss:0.139077\teval-logloss:0.228839\n",
      "[1605]\ttrain-logloss:0.138556\teval-logloss:0.228817\n",
      "[1620]\ttrain-logloss:0.138142\teval-logloss:0.228816\n",
      "[1635]\ttrain-logloss:0.137389\teval-logloss:0.228754\n",
      "[1650]\ttrain-logloss:0.136741\teval-logloss:0.228741\n",
      "[1665]\ttrain-logloss:0.136131\teval-logloss:0.228665\n",
      "[1680]\ttrain-logloss:0.135651\teval-logloss:0.228668\n",
      "[1695]\ttrain-logloss:0.135053\teval-logloss:0.228649\n",
      "[1710]\ttrain-logloss:0.134574\teval-logloss:0.228676\n",
      "[1725]\ttrain-logloss:0.134246\teval-logloss:0.228664\n",
      "[1740]\ttrain-logloss:0.133923\teval-logloss:0.228676\n",
      "[1755]\ttrain-logloss:0.133229\teval-logloss:0.228559\n",
      "[1770]\ttrain-logloss:0.132535\teval-logloss:0.228491\n",
      "[1785]\ttrain-logloss:0.132003\teval-logloss:0.22844\n",
      "[1800]\ttrain-logloss:0.131404\teval-logloss:0.228364\n",
      "[1815]\ttrain-logloss:0.130988\teval-logloss:0.228345\n",
      "[1830]\ttrain-logloss:0.130266\teval-logloss:0.228298\n",
      "[1845]\ttrain-logloss:0.129941\teval-logloss:0.22831\n",
      "[1860]\ttrain-logloss:0.129493\teval-logloss:0.228291\n",
      "[1875]\ttrain-logloss:0.129001\teval-logloss:0.228273\n",
      "[1890]\ttrain-logloss:0.128612\teval-logloss:0.228253\n",
      "[1905]\ttrain-logloss:0.127932\teval-logloss:0.228156\n",
      "[1920]\ttrain-logloss:0.127433\teval-logloss:0.228146\n",
      "[1935]\ttrain-logloss:0.126923\teval-logloss:0.228119\n",
      "[1950]\ttrain-logloss:0.126465\teval-logloss:0.228137\n",
      "[1965]\ttrain-logloss:0.126093\teval-logloss:0.22809\n",
      "[1980]\ttrain-logloss:0.12556\teval-logloss:0.2281\n",
      "[1995]\ttrain-logloss:0.125325\teval-logloss:0.228122\n",
      "[0]\ttrain-logloss:0.633523\teval-logloss:0.633466\n",
      "[15]\ttrain-logloss:0.322365\teval-logloss:0.323309\n",
      "[30]\ttrain-logloss:0.26963\teval-logloss:0.272102\n",
      "[45]\ttrain-logloss:0.255002\teval-logloss:0.258631\n",
      "[60]\ttrain-logloss:0.24608\teval-logloss:0.251834\n",
      "[75]\ttrain-logloss:0.238861\teval-logloss:0.24716\n",
      "[90]\ttrain-logloss:0.234069\teval-logloss:0.244546\n",
      "[105]\ttrain-logloss:0.229787\teval-logloss:0.242293\n",
      "[120]\ttrain-logloss:0.226779\teval-logloss:0.241019\n",
      "[135]\ttrain-logloss:0.223743\teval-logloss:0.23973\n",
      "[150]\ttrain-logloss:0.221184\teval-logloss:0.238792\n",
      "[165]\ttrain-logloss:0.218569\teval-logloss:0.237801\n",
      "[180]\ttrain-logloss:0.216533\teval-logloss:0.237102\n",
      "[195]\ttrain-logloss:0.214026\teval-logloss:0.236078\n",
      "[210]\ttrain-logloss:0.212112\teval-logloss:0.235467\n",
      "[225]\ttrain-logloss:0.210335\teval-logloss:0.234904\n",
      "[240]\ttrain-logloss:0.208742\teval-logloss:0.234652\n",
      "[255]\ttrain-logloss:0.207171\teval-logloss:0.234145\n",
      "[270]\ttrain-logloss:0.205781\teval-logloss:0.23384\n",
      "[285]\ttrain-logloss:0.203945\teval-logloss:0.233345\n",
      "[300]\ttrain-logloss:0.202768\teval-logloss:0.233128\n",
      "[315]\ttrain-logloss:0.201397\teval-logloss:0.2329\n",
      "[330]\ttrain-logloss:0.200356\teval-logloss:0.232741\n",
      "[345]\ttrain-logloss:0.199189\teval-logloss:0.232556\n",
      "[360]\ttrain-logloss:0.197854\teval-logloss:0.232325\n",
      "[375]\ttrain-logloss:0.197023\teval-logloss:0.232187\n",
      "[390]\ttrain-logloss:0.195977\teval-logloss:0.232062\n",
      "[405]\ttrain-logloss:0.194849\teval-logloss:0.231877\n",
      "[420]\ttrain-logloss:0.193511\teval-logloss:0.23166\n",
      "[435]\ttrain-logloss:0.192182\teval-logloss:0.231341\n",
      "[450]\ttrain-logloss:0.190949\teval-logloss:0.231022\n",
      "[465]\ttrain-logloss:0.190239\teval-logloss:0.230985\n",
      "[480]\ttrain-logloss:0.189405\teval-logloss:0.230866\n",
      "[495]\ttrain-logloss:0.18867\teval-logloss:0.230816\n",
      "[510]\ttrain-logloss:0.18789\teval-logloss:0.23075\n",
      "[525]\ttrain-logloss:0.187082\teval-logloss:0.230716\n",
      "[540]\ttrain-logloss:0.185943\teval-logloss:0.230429\n",
      "[555]\ttrain-logloss:0.185102\teval-logloss:0.230346\n",
      "[570]\ttrain-logloss:0.18448\teval-logloss:0.230355\n",
      "[585]\ttrain-logloss:0.183215\teval-logloss:0.230175\n",
      "[600]\ttrain-logloss:0.182582\teval-logloss:0.230243\n",
      "[615]\ttrain-logloss:0.181619\teval-logloss:0.230064\n",
      "[630]\ttrain-logloss:0.18089\teval-logloss:0.229993\n",
      "[645]\ttrain-logloss:0.180263\teval-logloss:0.229992\n",
      "[660]\ttrain-logloss:0.179475\teval-logloss:0.229973\n",
      "[675]\ttrain-logloss:0.178508\teval-logloss:0.229786\n",
      "[690]\ttrain-logloss:0.177672\teval-logloss:0.229789\n",
      "[705]\ttrain-logloss:0.176282\teval-logloss:0.229489\n",
      "[720]\ttrain-logloss:0.17557\teval-logloss:0.229438\n",
      "[735]\ttrain-logloss:0.175016\teval-logloss:0.229439\n",
      "[750]\ttrain-logloss:0.174506\teval-logloss:0.229449\n",
      "[765]\ttrain-logloss:0.17391\teval-logloss:0.229361\n",
      "[780]\ttrain-logloss:0.17318\teval-logloss:0.22925\n",
      "[795]\ttrain-logloss:0.171906\teval-logloss:0.229066\n",
      "[810]\ttrain-logloss:0.171327\teval-logloss:0.229037\n",
      "[825]\ttrain-logloss:0.170794\teval-logloss:0.229043\n",
      "[840]\ttrain-logloss:0.170063\teval-logloss:0.22896\n",
      "[855]\ttrain-logloss:0.169471\teval-logloss:0.228897\n",
      "[870]\ttrain-logloss:0.16876\teval-logloss:0.228836\n",
      "[885]\ttrain-logloss:0.167962\teval-logloss:0.228765\n",
      "[900]\ttrain-logloss:0.167308\teval-logloss:0.228687\n",
      "[915]\ttrain-logloss:0.1666\teval-logloss:0.228576\n",
      "[930]\ttrain-logloss:0.166117\teval-logloss:0.228603\n",
      "[945]\ttrain-logloss:0.165643\teval-logloss:0.22857\n",
      "[960]\ttrain-logloss:0.164949\teval-logloss:0.228517\n",
      "[975]\ttrain-logloss:0.16447\teval-logloss:0.228474\n",
      "[990]\ttrain-logloss:0.163563\teval-logloss:0.228387\n",
      "[1005]\ttrain-logloss:0.162625\teval-logloss:0.228288\n",
      "[1020]\ttrain-logloss:0.161861\teval-logloss:0.228235\n",
      "[1035]\ttrain-logloss:0.161252\teval-logloss:0.228261\n",
      "[1050]\ttrain-logloss:0.160442\teval-logloss:0.228131\n",
      "[1065]\ttrain-logloss:0.159893\teval-logloss:0.228051\n",
      "[1080]\ttrain-logloss:0.159294\teval-logloss:0.227987\n",
      "[1095]\ttrain-logloss:0.158864\teval-logloss:0.227963\n",
      "[1110]\ttrain-logloss:0.158323\teval-logloss:0.2279\n",
      "[1125]\ttrain-logloss:0.157953\teval-logloss:0.227889\n",
      "[1140]\ttrain-logloss:0.157478\teval-logloss:0.227868\n",
      "[1155]\ttrain-logloss:0.15668\teval-logloss:0.227743\n",
      "[1170]\ttrain-logloss:0.155927\teval-logloss:0.227682\n",
      "[1185]\ttrain-logloss:0.155412\teval-logloss:0.227684\n",
      "[1200]\ttrain-logloss:0.15471\teval-logloss:0.227595\n",
      "[1215]\ttrain-logloss:0.154\teval-logloss:0.227556\n",
      "[1230]\ttrain-logloss:0.153425\teval-logloss:0.227465\n",
      "[1245]\ttrain-logloss:0.152876\teval-logloss:0.227477\n",
      "[1260]\ttrain-logloss:0.152236\teval-logloss:0.227428\n",
      "[1275]\ttrain-logloss:0.151709\teval-logloss:0.227382\n",
      "[1290]\ttrain-logloss:0.151302\teval-logloss:0.227372\n",
      "[1305]\ttrain-logloss:0.150729\teval-logloss:0.227323\n",
      "[1320]\ttrain-logloss:0.150031\teval-logloss:0.227206\n",
      "[1335]\ttrain-logloss:0.149484\teval-logloss:0.22719\n",
      "[1350]\ttrain-logloss:0.148897\teval-logloss:0.227186\n",
      "[1365]\ttrain-logloss:0.148255\teval-logloss:0.227146\n",
      "[1380]\ttrain-logloss:0.147641\teval-logloss:0.227091\n",
      "[1395]\ttrain-logloss:0.14726\teval-logloss:0.227074\n",
      "[1410]\ttrain-logloss:0.146677\teval-logloss:0.22711\n",
      "[1425]\ttrain-logloss:0.145875\teval-logloss:0.226996\n",
      "[1440]\ttrain-logloss:0.145381\teval-logloss:0.226964\n",
      "[1455]\ttrain-logloss:0.14482\teval-logloss:0.226955\n",
      "[1470]\ttrain-logloss:0.144075\teval-logloss:0.226929\n",
      "[1485]\ttrain-logloss:0.143489\teval-logloss:0.226878\n",
      "[1500]\ttrain-logloss:0.143055\teval-logloss:0.226844\n",
      "[1515]\ttrain-logloss:0.142163\teval-logloss:0.226758\n",
      "[1530]\ttrain-logloss:0.141495\teval-logloss:0.22675\n",
      "[1545]\ttrain-logloss:0.141046\teval-logloss:0.22672\n",
      "[1560]\ttrain-logloss:0.140579\teval-logloss:0.226707\n",
      "[1575]\ttrain-logloss:0.140224\teval-logloss:0.226701\n",
      "[1590]\ttrain-logloss:0.139355\teval-logloss:0.226636\n",
      "[1605]\ttrain-logloss:0.138995\teval-logloss:0.22666\n",
      "[1620]\ttrain-logloss:0.138656\teval-logloss:0.226689\n",
      "[1635]\ttrain-logloss:0.137632\teval-logloss:0.226461\n",
      "[1650]\ttrain-logloss:0.137061\teval-logloss:0.22645\n",
      "[1665]\ttrain-logloss:0.136324\teval-logloss:0.226402\n",
      "[1680]\ttrain-logloss:0.135894\teval-logloss:0.226385\n",
      "[1695]\ttrain-logloss:0.135595\teval-logloss:0.226382\n",
      "[1710]\ttrain-logloss:0.135154\teval-logloss:0.22639\n",
      "[1725]\ttrain-logloss:0.134304\teval-logloss:0.22637\n",
      "[1740]\ttrain-logloss:0.133933\teval-logloss:0.226377\n",
      "[1755]\ttrain-logloss:0.133635\teval-logloss:0.226383\n",
      "[1770]\ttrain-logloss:0.133117\teval-logloss:0.226365\n",
      "[1785]\ttrain-logloss:0.132456\teval-logloss:0.226308\n",
      "[1800]\ttrain-logloss:0.132176\teval-logloss:0.226326\n",
      "[1815]\ttrain-logloss:0.131519\teval-logloss:0.226257\n",
      "[1830]\ttrain-logloss:0.13097\teval-logloss:0.226238\n",
      "[1845]\ttrain-logloss:0.130404\teval-logloss:0.226194\n",
      "[1860]\ttrain-logloss:0.129815\teval-logloss:0.22613\n",
      "[1875]\ttrain-logloss:0.129245\teval-logloss:0.226109\n",
      "[1890]\ttrain-logloss:0.128853\teval-logloss:0.226122\n",
      "[1905]\ttrain-logloss:0.128518\teval-logloss:0.226115\n",
      "[1920]\ttrain-logloss:0.128105\teval-logloss:0.226134\n",
      "[1935]\ttrain-logloss:0.127881\teval-logloss:0.226157\n",
      "[1950]\ttrain-logloss:0.127198\teval-logloss:0.226142\n",
      "[1965]\ttrain-logloss:0.126536\teval-logloss:0.226088\n",
      "[1980]\ttrain-logloss:0.126041\teval-logloss:0.226094\n",
      "[1995]\ttrain-logloss:0.125517\teval-logloss:0.22613\n",
      "[0]\ttrain-logloss:0.633542\teval-logloss:0.633544\n",
      "[15]\ttrain-logloss:0.322626\teval-logloss:0.323843\n",
      "[30]\ttrain-logloss:0.269458\teval-logloss:0.272682\n",
      "[45]\ttrain-logloss:0.254621\teval-logloss:0.259398\n",
      "[60]\ttrain-logloss:0.245472\teval-logloss:0.252199\n",
      "[75]\ttrain-logloss:0.238788\teval-logloss:0.247736\n",
      "[90]\ttrain-logloss:0.234296\teval-logloss:0.245073\n",
      "[105]\ttrain-logloss:0.230148\teval-logloss:0.242845\n",
      "[120]\ttrain-logloss:0.226414\teval-logloss:0.241038\n",
      "[135]\ttrain-logloss:0.223087\teval-logloss:0.23949\n",
      "[150]\ttrain-logloss:0.22064\teval-logloss:0.238578\n",
      "[165]\ttrain-logloss:0.218208\teval-logloss:0.237746\n",
      "[180]\ttrain-logloss:0.216158\teval-logloss:0.237151\n",
      "[195]\ttrain-logloss:0.214198\teval-logloss:0.236548\n",
      "[210]\ttrain-logloss:0.212355\teval-logloss:0.236043\n",
      "[225]\ttrain-logloss:0.210743\teval-logloss:0.235653\n",
      "[240]\ttrain-logloss:0.209216\teval-logloss:0.235124\n",
      "[255]\ttrain-logloss:0.207733\teval-logloss:0.234803\n",
      "[270]\ttrain-logloss:0.206305\teval-logloss:0.234505\n",
      "[285]\ttrain-logloss:0.205125\teval-logloss:0.234242\n",
      "[300]\ttrain-logloss:0.203356\teval-logloss:0.233733\n",
      "[315]\ttrain-logloss:0.202216\teval-logloss:0.233565\n",
      "[330]\ttrain-logloss:0.200831\teval-logloss:0.233326\n",
      "[345]\ttrain-logloss:0.199626\teval-logloss:0.233046\n",
      "[360]\ttrain-logloss:0.198376\teval-logloss:0.232746\n",
      "[375]\ttrain-logloss:0.197197\teval-logloss:0.232538\n",
      "[390]\ttrain-logloss:0.195915\teval-logloss:0.232341\n",
      "[405]\ttrain-logloss:0.194687\teval-logloss:0.232145\n",
      "[420]\ttrain-logloss:0.193652\teval-logloss:0.231947\n",
      "[435]\ttrain-logloss:0.192855\teval-logloss:0.23189\n",
      "[450]\ttrain-logloss:0.191824\teval-logloss:0.231708\n",
      "[465]\ttrain-logloss:0.190892\teval-logloss:0.231635\n",
      "[480]\ttrain-logloss:0.190118\teval-logloss:0.231583\n",
      "[495]\ttrain-logloss:0.188633\teval-logloss:0.231288\n",
      "[510]\ttrain-logloss:0.18762\teval-logloss:0.231078\n",
      "[525]\ttrain-logloss:0.186877\teval-logloss:0.231008\n",
      "[540]\ttrain-logloss:0.185878\teval-logloss:0.23078\n",
      "[555]\ttrain-logloss:0.185142\teval-logloss:0.230705\n",
      "[570]\ttrain-logloss:0.184235\teval-logloss:0.230618\n",
      "[585]\ttrain-logloss:0.183646\teval-logloss:0.230601\n",
      "[600]\ttrain-logloss:0.182731\teval-logloss:0.23048\n",
      "[615]\ttrain-logloss:0.182015\teval-logloss:0.230415\n",
      "[630]\ttrain-logloss:0.18085\teval-logloss:0.230233\n",
      "[645]\ttrain-logloss:0.179776\teval-logloss:0.230154\n",
      "[660]\ttrain-logloss:0.179083\teval-logloss:0.230121\n",
      "[675]\ttrain-logloss:0.17844\teval-logloss:0.230083\n",
      "[690]\ttrain-logloss:0.177768\teval-logloss:0.229997\n",
      "[705]\ttrain-logloss:0.176635\teval-logloss:0.229921\n",
      "[720]\ttrain-logloss:0.175485\teval-logloss:0.229725\n",
      "[735]\ttrain-logloss:0.174786\teval-logloss:0.229614\n",
      "[750]\ttrain-logloss:0.174123\teval-logloss:0.229544\n",
      "[765]\ttrain-logloss:0.173525\teval-logloss:0.229462\n",
      "[780]\ttrain-logloss:0.172789\teval-logloss:0.229366\n",
      "[795]\ttrain-logloss:0.171668\teval-logloss:0.229027\n",
      "[810]\ttrain-logloss:0.170692\teval-logloss:0.228849\n",
      "[825]\ttrain-logloss:0.170035\teval-logloss:0.228801\n",
      "[840]\ttrain-logloss:0.169203\teval-logloss:0.228677\n",
      "[855]\ttrain-logloss:0.168474\teval-logloss:0.228617\n",
      "[870]\ttrain-logloss:0.167905\teval-logloss:0.228611\n",
      "[885]\ttrain-logloss:0.167212\teval-logloss:0.228536\n",
      "[900]\ttrain-logloss:0.166658\teval-logloss:0.228458\n",
      "[915]\ttrain-logloss:0.165887\teval-logloss:0.228393\n",
      "[930]\ttrain-logloss:0.165001\teval-logloss:0.228296\n",
      "[945]\ttrain-logloss:0.164246\teval-logloss:0.228211\n",
      "[960]\ttrain-logloss:0.163721\teval-logloss:0.228217\n",
      "[975]\ttrain-logloss:0.163089\teval-logloss:0.228194\n",
      "[990]\ttrain-logloss:0.162484\teval-logloss:0.228125\n",
      "[1005]\ttrain-logloss:0.161338\teval-logloss:0.227987\n",
      "[1020]\ttrain-logloss:0.160461\teval-logloss:0.227892\n",
      "[1035]\ttrain-logloss:0.159907\teval-logloss:0.227778\n",
      "[1050]\ttrain-logloss:0.15944\teval-logloss:0.227783\n",
      "[1065]\ttrain-logloss:0.158754\teval-logloss:0.227712\n",
      "[1080]\ttrain-logloss:0.158295\teval-logloss:0.227718\n",
      "[1095]\ttrain-logloss:0.157855\teval-logloss:0.227735\n",
      "[1110]\ttrain-logloss:0.156878\teval-logloss:0.227642\n",
      "[1125]\ttrain-logloss:0.156224\teval-logloss:0.22758\n",
      "[1140]\ttrain-logloss:0.155455\teval-logloss:0.227487\n",
      "[1155]\ttrain-logloss:0.154833\teval-logloss:0.227399\n",
      "[1170]\ttrain-logloss:0.15441\teval-logloss:0.227402\n",
      "[1185]\ttrain-logloss:0.153675\teval-logloss:0.227317\n",
      "[1200]\ttrain-logloss:0.153223\teval-logloss:0.227281\n",
      "[1215]\ttrain-logloss:0.152592\teval-logloss:0.227268\n",
      "[1230]\ttrain-logloss:0.152176\teval-logloss:0.227294\n",
      "[1245]\ttrain-logloss:0.15146\teval-logloss:0.227149\n",
      "[1260]\ttrain-logloss:0.150874\teval-logloss:0.227083\n",
      "[1275]\ttrain-logloss:0.150332\teval-logloss:0.227052\n",
      "[1290]\ttrain-logloss:0.149766\teval-logloss:0.226976\n",
      "[1305]\ttrain-logloss:0.149126\teval-logloss:0.226862\n",
      "[1320]\ttrain-logloss:0.148609\teval-logloss:0.226838\n",
      "[1335]\ttrain-logloss:0.148166\teval-logloss:0.226856\n",
      "[1350]\ttrain-logloss:0.147692\teval-logloss:0.226866\n",
      "[1365]\ttrain-logloss:0.147011\teval-logloss:0.226859\n",
      "[1380]\ttrain-logloss:0.146588\teval-logloss:0.226837\n",
      "[1395]\ttrain-logloss:0.146187\teval-logloss:0.226838\n",
      "[1410]\ttrain-logloss:0.145587\teval-logloss:0.226796\n",
      "[1425]\ttrain-logloss:0.144333\teval-logloss:0.226414\n",
      "[1440]\ttrain-logloss:0.143274\teval-logloss:0.226247\n",
      "[1455]\ttrain-logloss:0.142838\teval-logloss:0.226245\n",
      "[1470]\ttrain-logloss:0.142356\teval-logloss:0.226187\n",
      "[1485]\ttrain-logloss:0.141823\teval-logloss:0.226163\n",
      "[1500]\ttrain-logloss:0.141259\teval-logloss:0.226127\n",
      "[1515]\ttrain-logloss:0.140379\teval-logloss:0.226055\n",
      "[1530]\ttrain-logloss:0.14\teval-logloss:0.226042\n",
      "[1545]\ttrain-logloss:0.13941\teval-logloss:0.225934\n",
      "[1560]\ttrain-logloss:0.138723\teval-logloss:0.225875\n",
      "[1575]\ttrain-logloss:0.138265\teval-logloss:0.225871\n",
      "[1590]\ttrain-logloss:0.137747\teval-logloss:0.225816\n",
      "[1605]\ttrain-logloss:0.137239\teval-logloss:0.225779\n",
      "[1620]\ttrain-logloss:0.136716\teval-logloss:0.225772\n",
      "[1635]\ttrain-logloss:0.136321\teval-logloss:0.225732\n",
      "[1650]\ttrain-logloss:0.135845\teval-logloss:0.225731\n",
      "[1665]\ttrain-logloss:0.135246\teval-logloss:0.225693\n",
      "[1680]\ttrain-logloss:0.134515\teval-logloss:0.225673\n",
      "[1695]\ttrain-logloss:0.134118\teval-logloss:0.225691\n",
      "[1710]\ttrain-logloss:0.133396\teval-logloss:0.225561\n",
      "[1725]\ttrain-logloss:0.132557\teval-logloss:0.22545\n",
      "[1740]\ttrain-logloss:0.132113\teval-logloss:0.225419\n",
      "[1755]\ttrain-logloss:0.131689\teval-logloss:0.225414\n",
      "[1770]\ttrain-logloss:0.131315\teval-logloss:0.225455\n",
      "[1785]\ttrain-logloss:0.130843\teval-logloss:0.225453\n",
      "[1800]\ttrain-logloss:0.130482\teval-logloss:0.225479\n",
      "[1815]\ttrain-logloss:0.130121\teval-logloss:0.225438\n",
      "[1830]\ttrain-logloss:0.12974\teval-logloss:0.225457\n",
      "[1845]\ttrain-logloss:0.128984\teval-logloss:0.225452\n",
      "[1860]\ttrain-logloss:0.128394\teval-logloss:0.225374\n",
      "[1875]\ttrain-logloss:0.128107\teval-logloss:0.225419\n",
      "[1890]\ttrain-logloss:0.127469\teval-logloss:0.225354\n",
      "[1905]\ttrain-logloss:0.12696\teval-logloss:0.225366\n",
      "[1920]\ttrain-logloss:0.126471\teval-logloss:0.225364\n",
      "[1935]\ttrain-logloss:0.126088\teval-logloss:0.225377\n",
      "[1950]\ttrain-logloss:0.125673\teval-logloss:0.225371\n",
      "[1965]\ttrain-logloss:0.125373\teval-logloss:0.225362\n",
      "[1980]\ttrain-logloss:0.124922\teval-logloss:0.225363\n",
      "[1995]\ttrain-logloss:0.124591\teval-logloss:0.225361\n",
      "XGB: 0.227 +- 0.001\n"
     ]
    }
   ],
   "source": [
    "param = {}\n",
    "param['objective'] = 'binary:logistic'\n",
    "param['eval_metric'] = 'logloss'\n",
    "param['max_depth'] = 7\n",
    "# param['booster'] = 'dart'\n",
    "#param['booster'] = 'gblinear'\n",
    "param['eta'] = 0.1\n",
    "#param['subsample'] = 1.0\n",
    "# param[\"scale_pos_weight\"] = 0.5\n",
    "#param['colsample_bytree'] = 0.7\n",
    "param['nthread'] = 8\n",
    "#param['num_class'] =3\n",
    "#param['alpha'] = 0.00\n",
    "#param['lambda_bias'] = 0.1\n",
    "# param['lambda'] = 0\n",
    "#param['min_child_weight'] = 100\n",
    "\n",
    "# if param['booster'] == 'gblinear': param['eta'] *= 1e-4\n",
    "numround = 2000\n",
    "\n",
    "#LGBM\n",
    "params = {\n",
    "    #'task': 'train',\n",
    "    #'boosting_type': 'gbdt',\n",
    "    #'objective': 'regression',\n",
    "    #'metric': {'l2', 'auc'},\n",
    "    #'num_leaves': 31,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    #'subsample': 0.8,\n",
    "    #'colsample_bytree': 0.7,\n",
    "    'objective': 'binary',\n",
    "    'nthread': -1,\n",
    "    'learning_rate': 0.1,\n",
    "    'min_child_weight': 100,\n",
    "    #'max_depth': 6\n",
    "    #'num_class': 3\n",
    "    \n",
    "}\n",
    "\n",
    "pred_train = np.zeros(len(y_train))\n",
    "xgbs = []\n",
    "sc,sc_mean = [],[]\n",
    "\n",
    "for itr, ite in folds:\n",
    "    ypred = []\n",
    "    for i in range(1):\n",
    "               \n",
    "        param['seed'] = i+1\n",
    "        \n",
    "        #xgboost\n",
    "        \n",
    "        Xdatatrain = xgb.DMatrix(data=X_train.ix[itr, :].values,\n",
    "                                     label=y_train.ix[itr].values)\n",
    "        Xdataval = xgb.DMatrix(data=X_train.ix[ite, :].values,\n",
    "                                     label=y_train.ix[ite].values)\n",
    "\n",
    "        plst = list(param.items())\n",
    "        watchlist = [(Xdatatrain, 'train'), (Xdataval, 'eval')]\n",
    "        bst = xgb.train(plst, Xdatatrain, numround, evals=watchlist, verbose_eval=15)\n",
    "\n",
    "        \n",
    "        #rc = ensemble.ExtraTreesClassifier(n_estimators=1300, criterion='gini', max_depth=None, n_jobs=-1)\n",
    "        #pred_train[ite] = bst.predict(Xdataval)\n",
    "        #neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "        #neigh.fit(X_train.ix[itr, :], y_train.ix[itr])\n",
    "        ypred = bst.predict(Xdataval)\n",
    "        #ypred = neigh.predict_proba(X_train.ix[ite, :])\n",
    "        xgbs.append(bst)\n",
    "        '''\n",
    "        \n",
    "        # train\n",
    "        lgb = lgbm.sklearn.LGBMClassifier(n_estimators=1500, seed=0, **params)\n",
    "        lgb.fit(X_train.ix[itr, :], y_train.ix[itr])\n",
    "        ypred = lgb.predict_proba(X_train.ix[ite, :])[:, 1]\n",
    "\n",
    "        \n",
    "        model.fit(X_train.ix[itr, :].values, y_train_cat[itr],\n",
    "            epochs=60,\n",
    "            batch_size=1000)\n",
    "        ypred = model.predict(X_train.ix[ite, :].values)\n",
    "        '''\n",
    "    #ypred = sum(ypred) / len(ypred) 0.401408 0.392476\n",
    "    pred_train[ite] = ypred\n",
    "\n",
    "    \n",
    "    sc.append(log_loss(y_train.ix[ite, :], pred_train[ite]))\n",
    "\n",
    "    \n",
    "print('XGB: {:.3f} +- {:.3f}'.format(np.mean(sc), np.std(sc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def foo(x):\n",
    "    a = 0.165 / 0.37\n",
    "    b = (1 - 0.165) / (1 - 0.37) \n",
    "    return  a * x / (a * x + b * (1 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train = foo(pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train = pd.DataFrame(pred_train)\n",
    "pred_train.columns = ['y']\n",
    "pred_train.to_csv(\"stacking/xgb_20.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-a593efe9e73a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LGBM\n",
    "params = {\n",
    "    #'task': 'train',\n",
    "    #'boosting_type': 'gbdt',\n",
    "    #'objective': 'regression',\n",
    "    #'metric': {'l2', 'auc'},\n",
    "    #'num_leaves': 31,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    #'subsample': 0.8,\n",
    "    #'colsample_bytree': 0.7,\n",
    "    'objective': 'binary',\n",
    "    'nthread': -1,\n",
    "    'learning_rate': 0.1,\n",
    "    'min_child_weight': 100,\n",
    "    #'max_depth': 6\n",
    "    #'num_class': 3\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fao3864/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:112: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/fao3864/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:147: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=10, min_child_weight=100,\n",
       "        min_split_gain=0, n_estimators=1500, nthread=-1, num_leaves=31,\n",
       "        objective='binary', reg_alpha=0, reg_lambda=0, scale_pos_weight=1,\n",
       "        seed=0, sigmoid=1.0, silent=True, skip_drop=0.5, subsample=1,\n",
       "        subsample_for_bin=50000, subsample_freq=1, uniform_drop=False,\n",
       "        xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb = lgbm.sklearn.LGBMClassifier(n_estimators=1500, seed=0, **params)\n",
    "lgb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X_test.ix[1000001:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pred2 = lgb.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00238842,  0.16396599,  0.48445084,  0.00186383])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred1[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.82320214e-04,   7.93127424e-02,   5.11417772e-01,\n",
       "         1.19566213e-04])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred1[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.04161744e-01,   5.60700423e-05,   1.55585459e-04,\n",
       "         9.90305652e-02])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred2[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.54776044e-04,   5.87363544e-05,   4.17804929e-01,\n",
       "         3.63192961e-01])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred2[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000001"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345795"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = np.concatenate([test_pred1, test_pred2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2345796"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.38842223e-03,   1.63965988e-01,   4.84450842e-01, ...,\n",
       "         5.87363544e-05,   4.17804929e-01,   3.63192961e-01])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {}\n",
    "param['objective'] = 'binary:logistic'\n",
    "param['eval_metric'] = 'logloss'\n",
    "param['max_depth'] = 7\n",
    "# param['booster'] = 'dart'\n",
    "#param['booster'] = 'gblinear'\n",
    "param['eta'] = 0.1\n",
    "#param['subsample'] = 1.0\n",
    "# param[\"scale_pos_weight\"] = 0.5\n",
    "#param['colsample_bytree'] = 0.7\n",
    "param['nthread'] = 8\n",
    "#param['num_class'] =3\n",
    "#param['alpha'] = 0.00\n",
    "#param['lambda_bias'] = 0.1\n",
    "# param['lambda'] = 0\n",
    "#param['min_child_weight'] = 100\n",
    "\n",
    "# if param['booster'] == 'gblinear': param['eta'] *= 1e-4\n",
    "numround = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xdatatrain = xgb.DMatrix(data=X_train.ix[:, :].values,\n",
    "                                     label=y_train.ix[:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xdataval = xgb.DMatrix(data=X_train.ix[:10, :].values,\n",
    "                                     label=y_train.ix[:10].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del Xdatatest1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xdatatest2 = xgb.DMatrix(data=X_test.ix[1000001:, :].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xdatatest1 = xgb.DMatrix(data=X_test.ix[:1000000, :].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plst = list(param.items())\n",
    "watchlist = [(Xdatatrain, 'train'), (Xdataval, 'eval')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.633472\teval-logloss:0.646774\n",
      "[15]\ttrain-logloss:0.322693\teval-logloss:0.383473\n",
      "[30]\ttrain-logloss:0.269731\teval-logloss:0.331517\n",
      "[45]\ttrain-logloss:0.254977\teval-logloss:0.311695\n",
      "[60]\ttrain-logloss:0.245918\teval-logloss:0.290251\n",
      "[75]\ttrain-logloss:0.239265\teval-logloss:0.279671\n",
      "[90]\ttrain-logloss:0.23455\teval-logloss:0.260438\n",
      "[105]\ttrain-logloss:0.230924\teval-logloss:0.252394\n",
      "[120]\ttrain-logloss:0.227844\teval-logloss:0.240618\n",
      "[135]\ttrain-logloss:0.224619\teval-logloss:0.239634\n",
      "[150]\ttrain-logloss:0.222101\teval-logloss:0.230879\n",
      "[165]\ttrain-logloss:0.2197\teval-logloss:0.229697\n",
      "[180]\ttrain-logloss:0.217265\teval-logloss:0.223317\n",
      "[195]\ttrain-logloss:0.215507\teval-logloss:0.221776\n",
      "[210]\ttrain-logloss:0.213701\teval-logloss:0.229091\n",
      "[225]\ttrain-logloss:0.212286\teval-logloss:0.226111\n",
      "[240]\ttrain-logloss:0.21098\teval-logloss:0.226461\n",
      "[255]\ttrain-logloss:0.209291\teval-logloss:0.227288\n",
      "[270]\ttrain-logloss:0.207949\teval-logloss:0.229171\n",
      "[285]\ttrain-logloss:0.206682\teval-logloss:0.228368\n",
      "[300]\ttrain-logloss:0.205701\teval-logloss:0.226876\n",
      "[315]\ttrain-logloss:0.204694\teval-logloss:0.224764\n",
      "[330]\ttrain-logloss:0.203657\teval-logloss:0.2216\n",
      "[345]\ttrain-logloss:0.202038\teval-logloss:0.220242\n",
      "[360]\ttrain-logloss:0.200842\teval-logloss:0.219165\n",
      "[375]\ttrain-logloss:0.199603\teval-logloss:0.215843\n",
      "[390]\ttrain-logloss:0.198605\teval-logloss:0.217405\n",
      "[405]\ttrain-logloss:0.197753\teval-logloss:0.218154\n",
      "[420]\ttrain-logloss:0.197076\teval-logloss:0.218151\n",
      "[435]\ttrain-logloss:0.195931\teval-logloss:0.214611\n",
      "[450]\ttrain-logloss:0.19494\teval-logloss:0.210596\n",
      "[465]\ttrain-logloss:0.194123\teval-logloss:0.211912\n",
      "[480]\ttrain-logloss:0.193237\teval-logloss:0.209964\n",
      "[495]\ttrain-logloss:0.19249\teval-logloss:0.209754\n",
      "[510]\ttrain-logloss:0.191523\teval-logloss:0.208185\n",
      "[525]\ttrain-logloss:0.190815\teval-logloss:0.20902\n",
      "[540]\ttrain-logloss:0.190227\teval-logloss:0.210083\n",
      "[555]\ttrain-logloss:0.188921\teval-logloss:0.213588\n",
      "[570]\ttrain-logloss:0.188063\teval-logloss:0.214033\n",
      "[585]\ttrain-logloss:0.187197\teval-logloss:0.213175\n",
      "[600]\ttrain-logloss:0.186414\teval-logloss:0.207974\n",
      "[615]\ttrain-logloss:0.185713\teval-logloss:0.208237\n",
      "[630]\ttrain-logloss:0.185113\teval-logloss:0.204453\n",
      "[645]\ttrain-logloss:0.184547\teval-logloss:0.208066\n",
      "[660]\ttrain-logloss:0.18401\teval-logloss:0.210379\n",
      "[675]\ttrain-logloss:0.182848\teval-logloss:0.209466\n",
      "[690]\ttrain-logloss:0.182053\teval-logloss:0.208796\n",
      "[705]\ttrain-logloss:0.18124\teval-logloss:0.208445\n",
      "[720]\ttrain-logloss:0.180508\teval-logloss:0.206346\n",
      "[735]\ttrain-logloss:0.179696\teval-logloss:0.205285\n",
      "[750]\ttrain-logloss:0.179031\teval-logloss:0.205042\n",
      "[765]\ttrain-logloss:0.178426\teval-logloss:0.202297\n",
      "[780]\ttrain-logloss:0.177745\teval-logloss:0.201258\n",
      "[795]\ttrain-logloss:0.177232\teval-logloss:0.202155\n",
      "[810]\ttrain-logloss:0.176344\teval-logloss:0.199487\n",
      "[825]\ttrain-logloss:0.175977\teval-logloss:0.194529\n",
      "[840]\ttrain-logloss:0.17534\teval-logloss:0.194596\n",
      "[855]\ttrain-logloss:0.174565\teval-logloss:0.194443\n",
      "[870]\ttrain-logloss:0.173969\teval-logloss:0.193045\n",
      "[885]\ttrain-logloss:0.17317\teval-logloss:0.190961\n",
      "[900]\ttrain-logloss:0.172618\teval-logloss:0.191853\n",
      "[915]\ttrain-logloss:0.171911\teval-logloss:0.188343\n",
      "[930]\ttrain-logloss:0.17132\teval-logloss:0.188493\n",
      "[945]\ttrain-logloss:0.170981\teval-logloss:0.189421\n",
      "[960]\ttrain-logloss:0.169946\teval-logloss:0.18989\n",
      "[975]\ttrain-logloss:0.169286\teval-logloss:0.189873\n",
      "[990]\ttrain-logloss:0.16889\teval-logloss:0.189521\n",
      "[1005]\ttrain-logloss:0.168077\teval-logloss:0.19056\n",
      "[1020]\ttrain-logloss:0.167708\teval-logloss:0.190572\n",
      "[1035]\ttrain-logloss:0.167167\teval-logloss:0.190699\n",
      "[1050]\ttrain-logloss:0.166546\teval-logloss:0.19146\n",
      "[1065]\ttrain-logloss:0.165865\teval-logloss:0.194367\n",
      "[1080]\ttrain-logloss:0.165236\teval-logloss:0.195051\n",
      "[1095]\ttrain-logloss:0.164753\teval-logloss:0.191767\n",
      "[1110]\ttrain-logloss:0.164211\teval-logloss:0.192787\n",
      "[1125]\ttrain-logloss:0.16367\teval-logloss:0.192565\n",
      "[1140]\ttrain-logloss:0.162847\teval-logloss:0.192204\n",
      "[1155]\ttrain-logloss:0.162453\teval-logloss:0.192199\n",
      "[1170]\ttrain-logloss:0.16203\teval-logloss:0.191605\n",
      "[1185]\ttrain-logloss:0.161533\teval-logloss:0.192208\n",
      "[1200]\ttrain-logloss:0.160841\teval-logloss:0.189936\n",
      "[1215]\ttrain-logloss:0.16031\teval-logloss:0.191134\n",
      "[1230]\ttrain-logloss:0.159526\teval-logloss:0.191721\n",
      "[1245]\ttrain-logloss:0.15917\teval-logloss:0.191706\n",
      "[1260]\ttrain-logloss:0.158478\teval-logloss:0.188282\n",
      "[1275]\ttrain-logloss:0.157846\teval-logloss:0.187383\n",
      "[1290]\ttrain-logloss:0.157289\teval-logloss:0.186645\n",
      "[1305]\ttrain-logloss:0.156941\teval-logloss:0.18598\n",
      "[1320]\ttrain-logloss:0.156315\teval-logloss:0.185366\n",
      "[1335]\ttrain-logloss:0.155832\teval-logloss:0.185239\n",
      "[1350]\ttrain-logloss:0.154925\teval-logloss:0.185447\n",
      "[1365]\ttrain-logloss:0.154001\teval-logloss:0.185104\n",
      "[1380]\ttrain-logloss:0.15345\teval-logloss:0.18471\n",
      "[1395]\ttrain-logloss:0.153002\teval-logloss:0.18306\n",
      "[1410]\ttrain-logloss:0.152634\teval-logloss:0.179595\n",
      "[1425]\ttrain-logloss:0.152177\teval-logloss:0.179597\n",
      "[1440]\ttrain-logloss:0.151836\teval-logloss:0.18057\n",
      "[1455]\ttrain-logloss:0.15149\teval-logloss:0.180573\n",
      "[1470]\ttrain-logloss:0.15123\teval-logloss:0.180578\n",
      "[1485]\ttrain-logloss:0.150766\teval-logloss:0.180343\n",
      "[1500]\ttrain-logloss:0.150359\teval-logloss:0.18058\n",
      "[1515]\ttrain-logloss:0.149551\teval-logloss:0.175247\n",
      "[1530]\ttrain-logloss:0.14917\teval-logloss:0.173626\n",
      "[1545]\ttrain-logloss:0.148727\teval-logloss:0.17361\n",
      "[1560]\ttrain-logloss:0.148304\teval-logloss:0.171845\n",
      "[1575]\ttrain-logloss:0.14741\teval-logloss:0.171646\n",
      "[1590]\ttrain-logloss:0.146904\teval-logloss:0.171243\n",
      "[1605]\ttrain-logloss:0.146523\teval-logloss:0.171245\n",
      "[1620]\ttrain-logloss:0.146073\teval-logloss:0.171413\n",
      "[1635]\ttrain-logloss:0.145749\teval-logloss:0.171423\n",
      "[1650]\ttrain-logloss:0.145378\teval-logloss:0.17141\n",
      "[1665]\ttrain-logloss:0.144975\teval-logloss:0.168965\n",
      "[1680]\ttrain-logloss:0.144684\teval-logloss:0.169908\n",
      "[1695]\ttrain-logloss:0.14425\teval-logloss:0.169918\n",
      "[1710]\ttrain-logloss:0.143813\teval-logloss:0.170331\n",
      "[1725]\ttrain-logloss:0.143439\teval-logloss:0.167049\n",
      "[1740]\ttrain-logloss:0.142943\teval-logloss:0.167248\n",
      "[1755]\ttrain-logloss:0.142473\teval-logloss:0.166665\n",
      "[1770]\ttrain-logloss:0.141896\teval-logloss:0.166706\n",
      "[1785]\ttrain-logloss:0.141321\teval-logloss:0.16591\n",
      "[1800]\ttrain-logloss:0.14084\teval-logloss:0.165736\n",
      "[1815]\ttrain-logloss:0.140295\teval-logloss:0.164383\n",
      "[1830]\ttrain-logloss:0.13986\teval-logloss:0.164703\n",
      "[1845]\ttrain-logloss:0.139423\teval-logloss:0.164723\n",
      "[1860]\ttrain-logloss:0.138851\teval-logloss:0.161282\n",
      "[1875]\ttrain-logloss:0.138109\teval-logloss:0.162912\n",
      "[1890]\ttrain-logloss:0.137756\teval-logloss:0.162651\n",
      "[1905]\ttrain-logloss:0.137463\teval-logloss:0.162642\n",
      "[1920]\ttrain-logloss:0.137019\teval-logloss:0.164063\n",
      "[1935]\ttrain-logloss:0.136529\teval-logloss:0.16207\n",
      "[1950]\ttrain-logloss:0.136218\teval-logloss:0.162713\n",
      "[1965]\ttrain-logloss:0.135701\teval-logloss:0.163214\n",
      "[1980]\ttrain-logloss:0.13528\teval-logloss:0.162998\n",
      "[1995]\ttrain-logloss:0.13475\teval-logloss:0.163154\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(plst, Xdatatrain, numround, evals=watchlist, verbose_eval=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pred2 = bst.predict(Xdatatest2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.99772982e-03,   1.35380983e-01,   4.63760883e-01,\n",
       "         1.97142363e-04], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred1[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000001"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.74771427e-02,   1.55032594e-05,   2.24600459e-04,\n",
       "         1.02886431e-01], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred2[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = np.concatenate([test_pred1, test_pred2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2345796"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pred = foo(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit['is_duplicate'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit.to_csv(\"stacking/xgb_20_test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.050046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.225398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.153910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.999033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.107626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.256820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.032084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.054397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.045547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.009870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.451490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.142813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.239037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.003972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.003417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.031240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.007674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.080474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345766</th>\n",
       "      <td>2345766</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345767</th>\n",
       "      <td>2345767</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345768</th>\n",
       "      <td>2345768</td>\n",
       "      <td>0.020111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345769</th>\n",
       "      <td>2345769</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345770</th>\n",
       "      <td>2345770</td>\n",
       "      <td>0.044307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345771</th>\n",
       "      <td>2345771</td>\n",
       "      <td>0.018992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345772</th>\n",
       "      <td>2345772</td>\n",
       "      <td>0.306606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345773</th>\n",
       "      <td>2345773</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345774</th>\n",
       "      <td>2345774</td>\n",
       "      <td>0.000364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345775</th>\n",
       "      <td>2345775</td>\n",
       "      <td>0.326139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345776</th>\n",
       "      <td>2345776</td>\n",
       "      <td>0.011620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345777</th>\n",
       "      <td>2345777</td>\n",
       "      <td>0.005192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345778</th>\n",
       "      <td>2345778</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345779</th>\n",
       "      <td>2345779</td>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345780</th>\n",
       "      <td>2345780</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345781</th>\n",
       "      <td>2345781</td>\n",
       "      <td>0.081946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345782</th>\n",
       "      <td>2345782</td>\n",
       "      <td>0.123281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345783</th>\n",
       "      <td>2345783</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345784</th>\n",
       "      <td>2345784</td>\n",
       "      <td>0.395903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345785</th>\n",
       "      <td>2345785</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345786</th>\n",
       "      <td>2345786</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345787</th>\n",
       "      <td>2345787</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345788</th>\n",
       "      <td>2345788</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345789</th>\n",
       "      <td>2345789</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345790</th>\n",
       "      <td>2345790</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345791</th>\n",
       "      <td>2345791</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345792</th>\n",
       "      <td>2345792</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345793</th>\n",
       "      <td>2345793</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345794</th>\n",
       "      <td>2345794</td>\n",
       "      <td>0.161929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345795</th>\n",
       "      <td>2345795</td>\n",
       "      <td>0.259490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2345796 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         test_id  is_duplicate\n",
       "0              0      0.002705\n",
       "1              1      0.050046\n",
       "2              2      0.225398\n",
       "3              3      0.000066\n",
       "4              4      0.153910\n",
       "5              5      0.002357\n",
       "6              6      0.999033\n",
       "7              7      0.107626\n",
       "8              8      0.256820\n",
       "9              9      0.000363\n",
       "10            10      0.032084\n",
       "11            11      0.000022\n",
       "12            12      0.000014\n",
       "13            13      0.054397\n",
       "14            14      0.045547\n",
       "15            15      0.009870\n",
       "16            16      0.000029\n",
       "17            17      0.451490\n",
       "18            18      0.142813\n",
       "19            19      0.239037\n",
       "20            20      0.000007\n",
       "21            21      0.003972\n",
       "22            22      0.001300\n",
       "23            23      0.003417\n",
       "24            24      0.000035\n",
       "25            25      0.031240\n",
       "26            26      0.000010\n",
       "27            27      0.007674\n",
       "28            28      0.080474\n",
       "29            29      0.000035\n",
       "...          ...           ...\n",
       "2345766  2345766      0.000002\n",
       "2345767  2345767      0.000013\n",
       "2345768  2345768      0.020111\n",
       "2345769  2345769      0.000006\n",
       "2345770  2345770      0.044307\n",
       "2345771  2345771      0.018992\n",
       "2345772  2345772      0.306606\n",
       "2345773  2345773      0.000006\n",
       "2345774  2345774      0.000364\n",
       "2345775  2345775      0.326139\n",
       "2345776  2345776      0.011620\n",
       "2345777  2345777      0.005192\n",
       "2345778  2345778      0.000055\n",
       "2345779  2345779      0.001193\n",
       "2345780  2345780      0.000051\n",
       "2345781  2345781      0.081946\n",
       "2345782  2345782      0.123281\n",
       "2345783  2345783      0.000019\n",
       "2345784  2345784      0.395903\n",
       "2345785  2345785      0.000006\n",
       "2345786  2345786      0.000011\n",
       "2345787  2345787      0.000016\n",
       "2345788  2345788      0.003585\n",
       "2345789  2345789      0.000001\n",
       "2345790  2345790      0.000014\n",
       "2345791  2345791      0.000021\n",
       "2345792  2345792      0.000036\n",
       "2345793  2345793      0.000026\n",
       "2345794  2345794      0.161929\n",
       "2345795  2345795      0.259490\n",
       "\n",
       "[2345796 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.323567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.218058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.095342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.007831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.956379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.098221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.363882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.002275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.423677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.034839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.033894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.020784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.392933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.150615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.284470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.018111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.004705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.004349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.061033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.009628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.099425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.000419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345766</th>\n",
       "      <td>2345766</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345767</th>\n",
       "      <td>2345767</td>\n",
       "      <td>0.000291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345768</th>\n",
       "      <td>2345768</td>\n",
       "      <td>0.023880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345769</th>\n",
       "      <td>2345769</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345770</th>\n",
       "      <td>2345770</td>\n",
       "      <td>0.063847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345771</th>\n",
       "      <td>2345771</td>\n",
       "      <td>0.446460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345772</th>\n",
       "      <td>2345772</td>\n",
       "      <td>0.089966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345773</th>\n",
       "      <td>2345773</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345774</th>\n",
       "      <td>2345774</td>\n",
       "      <td>0.001341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345775</th>\n",
       "      <td>2345775</td>\n",
       "      <td>0.276473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345776</th>\n",
       "      <td>2345776</td>\n",
       "      <td>0.032203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345777</th>\n",
       "      <td>2345777</td>\n",
       "      <td>0.009648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345778</th>\n",
       "      <td>2345778</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345779</th>\n",
       "      <td>2345779</td>\n",
       "      <td>0.009260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345780</th>\n",
       "      <td>2345780</td>\n",
       "      <td>0.000238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345781</th>\n",
       "      <td>2345781</td>\n",
       "      <td>0.058332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345782</th>\n",
       "      <td>2345782</td>\n",
       "      <td>0.088220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345783</th>\n",
       "      <td>2345783</td>\n",
       "      <td>0.000379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345784</th>\n",
       "      <td>2345784</td>\n",
       "      <td>0.418718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345785</th>\n",
       "      <td>2345785</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345786</th>\n",
       "      <td>2345786</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345787</th>\n",
       "      <td>2345787</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345788</th>\n",
       "      <td>2345788</td>\n",
       "      <td>0.011994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345789</th>\n",
       "      <td>2345789</td>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345790</th>\n",
       "      <td>2345790</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345791</th>\n",
       "      <td>2345791</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345792</th>\n",
       "      <td>2345792</td>\n",
       "      <td>0.000564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345793</th>\n",
       "      <td>2345793</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345794</th>\n",
       "      <td>2345794</td>\n",
       "      <td>0.166718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345795</th>\n",
       "      <td>2345795</td>\n",
       "      <td>0.176853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2345796 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         test_id  is_duplicate\n",
       "0              0      0.002633\n",
       "1              1      0.323567\n",
       "2              2      0.218058\n",
       "3              3      0.000561\n",
       "4              4      0.095342\n",
       "5              5      0.007831\n",
       "6              6      0.956379\n",
       "7              7      0.098221\n",
       "8              8      0.363882\n",
       "9              9      0.002275\n",
       "10            10      0.423677\n",
       "11            11      0.000240\n",
       "12            12      0.000097\n",
       "13            13      0.034839\n",
       "14            14      0.033894\n",
       "15            15      0.020784\n",
       "16            16      0.000108\n",
       "17            17      0.392933\n",
       "18            18      0.150615\n",
       "19            19      0.284470\n",
       "20            20      0.000149\n",
       "21            21      0.018111\n",
       "22            22      0.004705\n",
       "23            23      0.004349\n",
       "24            24      0.000205\n",
       "25            25      0.061033\n",
       "26            26      0.000158\n",
       "27            27      0.009628\n",
       "28            28      0.099425\n",
       "29            29      0.000419\n",
       "...          ...           ...\n",
       "2345766  2345766      0.000039\n",
       "2345767  2345767      0.000291\n",
       "2345768  2345768      0.023880\n",
       "2345769  2345769      0.000088\n",
       "2345770  2345770      0.063847\n",
       "2345771  2345771      0.446460\n",
       "2345772  2345772      0.089966\n",
       "2345773  2345773      0.000060\n",
       "2345774  2345774      0.001341\n",
       "2345775  2345775      0.276473\n",
       "2345776  2345776      0.032203\n",
       "2345777  2345777      0.009648\n",
       "2345778  2345778      0.000300\n",
       "2345779  2345779      0.009260\n",
       "2345780  2345780      0.000238\n",
       "2345781  2345781      0.058332\n",
       "2345782  2345782      0.088220\n",
       "2345783  2345783      0.000379\n",
       "2345784  2345784      0.418718\n",
       "2345785  2345785      0.000115\n",
       "2345786  2345786      0.000045\n",
       "2345787  2345787      0.000076\n",
       "2345788  2345788      0.011994\n",
       "2345789  2345789      0.000081\n",
       "2345790  2345790      0.000093\n",
       "2345791  2345791      0.000088\n",
       "2345792  2345792      0.000564\n",
       "2345793  2345793      0.000078\n",
       "2345794  2345794      0.166718\n",
       "2345795  2345795      0.176853\n",
       "\n",
       "[2345796 rows x 2 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
