{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fao3864/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn import manifold, decomposition, linear_model, ensemble, neighbors, cross_validation\n",
    "import sklearn\n",
    "from sklearn import pipeline, preprocessing, feature_extraction\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, auc, roc_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"data/train.csv\")\n",
    "X_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "y_train = pd.DataFrame(X_train['is_duplicate'])\n",
    "y_train.columns = ['y']\n",
    "del X_train['is_duplicate']\n",
    "\n",
    "skf = StratifiedKFold(5, shuffle=True, random_state=0)\n",
    "folds = []\n",
    "for itr, ite in skf.split(X_train, y_train.y):\n",
    "    folds += [[itr, ite]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"data/train_inter.csv\")\n",
    "X_test = pd.read_csv(\"data/test_inter.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train['question1'] = X_train['question1'].apply(lambda x: str(x))\n",
    "X_train['question2'] = X_train['question2'].apply(lambda x: str(x))\n",
    "X_test['question1'] = X_test['question1'].apply(lambda x: str(x))\n",
    "X_test['question2'] = X_test['question2'].apply(lambda x: str(x))\n",
    "\n",
    "X_train['question1'] = X_train['question1'].apply(lambda x: x.lower().split())\n",
    "X_train['question2'] = X_train['question2'].apply(lambda x: x.lower().split())\n",
    "X_test['question1'] = X_test['question1'].apply(lambda x: x.lower().split())\n",
    "X_test['question2'] = X_test['question2'].apply(lambda x: x.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = map(lambda x: ' '.join(x), X_train['question1'])\n",
    "X1 = map(lambda x: ' '.join(x), X_train['question2'])\n",
    "Y = map(lambda x: ' '.join(x), X_test['question1'])\n",
    "Y1 = map(lambda x: ' '.join(x), X_test['question2'])\n",
    "\n",
    "hw = HashingVectorizer(n_features=200).fit(X_train['question1'] + X_train['question2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(hw.transform(X).todense())\n",
    "X1 = pd.DataFrame(hw.transform(X1).todense())\n",
    "Y = pd.DataFrame(hw.transform(Y).todense())\n",
    "Y1 = pd.DataFrame(hw.transform(Y1).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = feature_extraction.text.TfidfTransformer()\n",
    "tfidf.fit(X+X1)\n",
    "\n",
    "X = pd.DataFrame(tfidf.transform(X).todense())\n",
    "X1 = pd.DataFrame(tfidf.transform(X1).todense())\n",
    "Y = pd.DataFrame(tfidf.transform(Y).todense())\n",
    "Y1 = pd.DataFrame(tfidf.transform(Y1).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train, X], axis=1, join_axes=[X_train.index])\n",
    "del X\n",
    "X_train = pd.concat([X_train, X1], axis=1, join_axes=[X_train.index])\n",
    "del X1\n",
    "X_test = pd.concat([X_test, Y], axis=1, join_axes=[X_test.index])\n",
    "del Y\n",
    "X_test = pd.concat([X_test, Y1], axis=1, join_axes=[X_test.index])\n",
    "del Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>word_match</th>\n",
       "      <th>tfidf_word_match</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[what, is, the, step, by, step, guide, to, inv...</td>\n",
       "      <td>[what, is, the, step, by, step, guide, to, inv...</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.772164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[what, is, the, story, of, kohinoor, (koh-i-no...</td>\n",
       "      <td>[what, would, happen, if, the, indian, governm...</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.361758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 407 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  [what, is, the, step, by, step, guide, to, inv...   \n",
       "1   1     3     4  [what, is, the, story, of, kohinoor, (koh-i-no...   \n",
       "\n",
       "                                           question2  word_match  \\\n",
       "0  [what, is, the, step, by, step, guide, to, inv...    0.727273   \n",
       "1  [what, would, happen, if, the, indian, governm...    0.307692   \n",
       "\n",
       "   tfidf_word_match    0    1    2 ...   190  191       192  193  194  195  \\\n",
       "0          0.772164  0.0  0.0  0.0 ...   0.0  0.0  0.325385  0.0  0.0  0.0   \n",
       "1          0.361758  0.0  0.0  0.0 ...   0.0  0.0  0.000000  0.0  0.0  0.0   \n",
       "\n",
       "   196  197  198  199  \n",
       "0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[2 rows x 407 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_train['question1']\n",
    "del X_train['question2']\n",
    "del X_train['qid1']\n",
    "del X_train['qid2']\n",
    "del X_train['id']\n",
    "\n",
    "del X_test['question1']\n",
    "del X_test['question2']\n",
    "del X_test['test_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tmp = abs(X_train.ix[:, 2:202] * X_train.ix[:, -200:])\n",
    "#X_train = pd.concat([X_train, tmp], axis=1, join_axes=[X_train.index])\n",
    "tmp = abs(X_train.ix[:, 2:202] - X_train.ix[:, -200:])\n",
    "X_train = pd.concat([X_train, tmp], axis=1, join_axes=[X_train.index])\n",
    "\n",
    "#tmp = abs(X_test.ix[:, 2:202] * X_test.ix[:, -200:])\n",
    "#X_test = pd.concat([X_test, tmp], axis=1, join_axes=[X_test.index])\n",
    "tmp = abs(X_test.ix[:, 2:202] - X_test.ix[:, -200:])\n",
    "X_test = pd.concat([X_test, tmp], axis=1, join_axes=[X_test.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.657163\teval-logloss:0.657517\n",
      "[15]\ttrain-logloss:0.464538\teval-logloss:0.468822\n",
      "[30]\ttrain-logloss:0.426281\teval-logloss:0.433582\n",
      "[45]\ttrain-logloss:0.412202\teval-logloss:0.421338\n",
      "[60]\ttrain-logloss:0.402868\teval-logloss:0.413885\n",
      "[75]\ttrain-logloss:0.39544\teval-logloss:0.408272\n",
      "[90]\ttrain-logloss:0.389643\teval-logloss:0.404072\n",
      "[105]\ttrain-logloss:0.384249\teval-logloss:0.400345\n",
      "[120]\ttrain-logloss:0.379075\teval-logloss:0.396732\n",
      "[135]\ttrain-logloss:0.374311\teval-logloss:0.393599\n",
      "[150]\ttrain-logloss:0.37043\teval-logloss:0.391306\n",
      "[165]\ttrain-logloss:0.366284\teval-logloss:0.388769\n",
      "[180]\ttrain-logloss:0.362379\teval-logloss:0.386283\n",
      "[195]\ttrain-logloss:0.359467\teval-logloss:0.384551\n",
      "[210]\ttrain-logloss:0.356178\teval-logloss:0.382432\n",
      "[225]\ttrain-logloss:0.353406\teval-logloss:0.380911\n",
      "[240]\ttrain-logloss:0.350421\teval-logloss:0.379203\n",
      "[255]\ttrain-logloss:0.348166\teval-logloss:0.378041\n",
      "[270]\ttrain-logloss:0.346012\teval-logloss:0.376838\n",
      "[285]\ttrain-logloss:0.34327\teval-logloss:0.375429\n",
      "[300]\ttrain-logloss:0.341308\teval-logloss:0.374367\n",
      "[315]\ttrain-logloss:0.338839\teval-logloss:0.373077\n",
      "[330]\ttrain-logloss:0.336927\teval-logloss:0.372124\n",
      "[345]\ttrain-logloss:0.335317\teval-logloss:0.371329\n",
      "[360]\ttrain-logloss:0.333622\teval-logloss:0.37059\n",
      "[375]\ttrain-logloss:0.331789\teval-logloss:0.369662\n",
      "[390]\ttrain-logloss:0.329873\teval-logloss:0.368797\n",
      "[405]\ttrain-logloss:0.327929\teval-logloss:0.367887\n",
      "[420]\ttrain-logloss:0.326625\teval-logloss:0.367343\n",
      "[435]\ttrain-logloss:0.325147\teval-logloss:0.366714\n",
      "[450]\ttrain-logloss:0.323607\teval-logloss:0.366123\n",
      "[465]\ttrain-logloss:0.321958\teval-logloss:0.36538\n",
      "[480]\ttrain-logloss:0.320678\teval-logloss:0.364853\n",
      "[495]\ttrain-logloss:0.31911\teval-logloss:0.364194\n",
      "[510]\ttrain-logloss:0.317923\teval-logloss:0.363703\n",
      "[525]\ttrain-logloss:0.316336\teval-logloss:0.363091\n",
      "[540]\ttrain-logloss:0.314872\teval-logloss:0.362476\n",
      "[555]\ttrain-logloss:0.313292\teval-logloss:0.361863\n",
      "[570]\ttrain-logloss:0.312215\teval-logloss:0.361422\n",
      "[585]\ttrain-logloss:0.311236\teval-logloss:0.36105\n",
      "[600]\ttrain-logloss:0.310283\teval-logloss:0.360703\n",
      "[615]\ttrain-logloss:0.309284\teval-logloss:0.360339\n",
      "[630]\ttrain-logloss:0.308186\teval-logloss:0.359894\n",
      "[645]\ttrain-logloss:0.306695\teval-logloss:0.359192\n",
      "[660]\ttrain-logloss:0.305498\teval-logloss:0.358757\n",
      "[675]\ttrain-logloss:0.304485\teval-logloss:0.358314\n",
      "[690]\ttrain-logloss:0.303664\teval-logloss:0.358038\n",
      "[705]\ttrain-logloss:0.302474\teval-logloss:0.357615\n",
      "[720]\ttrain-logloss:0.301129\teval-logloss:0.357016\n",
      "[735]\ttrain-logloss:0.300112\teval-logloss:0.356689\n",
      "[750]\ttrain-logloss:0.299238\teval-logloss:0.356377\n",
      "[765]\ttrain-logloss:0.298336\teval-logloss:0.356127\n",
      "[780]\ttrain-logloss:0.297336\teval-logloss:0.355724\n",
      "[795]\ttrain-logloss:0.296551\teval-logloss:0.355451\n",
      "[810]\ttrain-logloss:0.295329\teval-logloss:0.354973\n",
      "[825]\ttrain-logloss:0.294366\teval-logloss:0.354624\n",
      "[840]\ttrain-logloss:0.293437\teval-logloss:0.354304\n",
      "[855]\ttrain-logloss:0.292539\teval-logloss:0.354026\n",
      "[870]\ttrain-logloss:0.291586\teval-logloss:0.353671\n",
      "[885]\ttrain-logloss:0.290877\teval-logloss:0.353445\n",
      "[900]\ttrain-logloss:0.290158\teval-logloss:0.353262\n",
      "[915]\ttrain-logloss:0.289453\teval-logloss:0.353044\n",
      "[930]\ttrain-logloss:0.288603\teval-logloss:0.3528\n",
      "[945]\ttrain-logloss:0.28792\teval-logloss:0.352564\n",
      "[960]\ttrain-logloss:0.286691\teval-logloss:0.352173\n",
      "[975]\ttrain-logloss:0.285924\teval-logloss:0.351945\n",
      "[990]\ttrain-logloss:0.284712\teval-logloss:0.351506\n",
      "[1005]\ttrain-logloss:0.283884\teval-logloss:0.351227\n",
      "[1020]\ttrain-logloss:0.282793\teval-logloss:0.350802\n",
      "[1035]\ttrain-logloss:0.281471\teval-logloss:0.350375\n",
      "[1050]\ttrain-logloss:0.280752\teval-logloss:0.350127\n",
      "[1065]\ttrain-logloss:0.279453\teval-logloss:0.349629\n",
      "[1080]\ttrain-logloss:0.278671\teval-logloss:0.34939\n",
      "[1095]\ttrain-logloss:0.277982\teval-logloss:0.349159\n",
      "[0]\ttrain-logloss:0.657023\teval-logloss:0.65735\n",
      "[15]\ttrain-logloss:0.464176\teval-logloss:0.467216\n",
      "[30]\ttrain-logloss:0.426525\teval-logloss:0.431935\n",
      "[45]\ttrain-logloss:0.412704\teval-logloss:0.419716\n",
      "[60]\ttrain-logloss:0.402956\teval-logloss:0.411883\n",
      "[75]\ttrain-logloss:0.395158\teval-logloss:0.405979\n",
      "[90]\ttrain-logloss:0.388942\teval-logloss:0.401603\n",
      "[105]\ttrain-logloss:0.382865\teval-logloss:0.397296\n",
      "[120]\ttrain-logloss:0.377784\teval-logloss:0.393874\n",
      "[135]\ttrain-logloss:0.373869\teval-logloss:0.391316\n",
      "[150]\ttrain-logloss:0.370213\teval-logloss:0.389071\n",
      "[165]\ttrain-logloss:0.366735\teval-logloss:0.387004\n",
      "[180]\ttrain-logloss:0.363163\teval-logloss:0.384909\n",
      "[195]\ttrain-logloss:0.359657\teval-logloss:0.3829\n",
      "[210]\ttrain-logloss:0.356177\teval-logloss:0.380819\n",
      "[225]\ttrain-logloss:0.353037\teval-logloss:0.378948\n",
      "[240]\ttrain-logloss:0.350721\teval-logloss:0.37776\n",
      "[255]\ttrain-logloss:0.347969\teval-logloss:0.376236\n",
      "[270]\ttrain-logloss:0.345741\teval-logloss:0.375115\n",
      "[285]\ttrain-logloss:0.343382\teval-logloss:0.373831\n",
      "[300]\ttrain-logloss:0.341273\teval-logloss:0.372747\n",
      "[315]\ttrain-logloss:0.339058\teval-logloss:0.371689\n",
      "[330]\ttrain-logloss:0.337032\teval-logloss:0.370599\n",
      "[345]\ttrain-logloss:0.334929\teval-logloss:0.36948\n",
      "[360]\ttrain-logloss:0.333129\teval-logloss:0.368639\n",
      "[375]\ttrain-logloss:0.331455\teval-logloss:0.367926\n",
      "[390]\ttrain-logloss:0.329641\teval-logloss:0.367074\n",
      "[405]\ttrain-logloss:0.32815\teval-logloss:0.366438\n",
      "[420]\ttrain-logloss:0.325824\teval-logloss:0.365282\n",
      "[435]\ttrain-logloss:0.324145\teval-logloss:0.364497\n",
      "[450]\ttrain-logloss:0.322666\teval-logloss:0.363891\n",
      "[465]\ttrain-logloss:0.321389\teval-logloss:0.363368\n",
      "[480]\ttrain-logloss:0.319921\teval-logloss:0.362698\n",
      "[495]\ttrain-logloss:0.318613\teval-logloss:0.362128\n",
      "[510]\ttrain-logloss:0.317235\teval-logloss:0.361525\n",
      "[525]\ttrain-logloss:0.315991\teval-logloss:0.361075\n",
      "[540]\ttrain-logloss:0.314742\teval-logloss:0.360626\n",
      "[555]\ttrain-logloss:0.313551\teval-logloss:0.36019\n",
      "[570]\ttrain-logloss:0.311925\teval-logloss:0.359584\n",
      "[585]\ttrain-logloss:0.310534\teval-logloss:0.359062\n",
      "[600]\ttrain-logloss:0.309361\teval-logloss:0.35865\n",
      "[615]\ttrain-logloss:0.308178\teval-logloss:0.358232\n",
      "[630]\ttrain-logloss:0.307058\teval-logloss:0.357739\n",
      "[645]\ttrain-logloss:0.305837\teval-logloss:0.357288\n",
      "[660]\ttrain-logloss:0.304939\teval-logloss:0.356952\n",
      "[675]\ttrain-logloss:0.303648\teval-logloss:0.356444\n",
      "[690]\ttrain-logloss:0.302678\teval-logloss:0.356129\n",
      "[705]\ttrain-logloss:0.301259\teval-logloss:0.355552\n",
      "[720]\ttrain-logloss:0.300317\teval-logloss:0.355221\n",
      "[735]\ttrain-logloss:0.299333\teval-logloss:0.354839\n",
      "[750]\ttrain-logloss:0.298191\teval-logloss:0.354356\n",
      "[765]\ttrain-logloss:0.297068\teval-logloss:0.353924\n",
      "[780]\ttrain-logloss:0.295953\teval-logloss:0.353579\n",
      "[795]\ttrain-logloss:0.294576\teval-logloss:0.353018\n",
      "[810]\ttrain-logloss:0.2936\teval-logloss:0.352688\n",
      "[825]\ttrain-logloss:0.292551\teval-logloss:0.352304\n",
      "[840]\ttrain-logloss:0.29173\teval-logloss:0.352096\n",
      "[855]\ttrain-logloss:0.290928\teval-logloss:0.351864\n",
      "[870]\ttrain-logloss:0.289961\teval-logloss:0.35162\n",
      "[885]\ttrain-logloss:0.289173\teval-logloss:0.351468\n",
      "[900]\ttrain-logloss:0.288557\teval-logloss:0.351274\n",
      "[915]\ttrain-logloss:0.287742\teval-logloss:0.351034\n",
      "[930]\ttrain-logloss:0.286994\teval-logloss:0.350793\n",
      "[945]\ttrain-logloss:0.286265\teval-logloss:0.350572\n",
      "[960]\ttrain-logloss:0.284804\teval-logloss:0.349983\n",
      "[975]\ttrain-logloss:0.283859\teval-logloss:0.349655\n",
      "[990]\ttrain-logloss:0.283092\teval-logloss:0.3494\n",
      "[1005]\ttrain-logloss:0.282071\teval-logloss:0.349062\n",
      "[1020]\ttrain-logloss:0.281248\teval-logloss:0.34881\n",
      "[1035]\ttrain-logloss:0.280689\teval-logloss:0.348707\n",
      "[1050]\ttrain-logloss:0.279622\teval-logloss:0.348327\n",
      "[1065]\ttrain-logloss:0.278914\teval-logloss:0.348122\n",
      "[1080]\ttrain-logloss:0.27813\teval-logloss:0.347861\n",
      "[1095]\ttrain-logloss:0.277271\teval-logloss:0.347584\n",
      "[0]\ttrain-logloss:0.65687\teval-logloss:0.657183\n",
      "[15]\ttrain-logloss:0.464003\teval-logloss:0.467714\n",
      "[30]\ttrain-logloss:0.426353\teval-logloss:0.432522\n",
      "[45]\ttrain-logloss:0.412417\teval-logloss:0.420549\n",
      "[60]\ttrain-logloss:0.402478\teval-logloss:0.412666\n",
      "[75]\ttrain-logloss:0.395521\teval-logloss:0.407597\n",
      "[90]\ttrain-logloss:0.389117\teval-logloss:0.403126\n",
      "[105]\ttrain-logloss:0.383888\teval-logloss:0.399622\n",
      "[120]\ttrain-logloss:0.378849\teval-logloss:0.396145\n",
      "[135]\ttrain-logloss:0.374518\teval-logloss:0.39337\n",
      "[150]\ttrain-logloss:0.370984\teval-logloss:0.391201\n",
      "[165]\ttrain-logloss:0.367031\teval-logloss:0.388651\n",
      "[180]\ttrain-logloss:0.36282\teval-logloss:0.385946\n",
      "[195]\ttrain-logloss:0.359719\teval-logloss:0.384342\n",
      "[210]\ttrain-logloss:0.356551\teval-logloss:0.382595\n",
      "[225]\ttrain-logloss:0.353335\teval-logloss:0.380626\n",
      "[240]\ttrain-logloss:0.350761\teval-logloss:0.379205\n",
      "[255]\ttrain-logloss:0.34833\teval-logloss:0.37792\n",
      "[270]\ttrain-logloss:0.346018\teval-logloss:0.376691\n",
      "[285]\ttrain-logloss:0.343696\teval-logloss:0.375516\n",
      "[300]\ttrain-logloss:0.341616\teval-logloss:0.374482\n",
      "[315]\ttrain-logloss:0.339617\teval-logloss:0.373556\n",
      "[330]\ttrain-logloss:0.337523\teval-logloss:0.372498\n",
      "[345]\ttrain-logloss:0.335672\teval-logloss:0.371665\n",
      "[360]\ttrain-logloss:0.334016\teval-logloss:0.370914\n",
      "[375]\ttrain-logloss:0.332299\teval-logloss:0.37\n",
      "[390]\ttrain-logloss:0.330748\teval-logloss:0.369162\n",
      "[405]\ttrain-logloss:0.329071\teval-logloss:0.368378\n",
      "[420]\ttrain-logloss:0.327495\teval-logloss:0.367716\n",
      "[435]\ttrain-logloss:0.32586\teval-logloss:0.366985\n",
      "[450]\ttrain-logloss:0.324094\teval-logloss:0.366167\n",
      "[465]\ttrain-logloss:0.322635\teval-logloss:0.365479\n",
      "[480]\ttrain-logloss:0.321306\teval-logloss:0.364846\n",
      "[495]\ttrain-logloss:0.320042\teval-logloss:0.364395\n",
      "[510]\ttrain-logloss:0.318478\teval-logloss:0.363714\n",
      "[525]\ttrain-logloss:0.316996\teval-logloss:0.363077\n",
      "[540]\ttrain-logloss:0.316015\teval-logloss:0.36274\n",
      "[555]\ttrain-logloss:0.314325\teval-logloss:0.361932\n",
      "[570]\ttrain-logloss:0.313238\teval-logloss:0.361516\n",
      "[585]\ttrain-logloss:0.31199\teval-logloss:0.361038\n",
      "[600]\ttrain-logloss:0.310699\teval-logloss:0.360423\n",
      "[615]\ttrain-logloss:0.309436\teval-logloss:0.360073\n",
      "[630]\ttrain-logloss:0.308131\teval-logloss:0.359493\n",
      "[645]\ttrain-logloss:0.306913\teval-logloss:0.358989\n",
      "[660]\ttrain-logloss:0.305524\teval-logloss:0.358279\n",
      "[675]\ttrain-logloss:0.304184\teval-logloss:0.357738\n",
      "[690]\ttrain-logloss:0.303347\teval-logloss:0.3574\n",
      "[705]\ttrain-logloss:0.301971\teval-logloss:0.35685\n",
      "[720]\ttrain-logloss:0.30099\teval-logloss:0.356516\n",
      "[735]\ttrain-logloss:0.299802\teval-logloss:0.356108\n",
      "[750]\ttrain-logloss:0.298687\teval-logloss:0.355791\n",
      "[765]\ttrain-logloss:0.297888\teval-logloss:0.355519\n",
      "[780]\ttrain-logloss:0.296927\teval-logloss:0.355123\n",
      "[795]\ttrain-logloss:0.295935\teval-logloss:0.354791\n",
      "[810]\ttrain-logloss:0.294737\teval-logloss:0.354374\n",
      "[825]\ttrain-logloss:0.293776\teval-logloss:0.354095\n",
      "[840]\ttrain-logloss:0.292703\teval-logloss:0.353734\n",
      "[855]\ttrain-logloss:0.291779\teval-logloss:0.353528\n",
      "[870]\ttrain-logloss:0.290796\teval-logloss:0.353201\n",
      "[885]\ttrain-logloss:0.28965\teval-logloss:0.352825\n",
      "[900]\ttrain-logloss:0.288874\teval-logloss:0.352542\n",
      "[915]\ttrain-logloss:0.288\teval-logloss:0.352253\n",
      "[930]\ttrain-logloss:0.287081\teval-logloss:0.351984\n",
      "[945]\ttrain-logloss:0.286178\teval-logloss:0.351671\n",
      "[960]\ttrain-logloss:0.285249\teval-logloss:0.351372\n",
      "[975]\ttrain-logloss:0.284269\teval-logloss:0.350942\n",
      "[990]\ttrain-logloss:0.283548\teval-logloss:0.350721\n",
      "[1005]\ttrain-logloss:0.282997\teval-logloss:0.350572\n",
      "[1020]\ttrain-logloss:0.28224\teval-logloss:0.350379\n",
      "[1035]\ttrain-logloss:0.281538\teval-logloss:0.35017\n",
      "[1050]\ttrain-logloss:0.280548\teval-logloss:0.349889\n",
      "[1065]\ttrain-logloss:0.279619\teval-logloss:0.349566\n",
      "[1080]\ttrain-logloss:0.27888\teval-logloss:0.349351\n",
      "[1095]\ttrain-logloss:0.278068\teval-logloss:0.34912\n",
      "[0]\ttrain-logloss:0.657156\teval-logloss:0.657064\n",
      "[15]\ttrain-logloss:0.464898\teval-logloss:0.465432\n",
      "[30]\ttrain-logloss:0.427512\teval-logloss:0.429404\n",
      "[45]\ttrain-logloss:0.413118\teval-logloss:0.416486\n",
      "[60]\ttrain-logloss:0.403911\teval-logloss:0.409022\n",
      "[75]\ttrain-logloss:0.396698\teval-logloss:0.4035\n",
      "[90]\ttrain-logloss:0.390473\teval-logloss:0.398883\n",
      "[105]\ttrain-logloss:0.385106\teval-logloss:0.395059\n",
      "[120]\ttrain-logloss:0.38029\teval-logloss:0.391804\n",
      "[135]\ttrain-logloss:0.37577\teval-logloss:0.388659\n",
      "[150]\ttrain-logloss:0.372021\teval-logloss:0.386226\n",
      "[165]\ttrain-logloss:0.368446\teval-logloss:0.383995\n",
      "[180]\ttrain-logloss:0.365199\teval-logloss:0.38189\n",
      "[195]\ttrain-logloss:0.361592\teval-logloss:0.379695\n",
      "[210]\ttrain-logloss:0.358393\teval-logloss:0.377804\n",
      "[225]\ttrain-logloss:0.355779\teval-logloss:0.376417\n",
      "[240]\ttrain-logloss:0.353144\teval-logloss:0.374972\n",
      "[255]\ttrain-logloss:0.350265\teval-logloss:0.37334\n",
      "[270]\ttrain-logloss:0.348098\teval-logloss:0.372225\n",
      "[285]\ttrain-logloss:0.345713\teval-logloss:0.370895\n",
      "[300]\ttrain-logloss:0.343573\teval-logloss:0.369703\n",
      "[315]\ttrain-logloss:0.341471\teval-logloss:0.368616\n",
      "[330]\ttrain-logloss:0.339371\teval-logloss:0.367516\n",
      "[345]\ttrain-logloss:0.337561\teval-logloss:0.366615\n",
      "[360]\ttrain-logloss:0.335473\teval-logloss:0.365573\n",
      "[375]\ttrain-logloss:0.333708\teval-logloss:0.364701\n",
      "[390]\ttrain-logloss:0.331501\teval-logloss:0.363621\n",
      "[405]\ttrain-logloss:0.329693\teval-logloss:0.362656\n",
      "[420]\ttrain-logloss:0.328065\teval-logloss:0.361904\n",
      "[435]\ttrain-logloss:0.326597\teval-logloss:0.361216\n",
      "[450]\ttrain-logloss:0.325277\teval-logloss:0.360563\n",
      "[465]\ttrain-logloss:0.323813\teval-logloss:0.35991\n",
      "[480]\ttrain-logloss:0.322396\teval-logloss:0.359351\n",
      "[495]\ttrain-logloss:0.321056\teval-logloss:0.358716\n",
      "[510]\ttrain-logloss:0.319789\teval-logloss:0.358217\n",
      "[525]\ttrain-logloss:0.318491\teval-logloss:0.357754\n",
      "[540]\ttrain-logloss:0.317361\teval-logloss:0.357338\n",
      "[555]\ttrain-logloss:0.316035\teval-logloss:0.35676\n",
      "[570]\ttrain-logloss:0.314872\teval-logloss:0.356363\n",
      "[585]\ttrain-logloss:0.313833\teval-logloss:0.355983\n",
      "[600]\ttrain-logloss:0.312452\teval-logloss:0.355333\n",
      "[615]\ttrain-logloss:0.311023\teval-logloss:0.354765\n",
      "[630]\ttrain-logloss:0.310066\teval-logloss:0.354349\n",
      "[645]\ttrain-logloss:0.308768\teval-logloss:0.353886\n",
      "[660]\ttrain-logloss:0.3075\teval-logloss:0.353389\n",
      "[675]\ttrain-logloss:0.306768\teval-logloss:0.353096\n",
      "[690]\ttrain-logloss:0.305247\teval-logloss:0.352421\n",
      "[705]\ttrain-logloss:0.304361\teval-logloss:0.352102\n",
      "[720]\ttrain-logloss:0.30329\teval-logloss:0.351683\n",
      "[735]\ttrain-logloss:0.302241\teval-logloss:0.3513\n",
      "[750]\ttrain-logloss:0.301316\teval-logloss:0.350995\n",
      "[765]\ttrain-logloss:0.300363\teval-logloss:0.350692\n",
      "[780]\ttrain-logloss:0.299413\teval-logloss:0.350345\n",
      "[795]\ttrain-logloss:0.298348\teval-logloss:0.349971\n",
      "[810]\ttrain-logloss:0.297429\teval-logloss:0.349642\n",
      "[825]\ttrain-logloss:0.296081\teval-logloss:0.349133\n",
      "[840]\ttrain-logloss:0.295128\teval-logloss:0.348815\n",
      "[855]\ttrain-logloss:0.294272\teval-logloss:0.348543\n",
      "[870]\ttrain-logloss:0.29344\teval-logloss:0.348307\n",
      "[885]\ttrain-logloss:0.292484\teval-logloss:0.347958\n",
      "[900]\ttrain-logloss:0.291898\teval-logloss:0.347796\n",
      "[915]\ttrain-logloss:0.290845\teval-logloss:0.347479\n",
      "[930]\ttrain-logloss:0.289929\teval-logloss:0.34716\n",
      "[945]\ttrain-logloss:0.289264\teval-logloss:0.346978\n",
      "[960]\ttrain-logloss:0.287692\teval-logloss:0.34634\n",
      "[975]\ttrain-logloss:0.286791\teval-logloss:0.346051\n",
      "[990]\ttrain-logloss:0.286073\teval-logloss:0.345789\n",
      "[1005]\ttrain-logloss:0.284894\teval-logloss:0.345447\n",
      "[1020]\ttrain-logloss:0.284163\teval-logloss:0.345244\n",
      "[1035]\ttrain-logloss:0.283473\teval-logloss:0.345015\n",
      "[1050]\ttrain-logloss:0.282614\teval-logloss:0.344747\n",
      "[1065]\ttrain-logloss:0.281623\teval-logloss:0.344449\n",
      "[1080]\ttrain-logloss:0.281027\teval-logloss:0.344311\n",
      "[1095]\ttrain-logloss:0.280435\teval-logloss:0.344181\n",
      "[0]\ttrain-logloss:0.65699\teval-logloss:0.657036\n",
      "[15]\ttrain-logloss:0.464509\teval-logloss:0.466358\n",
      "[30]\ttrain-logloss:0.426754\teval-logloss:0.4307\n",
      "[45]\ttrain-logloss:0.412818\teval-logloss:0.418216\n",
      "[60]\ttrain-logloss:0.403288\teval-logloss:0.410433\n",
      "[75]\ttrain-logloss:0.396131\teval-logloss:0.404977\n",
      "[90]\ttrain-logloss:0.390157\teval-logloss:0.400822\n",
      "[105]\ttrain-logloss:0.384106\teval-logloss:0.396317\n",
      "[120]\ttrain-logloss:0.379584\teval-logloss:0.393141\n",
      "[135]\ttrain-logloss:0.375334\teval-logloss:0.390371\n",
      "[150]\ttrain-logloss:0.371198\teval-logloss:0.387674\n",
      "[165]\ttrain-logloss:0.367148\teval-logloss:0.385018\n",
      "[180]\ttrain-logloss:0.363948\teval-logloss:0.383046\n",
      "[195]\ttrain-logloss:0.360736\teval-logloss:0.381096\n",
      "[210]\ttrain-logloss:0.357888\teval-logloss:0.379556\n",
      "[225]\ttrain-logloss:0.354937\teval-logloss:0.377808\n",
      "[240]\ttrain-logloss:0.352243\teval-logloss:0.376352\n",
      "[255]\ttrain-logloss:0.349629\teval-logloss:0.374908\n",
      "[270]\ttrain-logloss:0.347199\teval-logloss:0.373661\n",
      "[285]\ttrain-logloss:0.344931\teval-logloss:0.372525\n",
      "[300]\ttrain-logloss:0.34287\teval-logloss:0.371521\n",
      "[315]\ttrain-logloss:0.340505\teval-logloss:0.370334\n",
      "[330]\ttrain-logloss:0.338667\teval-logloss:0.369459\n",
      "[345]\ttrain-logloss:0.336694\teval-logloss:0.368539\n",
      "[360]\ttrain-logloss:0.334904\teval-logloss:0.367636\n",
      "[375]\ttrain-logloss:0.33316\teval-logloss:0.366726\n",
      "[390]\ttrain-logloss:0.331269\teval-logloss:0.365867\n",
      "[405]\ttrain-logloss:0.329688\teval-logloss:0.365105\n",
      "[420]\ttrain-logloss:0.327965\teval-logloss:0.364366\n",
      "[435]\ttrain-logloss:0.326638\teval-logloss:0.363825\n",
      "[450]\ttrain-logloss:0.325138\teval-logloss:0.363135\n",
      "[465]\ttrain-logloss:0.323751\teval-logloss:0.362496\n",
      "[480]\ttrain-logloss:0.322073\teval-logloss:0.361802\n",
      "[495]\ttrain-logloss:0.320445\teval-logloss:0.361121\n",
      "[510]\ttrain-logloss:0.318879\teval-logloss:0.360322\n",
      "[525]\ttrain-logloss:0.317915\teval-logloss:0.359916\n",
      "[540]\ttrain-logloss:0.316654\teval-logloss:0.359491\n",
      "[555]\ttrain-logloss:0.315333\teval-logloss:0.358933\n",
      "[570]\ttrain-logloss:0.31411\teval-logloss:0.358475\n",
      "[585]\ttrain-logloss:0.312801\teval-logloss:0.358016\n",
      "[600]\ttrain-logloss:0.31158\teval-logloss:0.357542\n",
      "[615]\ttrain-logloss:0.310397\teval-logloss:0.357094\n",
      "[630]\ttrain-logloss:0.309386\teval-logloss:0.356747\n",
      "[645]\ttrain-logloss:0.308577\teval-logloss:0.356501\n",
      "[660]\ttrain-logloss:0.307489\teval-logloss:0.35612\n",
      "[675]\ttrain-logloss:0.306682\teval-logloss:0.355784\n",
      "[690]\ttrain-logloss:0.30542\teval-logloss:0.355305\n",
      "[705]\ttrain-logloss:0.304311\teval-logloss:0.354894\n",
      "[720]\ttrain-logloss:0.303307\teval-logloss:0.354538\n",
      "[735]\ttrain-logloss:0.302258\teval-logloss:0.354164\n",
      "[750]\ttrain-logloss:0.300926\teval-logloss:0.353641\n",
      "[765]\ttrain-logloss:0.299639\teval-logloss:0.353154\n",
      "[780]\ttrain-logloss:0.298352\teval-logloss:0.352568\n",
      "[795]\ttrain-logloss:0.297562\teval-logloss:0.352339\n",
      "[810]\ttrain-logloss:0.296741\teval-logloss:0.352118\n",
      "[825]\ttrain-logloss:0.295765\teval-logloss:0.351821\n",
      "[840]\ttrain-logloss:0.294842\teval-logloss:0.35153\n",
      "[855]\ttrain-logloss:0.293967\teval-logloss:0.351167\n",
      "[870]\ttrain-logloss:0.293243\teval-logloss:0.350943\n",
      "[885]\ttrain-logloss:0.2915\teval-logloss:0.350211\n",
      "[900]\ttrain-logloss:0.290354\teval-logloss:0.349779\n",
      "[915]\ttrain-logloss:0.2897\teval-logloss:0.349621\n",
      "[930]\ttrain-logloss:0.288649\teval-logloss:0.349181\n",
      "[945]\ttrain-logloss:0.287795\teval-logloss:0.348878\n",
      "[960]\ttrain-logloss:0.286953\teval-logloss:0.348525\n",
      "[975]\ttrain-logloss:0.286219\teval-logloss:0.348345\n",
      "[990]\ttrain-logloss:0.285373\teval-logloss:0.348096\n",
      "[1005]\ttrain-logloss:0.284617\teval-logloss:0.347928\n",
      "[1020]\ttrain-logloss:0.283839\teval-logloss:0.347662\n",
      "[1035]\ttrain-logloss:0.282989\teval-logloss:0.34749\n",
      "[1050]\ttrain-logloss:0.281801\teval-logloss:0.347104\n",
      "[1065]\ttrain-logloss:0.28078\teval-logloss:0.346812\n",
      "[1080]\ttrain-logloss:0.280172\teval-logloss:0.346654\n",
      "[1095]\ttrain-logloss:0.279338\teval-logloss:0.346375\n",
      "XGB: 0.347 +- 0.002\n"
     ]
    }
   ],
   "source": [
    "param = {}\n",
    "param['objective'] = 'binary:logistic'\n",
    "param['eval_metric'] = 'logloss'\n",
    "param['max_depth'] = 7\n",
    "# param['booster'] = 'dart'\n",
    "#param['booster'] = 'gblinear'\n",
    "param['eta'] = 0.1\n",
    "#param['subsample'] = 1.0\n",
    "# param[\"scale_pos_weight\"] = 0.5\n",
    "#param['colsample_bytree'] = 0.7\n",
    "param['nthread'] = 8\n",
    "#param['num_class'] =3\n",
    "#param['alpha'] = 0.00\n",
    "#param['lambda_bias'] = 0.1\n",
    "# param['lambda'] = 0\n",
    "#param['min_child_weight'] = 100\n",
    "\n",
    "# if param['booster'] == 'gblinear': param['eta'] *= 1e-4\n",
    "numround = 1100\n",
    "\n",
    "#LGBM\n",
    "params = {\n",
    "    #'task': 'train',\n",
    "    #'boosting_type': 'gbdt',\n",
    "    #'objective': 'regression',\n",
    "    #'metric': {'l2', 'auc'},\n",
    "    #'num_leaves': 31,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    #'subsample': 0.8,\n",
    "    #'colsample_bytree': 0.7,\n",
    "    'objective': 'binary',\n",
    "    'nthread': -1,\n",
    "    'learning_rate': 0.1,\n",
    "    'min_child_weight': 5,\n",
    "    #'max_depth': 6\n",
    "    #'num_class': 3\n",
    "    \n",
    "}\n",
    "\n",
    "pred_train = np.zeros(len(y_train))\n",
    "xgbs = []\n",
    "sc,sc_mean = [],[]\n",
    "\n",
    "for itr, ite in folds:\n",
    "    ypred = []\n",
    "    for i in range(1):\n",
    "               \n",
    "        param['seed'] = i+1\n",
    "        \n",
    "        #xgboost\n",
    "        \n",
    "        Xdatatrain = xgb.DMatrix(data=X_train.ix[itr, :].values,\n",
    "                                     label=y_train.ix[itr].values)\n",
    "        Xdataval = xgb.DMatrix(data=X_train.ix[ite, :].values,\n",
    "                                     label=y_train.ix[ite].values)\n",
    "\n",
    "        plst = list(param.items())\n",
    "        watchlist = [(Xdatatrain, 'train'), (Xdataval, 'eval')]\n",
    "        bst = xgb.train(plst, Xdatatrain, numround, evals=watchlist, verbose_eval=15)\n",
    "\n",
    "        \n",
    "        #rc = ensemble.ExtraTreesClassifier(n_estimators=1300, criterion='gini', max_depth=None, n_jobs=-1)\n",
    "        #pred_train[ite] = bst.predict(Xdataval)\n",
    "        #neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "        #neigh.fit(X_train.ix[itr, :], y_train.ix[itr])\n",
    "        ypred = bst.predict(Xdataval)\n",
    "        #ypred = neigh.predict_proba(X_train.ix[ite, :])\n",
    "        xgbs.append(bst)\n",
    "        '''\n",
    "        \n",
    "        # train\n",
    "        lgb = lgbm.sklearn.LGBMClassifier(n_estimators=300, seed=0, **params)\n",
    "        lgb.fit(X_train.ix[itr, :], y_train.ix[itr])\n",
    "        ypred = lgb.predict_proba(X_train.ix[ite, :])[:, 1]\n",
    "\n",
    "        \n",
    "        model.fit(X_train.ix[itr, :].values, y_train_cat[itr],\n",
    "            epochs=60,\n",
    "            batch_size=1000)\n",
    "        ypred = model.predict(X_train.ix[ite, :].values)\n",
    "        '''\n",
    "    #ypred = sum(ypred) / len(ypred) 0.401408 0.392476\n",
    "    pred_train[ite] = ypred\n",
    "    \n",
    "    \n",
    "    sc.append(log_loss(y_train.ix[ite, :], pred_train[ite]))\n",
    "\n",
    "    \n",
    "print('XGB: {:.3f} +- {:.3f}'.format(np.mean(sc), np.std(sc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def foo(x):\n",
    "    a = 0.165 / 0.37\n",
    "    b = (1 - 0.165) / (1 - 0.37) \n",
    "    return  a * x / (a * x + b * (1 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train = foo(pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train = pd.DataFrame(pred_train)\n",
    "pred_train.columns = ['y']\n",
    "pred_train.to_csv(\"stacking/xgb_7.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fao3864/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:112: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/fao3864/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:147: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "lgb = lgbm.sklearn.LGBMClassifier(n_estimators=300, seed=0, **params)\n",
    "lgb.fit(X_train, y_train)\n",
    "test_pred = lgb.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {}\n",
    "param['objective'] = 'binary:logistic'\n",
    "param['eval_metric'] = 'logloss'\n",
    "param['max_depth'] = 7\n",
    "# param['booster'] = 'dart'\n",
    "#param['booster'] = 'gblinear'\n",
    "param['eta'] = 0.1\n",
    "#param['subsample'] = 1.0\n",
    "# param[\"scale_pos_weight\"] = 0.5\n",
    "#param['colsample_bytree'] = 0.7\n",
    "param['nthread'] = 8\n",
    "#param['num_class'] =3\n",
    "#param['alpha'] = 0.00\n",
    "#param['lambda_bias'] = 0.1\n",
    "# param['lambda'] = 0\n",
    "#param['min_child_weight'] = 100\n",
    "\n",
    "# if param['booster'] == 'gblinear': param['eta'] *= 1e-4\n",
    "numround = 1100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xdatatrain = xgb.DMatrix(data=X_train.values,\n",
    "                                     label=y_train.values)\n",
    "Xdataval = xgb.DMatrix(data=X_train.values,\n",
    "                                     label=y_train.values)\n",
    "Xdatatest = xgb.DMatrix(data=X_test.values)\n",
    "\n",
    "\n",
    "plst = list(param.items())\n",
    "watchlist = [(Xdatatrain, 'train'), (Xdataval, 'eval')]\n",
    "bst = xgb.train(plst, Xdatatrain, numround, evals=watchlist, verbose_eval=15)\n",
    "\n",
    "\n",
    "test_pred = bst.predict(Xdatatest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.657056\teval-logloss:0.657056\n",
      "[15]\ttrain-logloss:0.464559\teval-logloss:0.464559\n",
      "[30]\ttrain-logloss:0.426999\teval-logloss:0.426999\n",
      "[45]\ttrain-logloss:0.412911\teval-logloss:0.412911\n",
      "[60]\ttrain-logloss:0.404036\teval-logloss:0.404036\n",
      "[75]\ttrain-logloss:0.396529\teval-logloss:0.396529\n",
      "[90]\ttrain-logloss:0.39033\teval-logloss:0.39033\n",
      "[105]\ttrain-logloss:0.384979\teval-logloss:0.384979\n",
      "[120]\ttrain-logloss:0.380431\teval-logloss:0.380431\n",
      "[135]\ttrain-logloss:0.375948\teval-logloss:0.375948\n",
      "[150]\ttrain-logloss:0.371985\teval-logloss:0.371985\n",
      "[165]\ttrain-logloss:0.368135\teval-logloss:0.368135\n",
      "[180]\ttrain-logloss:0.364906\teval-logloss:0.364906\n",
      "[195]\ttrain-logloss:0.361831\teval-logloss:0.361831\n",
      "[210]\ttrain-logloss:0.358712\teval-logloss:0.358712\n",
      "[225]\ttrain-logloss:0.355959\teval-logloss:0.355959\n",
      "[240]\ttrain-logloss:0.353491\teval-logloss:0.353491\n",
      "[255]\ttrain-logloss:0.350956\teval-logloss:0.350956\n",
      "[270]\ttrain-logloss:0.348754\teval-logloss:0.348754\n",
      "[285]\ttrain-logloss:0.346606\teval-logloss:0.346606\n",
      "[300]\ttrain-logloss:0.344308\teval-logloss:0.344308\n",
      "[315]\ttrain-logloss:0.342359\teval-logloss:0.342359\n",
      "[330]\ttrain-logloss:0.340315\teval-logloss:0.340315\n",
      "[345]\ttrain-logloss:0.33838\teval-logloss:0.33838\n",
      "[360]\ttrain-logloss:0.336568\teval-logloss:0.336568\n",
      "[375]\ttrain-logloss:0.33494\teval-logloss:0.33494\n",
      "[390]\ttrain-logloss:0.333184\teval-logloss:0.333184\n",
      "[405]\ttrain-logloss:0.331723\teval-logloss:0.331723\n",
      "[420]\ttrain-logloss:0.330335\teval-logloss:0.330335\n",
      "[435]\ttrain-logloss:0.329077\teval-logloss:0.329077\n",
      "[450]\ttrain-logloss:0.327135\teval-logloss:0.327135\n",
      "[465]\ttrain-logloss:0.325945\teval-logloss:0.325945\n",
      "[480]\ttrain-logloss:0.324304\teval-logloss:0.324304\n",
      "[495]\ttrain-logloss:0.322919\teval-logloss:0.322919\n",
      "[510]\ttrain-logloss:0.321157\teval-logloss:0.321157\n",
      "[525]\ttrain-logloss:0.319854\teval-logloss:0.319854\n",
      "[540]\ttrain-logloss:0.318889\teval-logloss:0.318889\n",
      "[555]\ttrain-logloss:0.317682\teval-logloss:0.317682\n",
      "[570]\ttrain-logloss:0.316808\teval-logloss:0.316808\n",
      "[585]\ttrain-logloss:0.315803\teval-logloss:0.315803\n",
      "[600]\ttrain-logloss:0.313851\teval-logloss:0.313851\n",
      "[615]\ttrain-logloss:0.312806\teval-logloss:0.312806\n",
      "[630]\ttrain-logloss:0.311895\teval-logloss:0.311895\n",
      "[645]\ttrain-logloss:0.311003\teval-logloss:0.311003\n",
      "[660]\ttrain-logloss:0.309795\teval-logloss:0.309795\n",
      "[675]\ttrain-logloss:0.30855\teval-logloss:0.30855\n",
      "[690]\ttrain-logloss:0.307708\teval-logloss:0.307708\n",
      "[705]\ttrain-logloss:0.307035\teval-logloss:0.307035\n",
      "[720]\ttrain-logloss:0.305846\teval-logloss:0.305846\n",
      "[735]\ttrain-logloss:0.304792\teval-logloss:0.304792\n",
      "[750]\ttrain-logloss:0.304087\teval-logloss:0.304087\n",
      "[765]\ttrain-logloss:0.302748\teval-logloss:0.302748\n",
      "[780]\ttrain-logloss:0.302012\teval-logloss:0.302012\n",
      "[795]\ttrain-logloss:0.300941\teval-logloss:0.300941\n",
      "[810]\ttrain-logloss:0.299946\teval-logloss:0.299946\n",
      "[825]\ttrain-logloss:0.298572\teval-logloss:0.298572\n",
      "[840]\ttrain-logloss:0.297829\teval-logloss:0.297829\n",
      "[855]\ttrain-logloss:0.297046\teval-logloss:0.297046\n",
      "[870]\ttrain-logloss:0.296303\teval-logloss:0.296303\n",
      "[885]\ttrain-logloss:0.295429\teval-logloss:0.295429\n",
      "[900]\ttrain-logloss:0.294776\teval-logloss:0.294776\n",
      "[915]\ttrain-logloss:0.293926\teval-logloss:0.293926\n",
      "[930]\ttrain-logloss:0.293081\teval-logloss:0.293081\n",
      "[945]\ttrain-logloss:0.292178\teval-logloss:0.292178\n",
      "[960]\ttrain-logloss:0.291594\teval-logloss:0.291594\n",
      "[975]\ttrain-logloss:0.290866\teval-logloss:0.290866\n",
      "[990]\ttrain-logloss:0.29022\teval-logloss:0.29022\n",
      "[1005]\ttrain-logloss:0.289307\teval-logloss:0.289307\n",
      "[1020]\ttrain-logloss:0.288488\teval-logloss:0.288488\n",
      "[1035]\ttrain-logloss:0.287776\teval-logloss:0.287776\n",
      "[1050]\ttrain-logloss:0.287082\teval-logloss:0.287082\n",
      "[1065]\ttrain-logloss:0.286511\teval-logloss:0.286511\n",
      "[1080]\ttrain-logloss:0.285711\teval-logloss:0.285711\n",
      "[1095]\ttrain-logloss:0.284736\teval-logloss:0.284736\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(plst, Xdatatrain, numround, evals=watchlist, verbose_eval=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = bst.predict(Xdatatest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = foo(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit['is_duplicate'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit.to_csv(\"stacking/xgb_7_test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.017854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.208719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.446425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.386106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.034602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.745208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.822379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.522575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.010114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.422683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.182815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.070921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.076765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.320507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.325414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.172652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.028081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.016614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.235834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.053829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.026339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.000856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345766</th>\n",
       "      <td>2345766</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345767</th>\n",
       "      <td>2345767</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345768</th>\n",
       "      <td>2345768</td>\n",
       "      <td>0.216007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345769</th>\n",
       "      <td>2345769</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345770</th>\n",
       "      <td>2345770</td>\n",
       "      <td>0.203319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345771</th>\n",
       "      <td>2345771</td>\n",
       "      <td>0.130428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345772</th>\n",
       "      <td>2345772</td>\n",
       "      <td>0.057990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345773</th>\n",
       "      <td>2345773</td>\n",
       "      <td>0.000414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345774</th>\n",
       "      <td>2345774</td>\n",
       "      <td>0.002240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345775</th>\n",
       "      <td>2345775</td>\n",
       "      <td>0.228611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345776</th>\n",
       "      <td>2345776</td>\n",
       "      <td>0.017448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345777</th>\n",
       "      <td>2345777</td>\n",
       "      <td>0.034472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345778</th>\n",
       "      <td>2345778</td>\n",
       "      <td>0.000954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345779</th>\n",
       "      <td>2345779</td>\n",
       "      <td>0.047621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345780</th>\n",
       "      <td>2345780</td>\n",
       "      <td>0.000368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345781</th>\n",
       "      <td>2345781</td>\n",
       "      <td>0.071291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345782</th>\n",
       "      <td>2345782</td>\n",
       "      <td>0.181039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345783</th>\n",
       "      <td>2345783</td>\n",
       "      <td>0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345784</th>\n",
       "      <td>2345784</td>\n",
       "      <td>0.406529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345785</th>\n",
       "      <td>2345785</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345786</th>\n",
       "      <td>2345786</td>\n",
       "      <td>0.000085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345787</th>\n",
       "      <td>2345787</td>\n",
       "      <td>0.000187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345788</th>\n",
       "      <td>2345788</td>\n",
       "      <td>0.011411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345789</th>\n",
       "      <td>2345789</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345790</th>\n",
       "      <td>2345790</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345791</th>\n",
       "      <td>2345791</td>\n",
       "      <td>0.000288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345792</th>\n",
       "      <td>2345792</td>\n",
       "      <td>0.008022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345793</th>\n",
       "      <td>2345793</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345794</th>\n",
       "      <td>2345794</td>\n",
       "      <td>0.108910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345795</th>\n",
       "      <td>2345795</td>\n",
       "      <td>0.406744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2345796 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         test_id  is_duplicate\n",
       "0              0      0.017854\n",
       "1              1      0.208719\n",
       "2              2      0.446425\n",
       "3              3      0.000751\n",
       "4              4      0.386106\n",
       "5              5      0.034602\n",
       "6              6      0.745208\n",
       "7              7      0.822379\n",
       "8              8      0.522575\n",
       "9              9      0.010114\n",
       "10            10      0.422683\n",
       "11            11      0.000209\n",
       "12            12      0.000151\n",
       "13            13      0.182815\n",
       "14            14      0.070921\n",
       "15            15      0.076765\n",
       "16            16      0.000107\n",
       "17            17      0.320507\n",
       "18            18      0.325414\n",
       "19            19      0.172652\n",
       "20            20      0.000125\n",
       "21            21      0.028081\n",
       "22            22      0.000210\n",
       "23            23      0.016614\n",
       "24            24      0.000430\n",
       "25            25      0.235834\n",
       "26            26      0.000152\n",
       "27            27      0.053829\n",
       "28            28      0.026339\n",
       "29            29      0.000856\n",
       "...          ...           ...\n",
       "2345766  2345766      0.000114\n",
       "2345767  2345767      0.000185\n",
       "2345768  2345768      0.216007\n",
       "2345769  2345769      0.000130\n",
       "2345770  2345770      0.203319\n",
       "2345771  2345771      0.130428\n",
       "2345772  2345772      0.057990\n",
       "2345773  2345773      0.000414\n",
       "2345774  2345774      0.002240\n",
       "2345775  2345775      0.228611\n",
       "2345776  2345776      0.017448\n",
       "2345777  2345777      0.034472\n",
       "2345778  2345778      0.000954\n",
       "2345779  2345779      0.047621\n",
       "2345780  2345780      0.000368\n",
       "2345781  2345781      0.071291\n",
       "2345782  2345782      0.181039\n",
       "2345783  2345783      0.000202\n",
       "2345784  2345784      0.406529\n",
       "2345785  2345785      0.000075\n",
       "2345786  2345786      0.000085\n",
       "2345787  2345787      0.000187\n",
       "2345788  2345788      0.011411\n",
       "2345789  2345789      0.000033\n",
       "2345790  2345790      0.000194\n",
       "2345791  2345791      0.000288\n",
       "2345792  2345792      0.008022\n",
       "2345793  2345793      0.000125\n",
       "2345794  2345794      0.108910\n",
       "2345795  2345795      0.406744\n",
       "\n",
       "[2345796 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
