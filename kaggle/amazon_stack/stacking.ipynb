{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fao3864/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation, metrics \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn.matrix as smatrix\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_label_maps(df_train):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    unique_labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
    "\n",
    "    label_map1 = {lbl: idx for idx, lbl in enumerate(unique_labels)}\n",
    "    inv_label_map = {idx: lbl for lbl, idx in label_map1.items()}\n",
    "    return label_map1, inv_label_map\n",
    "\n",
    "df_train = pd.read_csv('../data/train_v2.csv', index_col='image_name')\n",
    "label_map1, inv_label_map = get_label_maps(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train_v2.csv')\n",
    "labels = df_train['tags'].apply(lambda x: x.split(' '))\n",
    "\n",
    "weather_labels = ['clear', 'partly_cloudy', 'haze', 'cloudy']\n",
    "weather = []\n",
    "\n",
    "for i in [l.split(' ') for l in df_train['tags'].values]:\n",
    "    for j in i:\n",
    "        if j in weather_labels:\n",
    "            weather += [j]\n",
    "\n",
    "df_train['weather'] = weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = np.array(['primary', 'artisinal_mine', 'cloudy','blooming','conventional_mine',\n",
    "                       'partly_cloudy','road','water','selective_logging','blow_down',\n",
    "                       'cultivation','agriculture','haze','habitation',\n",
    "                       'slash_burn','bare_ground','clear'])\n",
    "\n",
    "label_map = {l: i for i, l in enumerate(categories)}\n",
    "\n",
    "\n",
    "#labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
    "#label_map = {l: i for i, l in enumerate(labels)}\n",
    "weather_label_map = {l: i for i, l in enumerate(weather_labels)}\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### мои фолды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(4, shuffle=True, random_state=1)\n",
    "folds = []\n",
    "for itr, ite in skf.split(df_train, df_train.weather):\n",
    "    folds += [[itr, ite]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### фолды asanakoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold    \n",
    "#df_train = pd.read_csv('train_v2.csv', index_col='image_name')\n",
    "\n",
    "n_folds = 5\n",
    "as_kf = KFold(n_splits=5, random_state=2002, shuffle=True)\n",
    "#as_folds = list(kfold.split(df_train))\n",
    "\n",
    "as_folds = []\n",
    "for itr, ite in as_kf.split(df_train):\n",
    "    as_folds += [[itr, ite]]\n",
    "\n",
    "#FOLD_ID = 0 # from 0 to 4\n",
    "#assert 0 <= FOLD_ID < n_folds\n",
    "#as_folds = folds[FOLD_ID:FOLD_ID + 1]\n",
    "#for train_index, valid_index in folds:\n",
    "#    ids_train_fold, ids_valid_fold = df_train.index[train_index], df_train.index[valid_index]\n",
    "\t# ids_valid_fold is what you need for each fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in range(1, 5):\n",
    "    df += [pd.read_csv('resnet_101/fold'+str(i)+'/train.csv', header=0)]\n",
    "    df[i-1]['idx'] = folds[i-1][1]\n",
    "tr1 = pd.concat([df[0], df[1], df[2], df[3]], axis=0)\n",
    "tr1 = tr1.sort_values('idx')\n",
    "del tr1['idx']\n",
    "tr1.index = range(len(tr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in range(1, 5):\n",
    "    df += [pd.read_csv('resnet_152/fold'+str(i)+'/train.csv', header=0)]\n",
    "    df[i-1]['idx'] = folds[i-1][1]\n",
    "tr2 = pd.concat([df[0], df[1], df[2], df[3]], axis=0)\n",
    "tr2 = tr2.sort_values('idx')\n",
    "del tr2['idx']\n",
    "tr2.index = range(len(tr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in range(1, 5):\n",
    "    df += [pd.read_csv('VGG16/fold'+str(i)+'/train.csv', header=0)]\n",
    "    df[i-1]['idx'] = folds[i-1][1]\n",
    "tr3 = pd.concat([df[0], df[1], df[2], df[3]], axis=0)\n",
    "tr3 = tr3.sort_values('idx')\n",
    "del tr3['idx']\n",
    "tr3.index = range(len(tr3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40479\n",
      "40479\n",
      "40479\n",
      "40479\n",
      "40479\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(len(np.load('asanakoy/resnet50/train/fold'+str(i)+'.npy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in range(5):\n",
    "    df += [pd.DataFrame(np.load('asanakoy/resnet50/train/fold'+str(i)+'.npy'))]\n",
    "#    df[i]['idx'] = as_folds[i][1]\n",
    "\n",
    "#tr4 = pd.concat([df[0], df[1], df[2], df[3], df[4]], axis=0)\n",
    "\n",
    "#tr4 = (df[0]+ df[1]+ df[2]+ df[3]+ df[4]) / 5\n",
    "\n",
    "#tr4 = tr4.sort_values('idx')\n",
    "#del tr4['idx']\n",
    "#tr4.index = range(len(tr4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tr4 = (df[0]+ df[1]+ df[2]+ df[3]+ df[4]) / 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr4 = pd.concat([df[0].ix[as_folds[0][1]],\n",
    "          df[1].ix[as_folds[1][1]],\n",
    "          df[2].ix[as_folds[2][1]],\n",
    "          df[3].ix[as_folds[3][1]],\n",
    "          df[4].ix[as_folds[4][1]]], axis=0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_tr4 = tr4 * 0\n",
    "for i in label_map:\n",
    "    new_tr4[label_map[i]] = tr4[label_map1[i]]\n",
    "tr4 = pd.DataFrame(new_tr4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr4 = pd.read_csv('asa.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in range(1, 5):\n",
    "    df += [pd.read_csv('VGG19/fold'+str(i)+'/train.csv', header=0)]\n",
    "    df[i-1]['idx'] = folds[i-1][1]\n",
    "tr5 = pd.concat([df[0], df[1], df[2], df[3]], axis=0)\n",
    "tr5 = tr5.sort_values('idx')\n",
    "del tr5['idx']\n",
    "tr5.index = range(len(tr5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in range(1, 5):\n",
    "    df += [pd.read_csv('Inception_v3/fold'+str(i)+'/train.csv', header=0)]\n",
    "    df[i-1]['idx'] = folds[i-1][1]\n",
    "tr6 = pd.concat([df[0], df[1], df[2], df[3]], axis=0)\n",
    "tr6 = tr6.sort_values('idx')\n",
    "del tr6['idx']\n",
    "tr6.index = range(len(tr6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in range(1, 5):\n",
    "    df += [pd.read_csv('weather/fold'+str(i)+'/train.csv', header=0)]\n",
    "    df[i-1]['idx'] = folds[i-1][1]\n",
    "tr7 = pd.concat([df[0], df[1], df[2], df[3]], axis=0)\n",
    "tr7 = tr7.sort_values('idx')\n",
    "del tr7['idx']\n",
    "tr7.index = range(len(tr7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in range(1, 5):\n",
    "    df += [pd.read_csv('VGG19_fail/fold'+str(i)+'/train.csv', header=0)]\n",
    "    df[i-1]['idx'] = folds[i-1][1]\n",
    "tr8 = pd.concat([df[0], df[1], df[2], df[3]], axis=0)\n",
    "tr8 = tr8.sort_values('idx')\n",
    "del tr8['idx']\n",
    "tr8.index = range(len(tr8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr = pd.DataFrame((((tr1+tr2+tr3+tr5+tr6+tr8 ) / 6).values * 0.6+ tr4.values * 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr = pd.DataFrame(((tr1+tr2+tr3+tr5+tr6 ) / 5).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te1 = []\n",
    "for i in range(1, 5):\n",
    "    te1+= [pd.read_csv(\"resnet_101/fold\"+str(i)+\"/test.csv\", header=0)]\n",
    "ave1 = te1[0]*0\n",
    "for i in range(4):\n",
    "    ave1 = ave1 + te1[i]\n",
    "ave1 = ave1 / 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te2 = []\n",
    "for i in range(1, 5):\n",
    "    te2 += [pd.read_csv(\"resnet_152/fold\"+str(i) + \"/test.csv\", header=0)]\n",
    "ave2 = te2[0]*0\n",
    "for i in range(4):\n",
    "    ave2 = ave2 + te2[i]\n",
    "ave2 = ave2 / 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "te3 = []\n",
    "for i in range(1, 5):\n",
    "    te3 += [pd.read_csv(\"VGG16/fold\"+str(i) + \"/test.csv\", header=0)]\n",
    "ave3 = te3[0]*0\n",
    "for i in range(4):\n",
    "    ave3 = ave3 + te3[i]\n",
    "ave3 = ave3 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "te4 = []\n",
    "for i in range(5):\n",
    "    te4 += [pd.DataFrame(np.load(\"asanakoy/resnet50/fold\"+str(i)+\".npy\"))]\n",
    "    tmp = te4[i] * 0\n",
    "    for k in label_map:\n",
    "        tmp[label_map[k]] = te4[i][label_map1[k]] \n",
    "    te4[i] = tmp\n",
    "ave4 = te4[0]*0\n",
    "for i in range(5):\n",
    "    ave4 = ave4 + te4[i]\n",
    "ave4 = ave4 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ave4 = pd.read_csv('asa_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "te5 = []\n",
    "for i in range(1, 5):\n",
    "    te5 += [pd.read_csv(\"VGG19/fold\"+str(i) + \"/test.csv\", header=0)]\n",
    "ave5 = te5[0]*0\n",
    "for i in range(4):\n",
    "    ave5 = ave5 + te5[i]\n",
    "ave5 = ave5 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te6 = []\n",
    "for i in range(1, 5):\n",
    "    te6 += [pd.read_csv(\"Inception_v3/fold\"+str(i) + \"/test.csv\", header=0)]\n",
    "ave6 = te6[0]*0\n",
    "for i in range(4):\n",
    "    ave6 = ave6 + te6[i]\n",
    "ave6 = ave6 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "te7 = []\n",
    "for i in range(1, 5):\n",
    "    te7 += [pd.read_csv(\"weather/fold\"+str(i) + \"/test.csv\", header=0)]\n",
    "ave7 = te7[0]*0\n",
    "for i in range(4):\n",
    "    ave7 = ave7 + te7[i]\n",
    "ave7 = ave7 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te8 = []\n",
    "for i in range(1, 5):\n",
    "    te8 += [pd.read_csv(\"VGG19_fail/fold\"+str(i) + \"/test.csv\", header=0)]\n",
    "ave8 = te8[0]*0\n",
    "for i in range(4):\n",
    "    ave8 = ave8 + te8[i]\n",
    "ave8 = ave8 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ave5 = pd.DataFrame(np.load(\"asanakoy/inception_v3/test.npy\"))\n",
    "tmp = ave5 * 0\n",
    "for k in label_map:\n",
    "    tmp[label_map[k]] = ave5[label_map1[k]] \n",
    "ave5 = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ave6 = pd.DataFrame(np.load(\"asanakoy/VGG19/test.npy\"))\n",
    "tmp = ave6 * 0\n",
    "for k in label_map:\n",
    "    tmp[label_map[k]] = ave6[label_map1[k]] \n",
    "ave6 = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ave = (ave1+ave2+ave3+ave5+ave6) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blend = pd.DataFrame(ave4.values*0.4 + ave.values*0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## трейн метки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_all = []\n",
    "for f, tags, weather in df_train.values:\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1\n",
    "    y_all += [targets]\n",
    "y_all = pd.DataFrame(y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скоры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### дефолтные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.923940216783\n",
      "0.921190038809\n",
      "0.920554321167\n",
      "0.926291514796\n",
      "0.923354667401\n",
      "0.922885248901\n",
      "0.930319674782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "print(fbeta_score(y_all, np.array(tr1) > 0.2, beta=2, average='samples'))\n",
    "print(fbeta_score(y_all, np.array(tr2) > 0.2, beta=2, average='samples'))\n",
    "print(fbeta_score(y_all, np.array(tr3) > 0.2, beta=2, average='samples'))\n",
    "print(fbeta_score(y_all, np.array(tr4) > 0.2, beta=2, average='samples'))\n",
    "print(fbeta_score(y_all, np.array(tr5) > 0.2, beta=2, average='samples'))\n",
    "print(fbeta_score(y_all, np.array(tr6) > 0.2, beta=2, average='samples'))\n",
    "\n",
    "print(fbeta_score(y_all, np.array(tr) > 0.2, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.923940216783\n",
    "0.921190038809\n",
    "0.920554321167\n",
    "0.926291514796\n",
    "0.923354667401\n",
    "0.922885248901\n",
    "0.930597913205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.930680003768\n",
    "0.930597913205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.930194455376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.929881697222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.928567876838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.927406709985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.923940216783\n",
    "0.921190038809\n",
    "0.920554321167\n",
    "0.926291514796\n",
    "0.929469718062"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### с подбором трешхолдов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_all = [tr1, tr2, tr3, tr4, tr5, tr6,tr8, tr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.931434613195\n"
     ]
    }
   ],
   "source": [
    "q_all = []\n",
    "for j in range(7,8):\n",
    "    ma = 0\n",
    "    q=[0.2]*17\n",
    "    for t in range(17):\n",
    "        ib = 0.2\n",
    "        for i in np.linspace(0.05, 0.3, 50):\n",
    "            q[t] = i\n",
    "            cur = fbeta_score(y_all, np.greater(np.array(tr_all[j]), q), beta=2, average='samples')\n",
    "            if (cur > ma):\n",
    "                ma = cur\n",
    "                ib = i\n",
    "        q[t] = ib\n",
    "    q_all += [q]\n",
    "    print(ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.931614474571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.931298423122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.925165066646\n",
    "0.922932640406\n",
    "0.921948863122\n",
    "0.927589436182\n",
    "0.930677501976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.925165066646\n",
      "0.922933610579\n",
      "0.921948863122\n",
      "0.927589436182\n",
      "0.930677501976\n"
     ]
    }
   ],
   "source": [
    "#q_all = []\n",
    "for j in range(5):\n",
    "    ma = fbeta_score(y_all, np.greater(np.array(tr_all[j]), q_all[j]), beta=2, average='samples')\n",
    "    #q=[0.2]*17\n",
    "    q = q_all[j]\n",
    "    for t in range(17):\n",
    "        ib = q_all[j][t]\n",
    "        for i in np.linspace(0.05, 0.3, 50):\n",
    "            q[t] = i\n",
    "            cur = fbeta_score(y_all, np.greater(np.array(tr_all[j]), q), beta=2, average='samples')\n",
    "            if (cur > ma):\n",
    "                ma = cur\n",
    "                ib = i\n",
    "        q[t] = ib\n",
    "    q_all[j] = q\n",
    "    print(ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28469387755102038,\n",
       " 0.14693877551020407,\n",
       " 0.12142857142857143,\n",
       " 0.23877551020408161,\n",
       " 0.1877551020408163,\n",
       " 0.2,\n",
       " 0.22346938775510206,\n",
       " 0.21836734693877552,\n",
       " 0.11122448979591837,\n",
       " 0.11632653061224489,\n",
       " 0.19795918367346937,\n",
       " 0.20816326530612245,\n",
       " 0.22346938775510206,\n",
       " 0.15714285714285714,\n",
       " 0.17244897959183675,\n",
       " 0.1520408163265306,\n",
       " 0.25918367346938775]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_all[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold    \n",
    "#df_train = pd.read_csv('train_v2.csv', index_col='image_name')\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=10, shuffle=True)\n",
    "cv_folds = []\n",
    "for itr, ite in kf.split(df_train):\n",
    "    cv_folds += [[itr, ite]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## модель второго уровня"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_conc = pd.concat([tr1, tr2, tr3, tr5, tr6, tr7, tr8], axis=1)\n",
    "te_conc = pd.concat([ave1, ave2, ave3, ave5, ave6, ave7, ave8], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.931281555493\n",
      "0.927431859689\n",
      "0.92974221138\n",
      "0.929242122832\n",
      "0.928508875515\n",
      "XGB: 0.9292413 +- 0.0012825\n"
     ]
    }
   ],
   "source": [
    "pred_train = np.zeros((len(y_all), 17))\n",
    "sc,sc_mean = [],[]\n",
    "\n",
    "params = {\n",
    "    #'task': 'train',\n",
    "    #'boosting_type': 'gbdt',\n",
    "    #'objective': 'regression',\n",
    "    #'metric': {'l2', 'auc'},\n",
    "    #'num_leaves': 31,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    #'subsample': 0.8,\n",
    "    #'colsample_bytree': 0.7,\n",
    "    'objective': 'binary',\n",
    "    'nthread': -1,\n",
    "    'learning_rate': 0.05,\n",
    "    'min_child_weight': 1,\n",
    "    #'max_depth': 7\n",
    "    #'num_class': 3\n",
    "    \n",
    "}\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "for itr, ite in cv_folds:\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    from sklearn.multioutput import MultiOutputClassifier\n",
    "    \n",
    "    # If you want to avoid the OneVsRestClassifier magic switch\n",
    "    # from sklearn.multioutput import MultiOutputClassifier\n",
    "    #clf_multilabel = OneVsRestClassifier(XGBClassifier(max_depth=2,learning_rate=0.1,n_estimators=100,min_child_weight=1))\n",
    "    #clf_multilabel = OneVsRestClassifier(LogisticRegression())\n",
    "    #clf_multilabel = OneVsRestClassifier(ExtraTreesClassifier(n_estimators=500, criterion='gini', max_depth=None))\n",
    "    clf_multilabel = OneVsRestClassifier(lgb.sklearn.LGBMClassifier(n_estimators=150, seed=0, **params))    \n",
    "    clf_multilabel.fit(tr_conc.ix[itr].values, y_all.ix[itr].values)\n",
    "    pred_train[ite] = clf_multilabel.predict_proba(tr_conc.ix[ite].values)\n",
    "    loc_sc = fbeta_score(y_all.ix[ite, :], pred_train[ite] > 0.2, beta=2, average='samples')\n",
    "    print(loc_sc)\n",
    "    sc.append(loc_sc)\n",
    "print('XGB: {:.7f} +- {:.7f}'.format(np.mean(sc), np.std(sc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb\n",
    "0.931281555493\n",
    "0.927431859689\n",
    "0.92974221138\n",
    "0.929242122832\n",
    "0.928508875515\n",
    "XGB: 0.9292413 +- 0.0012825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ext\n",
    "0.931531284908\n",
    "0.928435852382\n",
    "0.929534133088\n",
    "0.92912937538\n",
    "0.928978721867\n",
    "XGB: 0.9295219 +- 0.0010646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "все кроме аса\n",
    "0.932518813701\n",
    "0.929238708513\n",
    "0.930865753919\n",
    "0.930662686451\n",
    "0.930655202032\n",
    "XGB: 0.9307882 +- 0.0010426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.933309758046\n",
    "0.928583782536\n",
    "0.930267854954\n",
    "0.930231368773\n",
    "0.93039906491\n",
    "XGB: 0.9305584 +- 0.0015288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.933309758046"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.932839340317"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.933822510095\n",
    "0.929195652376\n",
    "0.930254248762\n",
    "0.930882339822\n",
    "0.931126247263\n",
    "XGB: 0.9310562 +- 0.0015357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.932848301499\n",
    "0.928869141334\n",
    "0.93039294455\n",
    "0.929984599408\n",
    "0.930869395107\n",
    "XGB: 0.9305929 +- 0.0013073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.932737092693\n",
    "0.929260698142\n",
    "0.928336012382\n",
    "0.929768274658\n",
    "0.930121586382\n",
    "XGB: 0.9300447 +- 0.0014741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.929303430684\n"
     ]
    }
   ],
   "source": [
    "ma = 0\n",
    "q=[0.2]*17\n",
    "for t in range(17):\n",
    "    ib = 0.2\n",
    "    for i in np.linspace(0.05, 0.3, 50):\n",
    "        q[t] = i\n",
    "        cur = fbeta_score(y_all, np.greater(np.array(pred_train), q), beta=2, average='samples')\n",
    "        if (cur > ma):\n",
    "            ma = cur\n",
    "            ib = i\n",
    "    q[t] = ib\n",
    "print(ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.931599919004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.931798389345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_stack = q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21836734693877552,\n",
       " 0.20306122448979591,\n",
       " 0.095918367346938774,\n",
       " 0.27448979591836731,\n",
       " 0.26428571428571429,\n",
       " 0.12653061224489795,\n",
       " 0.19795918367346937,\n",
       " 0.21836734693877552,\n",
       " 0.23367346938775507,\n",
       " 0.22346938775510206,\n",
       " 0.21836734693877552,\n",
       " 0.20816326530612245,\n",
       " 0.18265306122448977,\n",
       " 0.19795918367346937,\n",
       " 0.20306122448979591,\n",
       " 0.16734693877551021,\n",
       " 0.16224489795918368]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.931798389345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.93140019935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.930719642795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92876002648202127"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbeta_score(y_all, np.greater(np.array(pred_train), q), beta=2, average='samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## второй уровень"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst_pred = pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lin_pred = pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ext_pred = pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_pred = pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_pred = pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_pred2 = pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train = (lin_pred*0.14 + bst_pred*0.3+ ext_pred*0.14+lgb_pred*0.14+nn_pred*0.14+nn_pred2*0.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13999999999999999"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.7 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93219020205\n"
     ]
    }
   ],
   "source": [
    "ma = 0\n",
    "q=[0.2]*17\n",
    "for t in range(17):\n",
    "    ib = 0.2\n",
    "    for i in np.linspace(0.05, 0.3, 100):\n",
    "        q[t] = i\n",
    "        cur = fbeta_score(y_all, np.greater(np.array(pred_train), q), beta=2, average='samples')\n",
    "        if (cur > ma):\n",
    "            ma = cur\n",
    "            ib = i\n",
    "    q[t] = ib\n",
    "print(ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.93219020205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.932143915682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.932066260632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.932029635321\n",
    "0.9319594244"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## стекинг + asanakoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr = pd.DataFrame((pred_train * 0.65+ tr4.values * 0.35))\n",
    "#tr = pd.DataFrame(pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.932521112002\n"
     ]
    }
   ],
   "source": [
    "ma = 0\n",
    "q=[0.2]*17\n",
    "for t in range(17):\n",
    "    ib = 0.2\n",
    "    for i in np.linspace(0.01, 0.4, 150):\n",
    "        q[t] = i\n",
    "        cur = fbeta_score(y_all, np.greater(np.array(tr), q), beta=2, average='samples')\n",
    "        if (cur > ma):\n",
    "            ma = cur\n",
    "            ib = i\n",
    "    q[t] = ib\n",
    "print(ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.932521112002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.932390846032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.932459845813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.932396806077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.932263096449"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.932188931342"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.932026601785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.931845707729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27174496644295304,\n",
       " 0.11993288590604027,\n",
       " 0.09375838926174497,\n",
       " 0.19060402684563763,\n",
       " 0.085906040268456385,\n",
       " 0.14087248322147652,\n",
       " 0.19583892617449666,\n",
       " 0.22724832214765103,\n",
       " 0.14087248322147652,\n",
       " 0.075436241610738253,\n",
       " 0.20107382550335573,\n",
       " 0.18798657718120809,\n",
       " 0.19060402684563763,\n",
       " 0.21677852348993293,\n",
       " 0.13825503355704699,\n",
       " 0.17489932885906043,\n",
       " 0.20369127516778526]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27174496644295304,\n",
       " 0.067583892617449667,\n",
       " 0.096375838926174504,\n",
       " 0.29530201342281881,\n",
       " 0.16704697986577183,\n",
       " 0.18536912751677856,\n",
       " 0.21939597315436243,\n",
       " 0.22201342281879197,\n",
       " 0.19322147651006713,\n",
       " 0.10684563758389262,\n",
       " 0.19060402684563763,\n",
       " 0.21677852348993293,\n",
       " 0.19060402684563763,\n",
       " 0.20892617449664433,\n",
       " 0.17228187919463089,\n",
       " 0.15134228187919466,\n",
       " 0.21677852348993293]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fao3864/anaconda3/lib/python3.6/site-packages/keras/models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32383 samples, validate on 8096 samples\n",
      "Epoch 1/250\n",
      "1s - loss: 0.1713 - acc: 0.9417 - val_loss: 0.0984 - val_acc: 0.9663\n",
      "Epoch 2/250\n",
      "1s - loss: 0.1068 - acc: 0.9644 - val_loss: 0.0913 - val_acc: 0.9676\n",
      "Epoch 3/250\n",
      "1s - loss: 0.1005 - acc: 0.9655 - val_loss: 0.0892 - val_acc: 0.9676\n",
      "Epoch 4/250\n",
      "1s - loss: 0.0977 - acc: 0.9659 - val_loss: 0.0871 - val_acc: 0.9681\n",
      "Epoch 5/250\n",
      "1s - loss: 0.0956 - acc: 0.9661 - val_loss: 0.0858 - val_acc: 0.9681\n",
      "Epoch 6/250\n",
      "1s - loss: 0.0945 - acc: 0.9663 - val_loss: 0.0848 - val_acc: 0.9684\n",
      "Epoch 7/250\n",
      "1s - loss: 0.0936 - acc: 0.9663 - val_loss: 0.0848 - val_acc: 0.9682\n",
      "Epoch 8/250\n",
      "1s - loss: 0.0929 - acc: 0.9665 - val_loss: 0.0838 - val_acc: 0.9685\n",
      "Epoch 9/250\n",
      "1s - loss: 0.0923 - acc: 0.9665 - val_loss: 0.0836 - val_acc: 0.9685\n",
      "Epoch 10/250\n",
      "1s - loss: 0.0917 - acc: 0.9665 - val_loss: 0.0839 - val_acc: 0.9681\n",
      "Epoch 11/250\n",
      "1s - loss: 0.0914 - acc: 0.9666 - val_loss: 0.0829 - val_acc: 0.9685\n",
      "Epoch 12/250\n",
      "1s - loss: 0.0906 - acc: 0.9668 - val_loss: 0.0829 - val_acc: 0.9687\n",
      "Epoch 13/250\n",
      "1s - loss: 0.0906 - acc: 0.9667 - val_loss: 0.0826 - val_acc: 0.9686\n",
      "Epoch 14/250\n",
      "1s - loss: 0.0899 - acc: 0.9667 - val_loss: 0.0828 - val_acc: 0.9685\n",
      "Epoch 15/250\n",
      "1s - loss: 0.0900 - acc: 0.9669 - val_loss: 0.0828 - val_acc: 0.9684\n",
      "Epoch 16/250\n",
      "1s - loss: 0.0897 - acc: 0.9668 - val_loss: 0.0825 - val_acc: 0.9687\n",
      "Epoch 17/250\n",
      "1s - loss: 0.0895 - acc: 0.9669 - val_loss: 0.0824 - val_acc: 0.9687\n",
      "Epoch 18/250\n",
      "1s - loss: 0.0890 - acc: 0.9671 - val_loss: 0.0821 - val_acc: 0.9686\n",
      "Epoch 19/250\n",
      "1s - loss: 0.0890 - acc: 0.9669 - val_loss: 0.0826 - val_acc: 0.9683\n",
      "Epoch 20/250\n",
      "1s - loss: 0.0889 - acc: 0.9672 - val_loss: 0.0826 - val_acc: 0.9685\n",
      "Epoch 21/250\n",
      "1s - loss: 0.0887 - acc: 0.9671 - val_loss: 0.0822 - val_acc: 0.9688\n",
      "Epoch 22/250\n",
      "1s - loss: 0.0883 - acc: 0.9672 - val_loss: 0.0823 - val_acc: 0.9683\n",
      "Epoch 23/250\n",
      "1s - loss: 0.0885 - acc: 0.9673 - val_loss: 0.0821 - val_acc: 0.9686\n",
      "Epoch 24/250\n",
      "1s - loss: 0.0879 - acc: 0.9674 - val_loss: 0.0824 - val_acc: 0.9683\n",
      "Epoch 25/250\n",
      "1s - loss: 0.0879 - acc: 0.9674 - val_loss: 0.0821 - val_acc: 0.9686\n",
      "Epoch 26/250\n",
      "1s - loss: 0.0879 - acc: 0.9673 - val_loss: 0.0819 - val_acc: 0.9685\n",
      "Epoch 27/250\n",
      "1s - loss: 0.0877 - acc: 0.9672 - val_loss: 0.0818 - val_acc: 0.9684\n",
      "Epoch 28/250\n",
      "1s - loss: 0.0875 - acc: 0.9675 - val_loss: 0.0824 - val_acc: 0.9684\n",
      "Epoch 29/250\n",
      "1s - loss: 0.0874 - acc: 0.9675 - val_loss: 0.0820 - val_acc: 0.9685\n",
      "Epoch 30/250\n",
      "1s - loss: 0.0873 - acc: 0.9674 - val_loss: 0.0818 - val_acc: 0.9686\n",
      "Epoch 31/250\n",
      "1s - loss: 0.0869 - acc: 0.9674 - val_loss: 0.0817 - val_acc: 0.9687\n",
      "Epoch 32/250\n",
      "1s - loss: 0.0872 - acc: 0.9674 - val_loss: 0.0822 - val_acc: 0.9684\n",
      "Epoch 33/250\n",
      "1s - loss: 0.0868 - acc: 0.9674 - val_loss: 0.0824 - val_acc: 0.9682\n",
      "Epoch 34/250\n",
      "1s - loss: 0.0869 - acc: 0.9676 - val_loss: 0.0819 - val_acc: 0.9686\n",
      "Epoch 35/250\n",
      "1s - loss: 0.0870 - acc: 0.9674 - val_loss: 0.0819 - val_acc: 0.9682\n",
      "Epoch 36/250\n",
      "1s - loss: 0.0868 - acc: 0.9677 - val_loss: 0.0817 - val_acc: 0.9687\n",
      "Epoch 37/250\n",
      "1s - loss: 0.0865 - acc: 0.9677 - val_loss: 0.0821 - val_acc: 0.9685\n",
      "Epoch 38/250\n",
      "1s - loss: 0.0864 - acc: 0.9677 - val_loss: 0.0819 - val_acc: 0.9685\n",
      "Epoch 39/250\n",
      "1s - loss: 0.0864 - acc: 0.9675 - val_loss: 0.0818 - val_acc: 0.9684\n",
      "Epoch 40/250\n",
      "1s - loss: 0.0863 - acc: 0.9676 - val_loss: 0.0821 - val_acc: 0.9685\n",
      "Epoch 41/250\n",
      "1s - loss: 0.0861 - acc: 0.9675 - val_loss: 0.0819 - val_acc: 0.9685\n",
      "Epoch 42/250\n",
      "1s - loss: 0.0861 - acc: 0.9677 - val_loss: 0.0819 - val_acc: 0.9686\n",
      "Epoch 43/250\n",
      "1s - loss: 0.0858 - acc: 0.9676 - val_loss: 0.0816 - val_acc: 0.9689\n",
      "Epoch 44/250\n",
      "1s - loss: 0.0860 - acc: 0.9679 - val_loss: 0.0821 - val_acc: 0.9684\n",
      "Epoch 45/250\n",
      "1s - loss: 0.0859 - acc: 0.9677 - val_loss: 0.0819 - val_acc: 0.9684\n",
      "Epoch 46/250\n",
      "1s - loss: 0.0859 - acc: 0.9678 - val_loss: 0.0821 - val_acc: 0.9684\n",
      "Epoch 47/250\n",
      "1s - loss: 0.0859 - acc: 0.9677 - val_loss: 0.0818 - val_acc: 0.9685\n",
      "Epoch 48/250\n",
      "1s - loss: 0.0855 - acc: 0.9678 - val_loss: 0.0820 - val_acc: 0.9686\n",
      "Epoch 49/250\n",
      "1s - loss: 0.0857 - acc: 0.9678 - val_loss: 0.0816 - val_acc: 0.9684\n",
      "Epoch 50/250\n",
      "1s - loss: 0.0855 - acc: 0.9679 - val_loss: 0.0817 - val_acc: 0.9684\n",
      "Epoch 51/250\n",
      "1s - loss: 0.0853 - acc: 0.9680 - val_loss: 0.0818 - val_acc: 0.9685\n",
      "Epoch 52/250\n",
      "1s - loss: 0.0854 - acc: 0.9680 - val_loss: 0.0819 - val_acc: 0.9682\n",
      "Epoch 53/250\n",
      "1s - loss: 0.0853 - acc: 0.9678 - val_loss: 0.0817 - val_acc: 0.9684\n",
      "Epoch 54/250\n",
      "1s - loss: 0.0852 - acc: 0.9677 - val_loss: 0.0816 - val_acc: 0.9688\n",
      "Epoch 55/250\n",
      "1s - loss: 0.0852 - acc: 0.9679 - val_loss: 0.0816 - val_acc: 0.9685\n",
      "Epoch 56/250\n",
      "1s - loss: 0.0851 - acc: 0.9680 - val_loss: 0.0817 - val_acc: 0.9684\n",
      "Epoch 57/250\n",
      "1s - loss: 0.0853 - acc: 0.9677 - val_loss: 0.0817 - val_acc: 0.9685\n",
      "Epoch 58/250\n",
      "1s - loss: 0.0852 - acc: 0.9681 - val_loss: 0.0818 - val_acc: 0.9685\n",
      "Epoch 59/250\n",
      "1s - loss: 0.0849 - acc: 0.9681 - val_loss: 0.0828 - val_acc: 0.9679\n",
      "Epoch 60/250\n",
      "1s - loss: 0.0850 - acc: 0.9679 - val_loss: 0.0821 - val_acc: 0.9684\n",
      "Epoch 61/250\n",
      "1s - loss: 0.0847 - acc: 0.9680 - val_loss: 0.0820 - val_acc: 0.9683\n",
      "Epoch 62/250\n",
      "1s - loss: 0.0848 - acc: 0.9680 - val_loss: 0.0817 - val_acc: 0.9681\n",
      "Epoch 63/250\n",
      "1s - loss: 0.0848 - acc: 0.9680 - val_loss: 0.0824 - val_acc: 0.9680\n",
      "Epoch 64/250\n",
      "1s - loss: 0.0848 - acc: 0.9681 - val_loss: 0.0817 - val_acc: 0.9684\n",
      "Epoch 65/250\n",
      "1s - loss: 0.0844 - acc: 0.9682 - val_loss: 0.0819 - val_acc: 0.9686\n",
      "Epoch 66/250\n",
      "1s - loss: 0.0845 - acc: 0.9683 - val_loss: 0.0820 - val_acc: 0.9685\n",
      "Epoch 67/250\n",
      "1s - loss: 0.0847 - acc: 0.9680 - val_loss: 0.0818 - val_acc: 0.9683\n",
      "Epoch 68/250\n",
      "1s - loss: 0.0846 - acc: 0.9680 - val_loss: 0.0821 - val_acc: 0.9682\n",
      "Epoch 69/250\n",
      "1s - loss: 0.0845 - acc: 0.9683 - val_loss: 0.0821 - val_acc: 0.9682\n",
      "Epoch 70/250\n",
      "1s - loss: 0.0842 - acc: 0.9683 - val_loss: 0.0820 - val_acc: 0.9683\n",
      "Epoch 71/250\n",
      "1s - loss: 0.0842 - acc: 0.9682 - val_loss: 0.0821 - val_acc: 0.9685\n",
      "Epoch 72/250\n",
      "1s - loss: 0.0843 - acc: 0.9682 - val_loss: 0.0818 - val_acc: 0.9685\n",
      "Epoch 73/250\n",
      "1s - loss: 0.0843 - acc: 0.9680 - val_loss: 0.0822 - val_acc: 0.9681\n",
      "Epoch 74/250\n",
      "1s - loss: 0.0840 - acc: 0.9682 - val_loss: 0.0823 - val_acc: 0.9682\n",
      "Epoch 75/250\n",
      "1s - loss: 0.0841 - acc: 0.9685 - val_loss: 0.0819 - val_acc: 0.9687\n",
      "Epoch 76/250\n",
      "1s - loss: 0.0839 - acc: 0.9682 - val_loss: 0.0819 - val_acc: 0.9687\n",
      "Epoch 77/250\n",
      "1s - loss: 0.0838 - acc: 0.9682 - val_loss: 0.0821 - val_acc: 0.9685\n",
      "Epoch 78/250\n",
      "1s - loss: 0.0839 - acc: 0.9684 - val_loss: 0.0820 - val_acc: 0.9686\n",
      "Epoch 79/250\n",
      "1s - loss: 0.0839 - acc: 0.9683 - val_loss: 0.0818 - val_acc: 0.9684\n",
      "Epoch 80/250\n",
      "1s - loss: 0.0840 - acc: 0.9683 - val_loss: 0.0820 - val_acc: 0.9684\n",
      "Epoch 81/250\n",
      "1s - loss: 0.0839 - acc: 0.9684 - val_loss: 0.0822 - val_acc: 0.9683\n",
      "Epoch 82/250\n",
      "1s - loss: 0.0841 - acc: 0.9684 - val_loss: 0.0817 - val_acc: 0.9686\n",
      "Epoch 83/250\n",
      "1s - loss: 0.0837 - acc: 0.9684 - val_loss: 0.0820 - val_acc: 0.9682\n",
      "Epoch 84/250\n",
      "1s - loss: 0.0837 - acc: 0.9683 - val_loss: 0.0818 - val_acc: 0.9683\n",
      "Epoch 85/250\n",
      "1s - loss: 0.0836 - acc: 0.9683 - val_loss: 0.0822 - val_acc: 0.9684\n",
      "Epoch 86/250\n",
      "1s - loss: 0.0836 - acc: 0.9683 - val_loss: 0.0823 - val_acc: 0.9682\n",
      "Epoch 87/250\n",
      "1s - loss: 0.0834 - acc: 0.9684 - val_loss: 0.0823 - val_acc: 0.9684\n",
      "Epoch 88/250\n",
      "1s - loss: 0.0834 - acc: 0.9684 - val_loss: 0.0822 - val_acc: 0.9684\n",
      "Epoch 89/250\n",
      "1s - loss: 0.0837 - acc: 0.9683 - val_loss: 0.0815 - val_acc: 0.9685\n",
      "Epoch 90/250\n",
      "1s - loss: 0.0836 - acc: 0.9684 - val_loss: 0.0819 - val_acc: 0.9684\n",
      "Epoch 91/250\n",
      "1s - loss: 0.0830 - acc: 0.9686 - val_loss: 0.0821 - val_acc: 0.9682\n",
      "Epoch 92/250\n",
      "1s - loss: 0.0837 - acc: 0.9683 - val_loss: 0.0820 - val_acc: 0.9683\n",
      "Epoch 93/250\n",
      "1s - loss: 0.0835 - acc: 0.9684 - val_loss: 0.0822 - val_acc: 0.9685\n",
      "Epoch 94/250\n",
      "1s - loss: 0.0834 - acc: 0.9683 - val_loss: 0.0820 - val_acc: 0.9688\n",
      "Epoch 95/250\n",
      "1s - loss: 0.0837 - acc: 0.9682 - val_loss: 0.0820 - val_acc: 0.9681\n",
      "Epoch 96/250\n",
      "1s - loss: 0.0834 - acc: 0.9684 - val_loss: 0.0820 - val_acc: 0.9684\n",
      "Epoch 97/250\n",
      "1s - loss: 0.0833 - acc: 0.9686 - val_loss: 0.0820 - val_acc: 0.9680\n",
      "Epoch 98/250\n",
      "1s - loss: 0.0832 - acc: 0.9682 - val_loss: 0.0826 - val_acc: 0.9681\n",
      "Epoch 99/250\n",
      "1s - loss: 0.0832 - acc: 0.9686 - val_loss: 0.0821 - val_acc: 0.9684\n",
      "Epoch 100/250\n",
      "1s - loss: 0.0833 - acc: 0.9684 - val_loss: 0.0821 - val_acc: 0.9685\n",
      "Epoch 101/250\n",
      "1s - loss: 0.0831 - acc: 0.9685 - val_loss: 0.0826 - val_acc: 0.9684\n",
      "Epoch 102/250\n",
      "1s - loss: 0.0832 - acc: 0.9684 - val_loss: 0.0822 - val_acc: 0.9682\n",
      "Epoch 103/250\n",
      "1s - loss: 0.0830 - acc: 0.9685 - val_loss: 0.0821 - val_acc: 0.9684\n",
      "Epoch 104/250\n",
      "1s - loss: 0.0830 - acc: 0.9685 - val_loss: 0.0822 - val_acc: 0.9683\n",
      "Epoch 105/250\n",
      "1s - loss: 0.0828 - acc: 0.9685 - val_loss: 0.0824 - val_acc: 0.9683\n",
      "Epoch 106/250\n",
      "1s - loss: 0.0829 - acc: 0.9684 - val_loss: 0.0822 - val_acc: 0.9684\n",
      "Epoch 107/250\n",
      "0s - loss: 0.0829 - acc: 0.9685 - val_loss: 0.0820 - val_acc: 0.9684\n",
      "Epoch 108/250\n",
      "1s - loss: 0.0828 - acc: 0.9686 - val_loss: 0.0825 - val_acc: 0.9682\n",
      "Epoch 109/250\n",
      "1s - loss: 0.0830 - acc: 0.9686 - val_loss: 0.0825 - val_acc: 0.9683\n",
      "Epoch 110/250\n",
      "1s - loss: 0.0827 - acc: 0.9687 - val_loss: 0.0820 - val_acc: 0.9684\n",
      "Epoch 111/250\n",
      "1s - loss: 0.0829 - acc: 0.9686 - val_loss: 0.0822 - val_acc: 0.9685\n",
      "Epoch 112/250\n",
      "1s - loss: 0.0827 - acc: 0.9687 - val_loss: 0.0825 - val_acc: 0.9681\n",
      "Epoch 113/250\n",
      "1s - loss: 0.0829 - acc: 0.9686 - val_loss: 0.0823 - val_acc: 0.9682\n",
      "Epoch 114/250\n",
      "1s - loss: 0.0827 - acc: 0.9685 - val_loss: 0.0823 - val_acc: 0.9685\n",
      "Epoch 115/250\n",
      "1s - loss: 0.0824 - acc: 0.9689 - val_loss: 0.0825 - val_acc: 0.9682\n",
      "Epoch 116/250\n",
      "1s - loss: 0.0827 - acc: 0.9686 - val_loss: 0.0822 - val_acc: 0.9684\n",
      "Epoch 117/250\n",
      "1s - loss: 0.0827 - acc: 0.9685 - val_loss: 0.0822 - val_acc: 0.9682\n",
      "Epoch 118/250\n",
      "1s - loss: 0.0825 - acc: 0.9687 - val_loss: 0.0822 - val_acc: 0.9683\n",
      "Epoch 119/250\n",
      "1s - loss: 0.0825 - acc: 0.9686 - val_loss: 0.0827 - val_acc: 0.9681\n",
      "Epoch 120/250\n",
      "1s - loss: 0.0823 - acc: 0.9687 - val_loss: 0.0823 - val_acc: 0.9685\n",
      "Epoch 121/250\n",
      "1s - loss: 0.0826 - acc: 0.9689 - val_loss: 0.0825 - val_acc: 0.9685\n",
      "Epoch 122/250\n",
      "1s - loss: 0.0824 - acc: 0.9687 - val_loss: 0.0826 - val_acc: 0.9684\n",
      "Epoch 123/250\n",
      "1s - loss: 0.0827 - acc: 0.9685 - val_loss: 0.0824 - val_acc: 0.9684\n",
      "Epoch 124/250\n",
      "1s - loss: 0.0823 - acc: 0.9686 - val_loss: 0.0827 - val_acc: 0.9680\n",
      "Epoch 125/250\n",
      "1s - loss: 0.0826 - acc: 0.9687 - val_loss: 0.0823 - val_acc: 0.9688\n",
      "Epoch 126/250\n",
      "1s - loss: 0.0822 - acc: 0.9686 - val_loss: 0.0822 - val_acc: 0.9685\n",
      "Epoch 127/250\n",
      "1s - loss: 0.0821 - acc: 0.9688 - val_loss: 0.0824 - val_acc: 0.9680\n",
      "Epoch 128/250\n",
      "1s - loss: 0.0824 - acc: 0.9686 - val_loss: 0.0827 - val_acc: 0.9683\n",
      "Epoch 129/250\n",
      "1s - loss: 0.0822 - acc: 0.9688 - val_loss: 0.0823 - val_acc: 0.9684\n",
      "Epoch 130/250\n",
      "1s - loss: 0.0822 - acc: 0.9688 - val_loss: 0.0822 - val_acc: 0.9684\n",
      "Epoch 131/250\n",
      "1s - loss: 0.0824 - acc: 0.9688 - val_loss: 0.0824 - val_acc: 0.9683\n",
      "Epoch 132/250\n",
      "1s - loss: 0.0821 - acc: 0.9688 - val_loss: 0.0823 - val_acc: 0.9683\n",
      "Epoch 133/250\n",
      "1s - loss: 0.0823 - acc: 0.9688 - val_loss: 0.0825 - val_acc: 0.9685\n",
      "Epoch 134/250\n",
      "1s - loss: 0.0819 - acc: 0.9688 - val_loss: 0.0823 - val_acc: 0.9686\n",
      "Epoch 135/250\n",
      "1s - loss: 0.0822 - acc: 0.9687 - val_loss: 0.0825 - val_acc: 0.9681\n",
      "Epoch 136/250\n",
      "1s - loss: 0.0821 - acc: 0.9688 - val_loss: 0.0825 - val_acc: 0.9682\n",
      "Epoch 137/250\n",
      "1s - loss: 0.0819 - acc: 0.9688 - val_loss: 0.0824 - val_acc: 0.9686\n",
      "Epoch 138/250\n",
      "1s - loss: 0.0820 - acc: 0.9689 - val_loss: 0.0823 - val_acc: 0.9685\n",
      "Epoch 139/250\n",
      "1s - loss: 0.0818 - acc: 0.9688 - val_loss: 0.0824 - val_acc: 0.9682\n",
      "Epoch 140/250\n",
      "1s - loss: 0.0821 - acc: 0.9688 - val_loss: 0.0828 - val_acc: 0.9681\n",
      "Epoch 141/250\n",
      "1s - loss: 0.0820 - acc: 0.9689 - val_loss: 0.0823 - val_acc: 0.9684\n",
      "Epoch 142/250\n",
      "1s - loss: 0.0819 - acc: 0.9691 - val_loss: 0.0824 - val_acc: 0.9684\n",
      "Epoch 143/250\n",
      "1s - loss: 0.0819 - acc: 0.9689 - val_loss: 0.0828 - val_acc: 0.9685\n",
      "Epoch 144/250\n",
      "1s - loss: 0.0820 - acc: 0.9690 - val_loss: 0.0827 - val_acc: 0.9682\n",
      "Epoch 145/250\n",
      "1s - loss: 0.0817 - acc: 0.9689 - val_loss: 0.0826 - val_acc: 0.9685\n",
      "Epoch 146/250\n",
      "1s - loss: 0.0818 - acc: 0.9689 - val_loss: 0.0831 - val_acc: 0.9682\n",
      "Epoch 147/250\n",
      "1s - loss: 0.0818 - acc: 0.9688 - val_loss: 0.0826 - val_acc: 0.9684\n",
      "Epoch 148/250\n",
      "1s - loss: 0.0817 - acc: 0.9690 - val_loss: 0.0826 - val_acc: 0.9683\n",
      "Epoch 149/250\n",
      "1s - loss: 0.0821 - acc: 0.9688 - val_loss: 0.0826 - val_acc: 0.9684\n",
      "Epoch 150/250\n",
      "1s - loss: 0.0819 - acc: 0.9688 - val_loss: 0.0828 - val_acc: 0.9682\n",
      "Epoch 151/250\n",
      "1s - loss: 0.0820 - acc: 0.9687 - val_loss: 0.0824 - val_acc: 0.9684\n",
      "Epoch 152/250\n",
      "1s - loss: 0.0817 - acc: 0.9690 - val_loss: 0.0824 - val_acc: 0.9684\n",
      "Epoch 153/250\n",
      "1s - loss: 0.0817 - acc: 0.9691 - val_loss: 0.0824 - val_acc: 0.9683\n",
      "Epoch 154/250\n",
      "1s - loss: 0.0818 - acc: 0.9691 - val_loss: 0.0830 - val_acc: 0.9683\n",
      "Epoch 155/250\n",
      "1s - loss: 0.0817 - acc: 0.9687 - val_loss: 0.0827 - val_acc: 0.9684\n",
      "Epoch 156/250\n",
      "1s - loss: 0.0818 - acc: 0.9688 - val_loss: 0.0829 - val_acc: 0.9683\n",
      "Epoch 157/250\n",
      "1s - loss: 0.0819 - acc: 0.9687 - val_loss: 0.0828 - val_acc: 0.9682\n",
      "Epoch 158/250\n",
      "1s - loss: 0.0819 - acc: 0.9689 - val_loss: 0.0827 - val_acc: 0.9687\n",
      "Epoch 159/250\n",
      "1s - loss: 0.0815 - acc: 0.9690 - val_loss: 0.0828 - val_acc: 0.9685\n",
      "Epoch 160/250\n",
      "1s - loss: 0.0816 - acc: 0.9690 - val_loss: 0.0829 - val_acc: 0.9682\n",
      "Epoch 161/250\n",
      "1s - loss: 0.0816 - acc: 0.9688 - val_loss: 0.0835 - val_acc: 0.9683\n",
      "Epoch 162/250\n",
      "1s - loss: 0.0814 - acc: 0.9691 - val_loss: 0.0827 - val_acc: 0.9683\n",
      "Epoch 163/250\n",
      "1s - loss: 0.0814 - acc: 0.9691 - val_loss: 0.0824 - val_acc: 0.9684\n",
      "Epoch 164/250\n",
      "1s - loss: 0.0812 - acc: 0.9689 - val_loss: 0.0828 - val_acc: 0.9684\n",
      "Epoch 165/250\n",
      "1s - loss: 0.0813 - acc: 0.9690 - val_loss: 0.0828 - val_acc: 0.9686\n",
      "Epoch 166/250\n",
      "1s - loss: 0.0813 - acc: 0.9692 - val_loss: 0.0826 - val_acc: 0.9683\n",
      "Epoch 167/250\n",
      "1s - loss: 0.0814 - acc: 0.9689 - val_loss: 0.0830 - val_acc: 0.9684\n",
      "Epoch 168/250\n",
      "1s - loss: 0.0814 - acc: 0.9691 - val_loss: 0.0831 - val_acc: 0.9684\n",
      "Epoch 169/250\n",
      "1s - loss: 0.0813 - acc: 0.9692 - val_loss: 0.0825 - val_acc: 0.9685\n",
      "Epoch 170/250\n",
      "1s - loss: 0.0814 - acc: 0.9690 - val_loss: 0.0825 - val_acc: 0.9684\n",
      "Epoch 171/250\n",
      "1s - loss: 0.0812 - acc: 0.9689 - val_loss: 0.0830 - val_acc: 0.9685\n",
      "Epoch 172/250\n",
      "1s - loss: 0.0817 - acc: 0.9689 - val_loss: 0.0828 - val_acc: 0.9683\n",
      "Epoch 173/250\n",
      "1s - loss: 0.0812 - acc: 0.9690 - val_loss: 0.0825 - val_acc: 0.9686\n",
      "Epoch 174/250\n",
      "1s - loss: 0.0813 - acc: 0.9688 - val_loss: 0.0829 - val_acc: 0.9685\n",
      "Epoch 175/250\n",
      "1s - loss: 0.0813 - acc: 0.9690 - val_loss: 0.0828 - val_acc: 0.9685\n",
      "Epoch 176/250\n",
      "1s - loss: 0.0810 - acc: 0.9693 - val_loss: 0.0831 - val_acc: 0.9682\n",
      "Epoch 177/250\n",
      "1s - loss: 0.0814 - acc: 0.9690 - val_loss: 0.0831 - val_acc: 0.9684\n",
      "Epoch 178/250\n",
      "1s - loss: 0.0811 - acc: 0.9690 - val_loss: 0.0824 - val_acc: 0.9686\n",
      "Epoch 179/250\n",
      "1s - loss: 0.0812 - acc: 0.9691 - val_loss: 0.0826 - val_acc: 0.9684\n",
      "Epoch 180/250\n",
      "1s - loss: 0.0814 - acc: 0.9690 - val_loss: 0.0829 - val_acc: 0.9684\n",
      "Epoch 181/250\n",
      "1s - loss: 0.0810 - acc: 0.9691 - val_loss: 0.0829 - val_acc: 0.9683\n",
      "Epoch 182/250\n",
      "1s - loss: 0.0812 - acc: 0.9691 - val_loss: 0.0827 - val_acc: 0.9684\n",
      "Epoch 183/250\n",
      "1s - loss: 0.0811 - acc: 0.9691 - val_loss: 0.0827 - val_acc: 0.9682\n",
      "Epoch 184/250\n",
      "1s - loss: 0.0807 - acc: 0.9691 - val_loss: 0.0829 - val_acc: 0.9682\n",
      "Epoch 185/250\n",
      "1s - loss: 0.0811 - acc: 0.9690 - val_loss: 0.0831 - val_acc: 0.9683\n",
      "Epoch 186/250\n",
      "1s - loss: 0.0807 - acc: 0.9691 - val_loss: 0.0830 - val_acc: 0.9685\n",
      "Epoch 187/250\n",
      "1s - loss: 0.0808 - acc: 0.9691 - val_loss: 0.0829 - val_acc: 0.9686\n",
      "Epoch 188/250\n",
      "1s - loss: 0.0813 - acc: 0.9691 - val_loss: 0.0827 - val_acc: 0.9686\n",
      "Epoch 189/250\n",
      "1s - loss: 0.0810 - acc: 0.9693 - val_loss: 0.0829 - val_acc: 0.9684\n",
      "Epoch 190/250\n",
      "1s - loss: 0.0812 - acc: 0.9691 - val_loss: 0.0831 - val_acc: 0.9683\n",
      "Epoch 191/250\n",
      "1s - loss: 0.0810 - acc: 0.9691 - val_loss: 0.0827 - val_acc: 0.9682\n",
      "Epoch 192/250\n",
      "1s - loss: 0.0809 - acc: 0.9692 - val_loss: 0.0827 - val_acc: 0.9684\n",
      "Epoch 193/250\n",
      "1s - loss: 0.0807 - acc: 0.9693 - val_loss: 0.0833 - val_acc: 0.9681\n",
      "Epoch 194/250\n",
      "1s - loss: 0.0812 - acc: 0.9691 - val_loss: 0.0829 - val_acc: 0.9684\n",
      "Epoch 195/250\n",
      "1s - loss: 0.0806 - acc: 0.9693 - val_loss: 0.0833 - val_acc: 0.9681\n",
      "Epoch 196/250\n",
      "1s - loss: 0.0809 - acc: 0.9690 - val_loss: 0.0829 - val_acc: 0.9683\n",
      "Epoch 197/250\n",
      "1s - loss: 0.0812 - acc: 0.9690 - val_loss: 0.0825 - val_acc: 0.9684\n",
      "Epoch 198/250\n",
      "1s - loss: 0.0810 - acc: 0.9691 - val_loss: 0.0828 - val_acc: 0.9685\n",
      "Epoch 199/250\n",
      "1s - loss: 0.0809 - acc: 0.9692 - val_loss: 0.0826 - val_acc: 0.9686\n",
      "Epoch 200/250\n",
      "1s - loss: 0.0808 - acc: 0.9692 - val_loss: 0.0827 - val_acc: 0.9685\n",
      "Epoch 201/250\n",
      "1s - loss: 0.0807 - acc: 0.9692 - val_loss: 0.0834 - val_acc: 0.9686\n",
      "Epoch 202/250\n",
      "1s - loss: 0.0808 - acc: 0.9693 - val_loss: 0.0833 - val_acc: 0.9684\n",
      "Epoch 203/250\n",
      "1s - loss: 0.0808 - acc: 0.9690 - val_loss: 0.0835 - val_acc: 0.9684\n",
      "Epoch 204/250\n",
      "1s - loss: 0.0808 - acc: 0.9692 - val_loss: 0.0827 - val_acc: 0.9683\n",
      "Epoch 205/250\n",
      "1s - loss: 0.0807 - acc: 0.9692 - val_loss: 0.0832 - val_acc: 0.9683\n",
      "Epoch 206/250\n",
      "1s - loss: 0.0805 - acc: 0.9694 - val_loss: 0.0831 - val_acc: 0.9683\n",
      "Epoch 207/250\n",
      "1s - loss: 0.0806 - acc: 0.9692 - val_loss: 0.0831 - val_acc: 0.9681\n",
      "Epoch 208/250\n",
      "1s - loss: 0.0809 - acc: 0.9692 - val_loss: 0.0835 - val_acc: 0.9685\n",
      "Epoch 209/250\n",
      "1s - loss: 0.0805 - acc: 0.9695 - val_loss: 0.0831 - val_acc: 0.9683\n",
      "Epoch 210/250\n",
      "1s - loss: 0.0807 - acc: 0.9693 - val_loss: 0.0833 - val_acc: 0.9682\n",
      "Epoch 211/250\n",
      "1s - loss: 0.0807 - acc: 0.9692 - val_loss: 0.0833 - val_acc: 0.9684\n",
      "Epoch 212/250\n",
      "1s - loss: 0.0807 - acc: 0.9692 - val_loss: 0.0830 - val_acc: 0.9684\n",
      "Epoch 213/250\n",
      "1s - loss: 0.0807 - acc: 0.9690 - val_loss: 0.0834 - val_acc: 0.9680\n",
      "Epoch 214/250\n",
      "1s - loss: 0.0806 - acc: 0.9695 - val_loss: 0.0831 - val_acc: 0.9684\n",
      "Epoch 215/250\n",
      "1s - loss: 0.0806 - acc: 0.9693 - val_loss: 0.0829 - val_acc: 0.9683\n",
      "Epoch 216/250\n",
      "1s - loss: 0.0804 - acc: 0.9695 - val_loss: 0.0831 - val_acc: 0.9684\n",
      "Epoch 217/250\n",
      "1s - loss: 0.0806 - acc: 0.9691 - val_loss: 0.0833 - val_acc: 0.9681\n",
      "Epoch 218/250\n",
      "1s - loss: 0.0804 - acc: 0.9691 - val_loss: 0.0833 - val_acc: 0.9683\n",
      "Epoch 219/250\n",
      "1s - loss: 0.0803 - acc: 0.9693 - val_loss: 0.0833 - val_acc: 0.9685\n",
      "Epoch 220/250\n",
      "1s - loss: 0.0806 - acc: 0.9693 - val_loss: 0.0829 - val_acc: 0.9682\n",
      "Epoch 221/250\n",
      "1s - loss: 0.0802 - acc: 0.9693 - val_loss: 0.0837 - val_acc: 0.9681\n",
      "Epoch 222/250\n",
      "1s - loss: 0.0802 - acc: 0.9693 - val_loss: 0.0831 - val_acc: 0.9683\n",
      "Epoch 223/250\n",
      "1s - loss: 0.0805 - acc: 0.9693 - val_loss: 0.0833 - val_acc: 0.9683\n",
      "Epoch 224/250\n",
      "1s - loss: 0.0804 - acc: 0.9692 - val_loss: 0.0832 - val_acc: 0.9684\n",
      "Epoch 225/250\n",
      "1s - loss: 0.0806 - acc: 0.9693 - val_loss: 0.0833 - val_acc: 0.9685\n",
      "Epoch 226/250\n",
      "1s - loss: 0.0803 - acc: 0.9693 - val_loss: 0.0834 - val_acc: 0.9677\n",
      "Epoch 227/250\n",
      "1s - loss: 0.0802 - acc: 0.9695 - val_loss: 0.0831 - val_acc: 0.9682\n",
      "Epoch 228/250\n",
      "1s - loss: 0.0805 - acc: 0.9693 - val_loss: 0.0834 - val_acc: 0.9685\n",
      "Epoch 229/250\n",
      "1s - loss: 0.0800 - acc: 0.9695 - val_loss: 0.0833 - val_acc: 0.9685\n",
      "Epoch 230/250\n",
      "1s - loss: 0.0803 - acc: 0.9693 - val_loss: 0.0834 - val_acc: 0.9682\n",
      "Epoch 231/250\n",
      "1s - loss: 0.0800 - acc: 0.9695 - val_loss: 0.0836 - val_acc: 0.9686\n",
      "Epoch 232/250\n",
      "1s - loss: 0.0803 - acc: 0.9693 - val_loss: 0.0835 - val_acc: 0.9680\n",
      "Epoch 233/250\n",
      "1s - loss: 0.0802 - acc: 0.9695 - val_loss: 0.0829 - val_acc: 0.9682\n",
      "Epoch 234/250\n",
      "1s - loss: 0.0800 - acc: 0.9693 - val_loss: 0.0832 - val_acc: 0.9682\n",
      "Epoch 235/250\n",
      "1s - loss: 0.0802 - acc: 0.9694 - val_loss: 0.0833 - val_acc: 0.9685\n",
      "Epoch 236/250\n",
      "1s - loss: 0.0803 - acc: 0.9693 - val_loss: 0.0834 - val_acc: 0.9686\n",
      "Epoch 237/250\n",
      "1s - loss: 0.0801 - acc: 0.9695 - val_loss: 0.0835 - val_acc: 0.9683\n",
      "Epoch 238/250\n",
      "1s - loss: 0.0803 - acc: 0.9693 - val_loss: 0.0835 - val_acc: 0.9684\n",
      "Epoch 239/250\n",
      "1s - loss: 0.0801 - acc: 0.9697 - val_loss: 0.0838 - val_acc: 0.9684\n",
      "Epoch 240/250\n",
      "1s - loss: 0.0802 - acc: 0.9695 - val_loss: 0.0836 - val_acc: 0.9681\n",
      "Epoch 241/250\n",
      "1s - loss: 0.0801 - acc: 0.9695 - val_loss: 0.0837 - val_acc: 0.9684\n",
      "Epoch 242/250\n",
      "1s - loss: 0.0805 - acc: 0.9694 - val_loss: 0.0832 - val_acc: 0.9682\n",
      "Epoch 243/250\n",
      "1s - loss: 0.0803 - acc: 0.9693 - val_loss: 0.0834 - val_acc: 0.9682\n",
      "Epoch 244/250\n",
      "1s - loss: 0.0805 - acc: 0.9694 - val_loss: 0.0832 - val_acc: 0.9683\n",
      "Epoch 245/250\n",
      "1s - loss: 0.0802 - acc: 0.9694 - val_loss: 0.0831 - val_acc: 0.9683\n",
      "Epoch 246/250\n",
      "1s - loss: 0.0804 - acc: 0.9694 - val_loss: 0.0835 - val_acc: 0.9681\n",
      "Epoch 247/250\n",
      "1s - loss: 0.0803 - acc: 0.9695 - val_loss: 0.0833 - val_acc: 0.9684\n",
      "Epoch 248/250\n",
      "1s - loss: 0.0799 - acc: 0.9694 - val_loss: 0.0835 - val_acc: 0.9682\n",
      "Epoch 249/250\n",
      "1s - loss: 0.0798 - acc: 0.9694 - val_loss: 0.0836 - val_acc: 0.9682\n",
      "Epoch 250/250\n",
      "1s - loss: 0.0801 - acc: 0.9696 - val_loss: 0.0833 - val_acc: 0.9684\n",
      "0.930231712577\n",
      "Train on 32383 samples, validate on 8096 samples\n",
      "Epoch 1/250\n",
      "1s - loss: 0.1748 - acc: 0.9392 - val_loss: 0.1016 - val_acc: 0.9656\n",
      "Epoch 2/250\n",
      "1s - loss: 0.1076 - acc: 0.9641 - val_loss: 0.0937 - val_acc: 0.9671\n",
      "Epoch 3/250\n",
      "1s - loss: 0.1006 - acc: 0.9655 - val_loss: 0.0901 - val_acc: 0.9675\n",
      "Epoch 4/250\n",
      "1s - loss: 0.0973 - acc: 0.9660 - val_loss: 0.0887 - val_acc: 0.9678\n",
      "Epoch 5/250\n",
      "1s - loss: 0.0954 - acc: 0.9663 - val_loss: 0.0873 - val_acc: 0.9678\n",
      "Epoch 6/250\n",
      "1s - loss: 0.0941 - acc: 0.9665 - val_loss: 0.0871 - val_acc: 0.9682\n",
      "Epoch 7/250\n",
      "1s - loss: 0.0928 - acc: 0.9665 - val_loss: 0.0867 - val_acc: 0.9684\n",
      "Epoch 8/250\n",
      "1s - loss: 0.0922 - acc: 0.9667 - val_loss: 0.0859 - val_acc: 0.9683\n",
      "Epoch 9/250\n",
      "1s - loss: 0.0918 - acc: 0.9668 - val_loss: 0.0856 - val_acc: 0.9682\n",
      "Epoch 10/250\n",
      "1s - loss: 0.0911 - acc: 0.9667 - val_loss: 0.0851 - val_acc: 0.9681\n",
      "Epoch 11/250\n",
      "1s - loss: 0.0909 - acc: 0.9668 - val_loss: 0.0847 - val_acc: 0.9682\n",
      "Epoch 12/250\n",
      "1s - loss: 0.0900 - acc: 0.9673 - val_loss: 0.0849 - val_acc: 0.9681\n",
      "Epoch 13/250\n",
      "1s - loss: 0.0898 - acc: 0.9669 - val_loss: 0.0848 - val_acc: 0.9680\n",
      "Epoch 14/250\n",
      "1s - loss: 0.0897 - acc: 0.9671 - val_loss: 0.0845 - val_acc: 0.9681\n",
      "Epoch 15/250\n",
      "1s - loss: 0.0891 - acc: 0.9671 - val_loss: 0.0845 - val_acc: 0.9680\n",
      "Epoch 16/250\n",
      "1s - loss: 0.0887 - acc: 0.9673 - val_loss: 0.0843 - val_acc: 0.9679\n",
      "Epoch 17/250\n",
      "1s - loss: 0.0888 - acc: 0.9672 - val_loss: 0.0841 - val_acc: 0.9680\n",
      "Epoch 18/250\n",
      "1s - loss: 0.0885 - acc: 0.9672 - val_loss: 0.0841 - val_acc: 0.9683\n",
      "Epoch 19/250\n",
      "1s - loss: 0.0884 - acc: 0.9672 - val_loss: 0.0847 - val_acc: 0.9681\n",
      "Epoch 20/250\n",
      "1s - loss: 0.0881 - acc: 0.9673 - val_loss: 0.0837 - val_acc: 0.9682\n",
      "Epoch 21/250\n",
      "1s - loss: 0.0880 - acc: 0.9675 - val_loss: 0.0839 - val_acc: 0.9684\n",
      "Epoch 22/250\n",
      "1s - loss: 0.0879 - acc: 0.9672 - val_loss: 0.0837 - val_acc: 0.9683\n",
      "Epoch 23/250\n",
      "1s - loss: 0.0877 - acc: 0.9675 - val_loss: 0.0836 - val_acc: 0.9682\n",
      "Epoch 24/250\n",
      "1s - loss: 0.0871 - acc: 0.9676 - val_loss: 0.0840 - val_acc: 0.9682\n",
      "Epoch 25/250\n",
      "1s - loss: 0.0872 - acc: 0.9676 - val_loss: 0.0840 - val_acc: 0.9681\n",
      "Epoch 26/250\n",
      "1s - loss: 0.0874 - acc: 0.9674 - val_loss: 0.0837 - val_acc: 0.9679\n",
      "Epoch 27/250\n",
      "1s - loss: 0.0870 - acc: 0.9675 - val_loss: 0.0837 - val_acc: 0.9683\n",
      "Epoch 28/250\n",
      "1s - loss: 0.0867 - acc: 0.9675 - val_loss: 0.0841 - val_acc: 0.9680\n",
      "Epoch 29/250\n",
      "0s - loss: 0.0869 - acc: 0.9675 - val_loss: 0.0837 - val_acc: 0.9682\n",
      "Epoch 30/250\n",
      "1s - loss: 0.0868 - acc: 0.9676 - val_loss: 0.0835 - val_acc: 0.9681\n",
      "Epoch 31/250\n",
      "1s - loss: 0.0867 - acc: 0.9675 - val_loss: 0.0841 - val_acc: 0.9682\n",
      "Epoch 32/250\n",
      "0s - loss: 0.0865 - acc: 0.9675 - val_loss: 0.0836 - val_acc: 0.9681\n",
      "Epoch 33/250\n",
      "1s - loss: 0.0866 - acc: 0.9676 - val_loss: 0.0835 - val_acc: 0.9681\n",
      "Epoch 34/250\n",
      "1s - loss: 0.0865 - acc: 0.9677 - val_loss: 0.0837 - val_acc: 0.9682\n",
      "Epoch 35/250\n",
      "1s - loss: 0.0865 - acc: 0.9676 - val_loss: 0.0838 - val_acc: 0.9680\n",
      "Epoch 36/250\n",
      "1s - loss: 0.0861 - acc: 0.9680 - val_loss: 0.0835 - val_acc: 0.9678\n",
      "Epoch 37/250\n",
      "1s - loss: 0.0860 - acc: 0.9678 - val_loss: 0.0835 - val_acc: 0.9681\n",
      "Epoch 38/250\n",
      "1s - loss: 0.0860 - acc: 0.9679 - val_loss: 0.0835 - val_acc: 0.9685\n",
      "Epoch 39/250\n",
      "1s - loss: 0.0857 - acc: 0.9677 - val_loss: 0.0833 - val_acc: 0.9681\n",
      "Epoch 40/250\n",
      "1s - loss: 0.0857 - acc: 0.9676 - val_loss: 0.0828 - val_acc: 0.9683\n",
      "Epoch 41/250\n",
      "1s - loss: 0.0857 - acc: 0.9677 - val_loss: 0.0833 - val_acc: 0.9682\n",
      "Epoch 42/250\n",
      "1s - loss: 0.0856 - acc: 0.9678 - val_loss: 0.0836 - val_acc: 0.9682\n",
      "Epoch 43/250\n",
      "1s - loss: 0.0852 - acc: 0.9679 - val_loss: 0.0835 - val_acc: 0.9684\n",
      "Epoch 44/250\n",
      "1s - loss: 0.0852 - acc: 0.9680 - val_loss: 0.0833 - val_acc: 0.9683\n",
      "Epoch 45/250\n",
      "1s - loss: 0.0852 - acc: 0.9679 - val_loss: 0.0833 - val_acc: 0.9683\n",
      "Epoch 46/250\n",
      "1s - loss: 0.0854 - acc: 0.9680 - val_loss: 0.0835 - val_acc: 0.9682\n",
      "Epoch 47/250\n",
      "1s - loss: 0.0851 - acc: 0.9679 - val_loss: 0.0834 - val_acc: 0.9682\n",
      "Epoch 48/250\n",
      "1s - loss: 0.0851 - acc: 0.9679 - val_loss: 0.0834 - val_acc: 0.9681\n",
      "Epoch 49/250\n",
      "1s - loss: 0.0849 - acc: 0.9679 - val_loss: 0.0837 - val_acc: 0.9682\n",
      "Epoch 50/250\n",
      "1s - loss: 0.0851 - acc: 0.9682 - val_loss: 0.0833 - val_acc: 0.9683\n",
      "Epoch 51/250\n",
      "1s - loss: 0.0847 - acc: 0.9681 - val_loss: 0.0835 - val_acc: 0.9680\n",
      "Epoch 52/250\n",
      "1s - loss: 0.0847 - acc: 0.9682 - val_loss: 0.0838 - val_acc: 0.9682\n",
      "Epoch 53/250\n",
      "1s - loss: 0.0849 - acc: 0.9683 - val_loss: 0.0834 - val_acc: 0.9682\n",
      "Epoch 54/250\n",
      "1s - loss: 0.0844 - acc: 0.9681 - val_loss: 0.0837 - val_acc: 0.9681\n",
      "Epoch 55/250\n",
      "1s - loss: 0.0845 - acc: 0.9684 - val_loss: 0.0835 - val_acc: 0.9683\n",
      "Epoch 56/250\n",
      "1s - loss: 0.0846 - acc: 0.9682 - val_loss: 0.0834 - val_acc: 0.9681\n",
      "Epoch 57/250\n",
      "0s - loss: 0.0844 - acc: 0.9683 - val_loss: 0.0840 - val_acc: 0.9681\n",
      "Epoch 58/250\n",
      "1s - loss: 0.0842 - acc: 0.9680 - val_loss: 0.0836 - val_acc: 0.9681\n",
      "Epoch 59/250\n",
      "1s - loss: 0.0844 - acc: 0.9683 - val_loss: 0.0835 - val_acc: 0.9681\n",
      "Epoch 60/250\n",
      "1s - loss: 0.0845 - acc: 0.9682 - val_loss: 0.0835 - val_acc: 0.9681\n",
      "Epoch 61/250\n",
      "1s - loss: 0.0842 - acc: 0.9682 - val_loss: 0.0833 - val_acc: 0.9681\n",
      "Epoch 62/250\n",
      "1s - loss: 0.0840 - acc: 0.9681 - val_loss: 0.0835 - val_acc: 0.9680\n",
      "Epoch 63/250\n",
      "1s - loss: 0.0840 - acc: 0.9681 - val_loss: 0.0833 - val_acc: 0.9681\n",
      "Epoch 64/250\n",
      "1s - loss: 0.0840 - acc: 0.9683 - val_loss: 0.0837 - val_acc: 0.9680\n",
      "Epoch 65/250\n",
      "0s - loss: 0.0838 - acc: 0.9684 - val_loss: 0.0834 - val_acc: 0.9682\n",
      "Epoch 66/250\n",
      "0s - loss: 0.0838 - acc: 0.9685 - val_loss: 0.0833 - val_acc: 0.9682\n",
      "Epoch 67/250\n",
      "1s - loss: 0.0838 - acc: 0.9683 - val_loss: 0.0832 - val_acc: 0.9681\n",
      "Epoch 68/250\n",
      "1s - loss: 0.0839 - acc: 0.9685 - val_loss: 0.0835 - val_acc: 0.9681\n",
      "Epoch 69/250\n",
      "0s - loss: 0.0842 - acc: 0.9681 - val_loss: 0.0834 - val_acc: 0.9682\n",
      "Epoch 70/250\n",
      "1s - loss: 0.0838 - acc: 0.9683 - val_loss: 0.0832 - val_acc: 0.9680\n",
      "Epoch 71/250\n",
      "1s - loss: 0.0839 - acc: 0.9684 - val_loss: 0.0836 - val_acc: 0.9678\n",
      "Epoch 72/250\n",
      "1s - loss: 0.0838 - acc: 0.9683 - val_loss: 0.0835 - val_acc: 0.9682\n",
      "Epoch 73/250\n",
      "1s - loss: 0.0836 - acc: 0.9685 - val_loss: 0.0838 - val_acc: 0.9681\n",
      "Epoch 74/250\n",
      "1s - loss: 0.0838 - acc: 0.9685 - val_loss: 0.0833 - val_acc: 0.9680\n",
      "Epoch 75/250\n",
      "1s - loss: 0.0833 - acc: 0.9685 - val_loss: 0.0837 - val_acc: 0.9680\n",
      "Epoch 76/250\n",
      "1s - loss: 0.0835 - acc: 0.9683 - val_loss: 0.0840 - val_acc: 0.9681\n",
      "Epoch 77/250\n",
      "1s - loss: 0.0833 - acc: 0.9687 - val_loss: 0.0839 - val_acc: 0.9679\n",
      "Epoch 78/250\n",
      "1s - loss: 0.0833 - acc: 0.9686 - val_loss: 0.0837 - val_acc: 0.9682\n",
      "Epoch 79/250\n",
      "1s - loss: 0.0835 - acc: 0.9683 - val_loss: 0.0836 - val_acc: 0.9681\n",
      "Epoch 80/250\n",
      "1s - loss: 0.0831 - acc: 0.9686 - val_loss: 0.0836 - val_acc: 0.9682\n",
      "Epoch 81/250\n",
      "1s - loss: 0.0835 - acc: 0.9683 - val_loss: 0.0833 - val_acc: 0.9681\n",
      "Epoch 82/250\n",
      "1s - loss: 0.0832 - acc: 0.9685 - val_loss: 0.0835 - val_acc: 0.9679\n",
      "Epoch 83/250\n",
      "1s - loss: 0.0832 - acc: 0.9686 - val_loss: 0.0837 - val_acc: 0.9678\n",
      "Epoch 84/250\n",
      "1s - loss: 0.0835 - acc: 0.9684 - val_loss: 0.0839 - val_acc: 0.9680\n",
      "Epoch 85/250\n",
      "1s - loss: 0.0832 - acc: 0.9686 - val_loss: 0.0845 - val_acc: 0.9679\n",
      "Epoch 86/250\n",
      "1s - loss: 0.0829 - acc: 0.9686 - val_loss: 0.0841 - val_acc: 0.9681\n",
      "Epoch 87/250\n",
      "1s - loss: 0.0828 - acc: 0.9686 - val_loss: 0.0836 - val_acc: 0.9681\n",
      "Epoch 88/250\n",
      "1s - loss: 0.0832 - acc: 0.9687 - val_loss: 0.0840 - val_acc: 0.9682\n",
      "Epoch 89/250\n",
      "1s - loss: 0.0831 - acc: 0.9686 - val_loss: 0.0838 - val_acc: 0.9679\n",
      "Epoch 90/250\n",
      "1s - loss: 0.0826 - acc: 0.9687 - val_loss: 0.0837 - val_acc: 0.9679\n",
      "Epoch 91/250\n",
      "1s - loss: 0.0826 - acc: 0.9688 - val_loss: 0.0836 - val_acc: 0.9680\n",
      "Epoch 92/250\n",
      "1s - loss: 0.0829 - acc: 0.9687 - val_loss: 0.0838 - val_acc: 0.9682\n",
      "Epoch 93/250\n",
      "1s - loss: 0.0828 - acc: 0.9687 - val_loss: 0.0838 - val_acc: 0.9683\n",
      "Epoch 94/250\n",
      "1s - loss: 0.0829 - acc: 0.9686 - val_loss: 0.0837 - val_acc: 0.9681\n",
      "Epoch 95/250\n",
      "1s - loss: 0.0825 - acc: 0.9689 - val_loss: 0.0837 - val_acc: 0.9682\n",
      "Epoch 96/250\n",
      "1s - loss: 0.0826 - acc: 0.9688 - val_loss: 0.0839 - val_acc: 0.9680\n",
      "Epoch 97/250\n",
      "1s - loss: 0.0827 - acc: 0.9688 - val_loss: 0.0836 - val_acc: 0.9679\n",
      "Epoch 98/250\n",
      "1s - loss: 0.0823 - acc: 0.9687 - val_loss: 0.0840 - val_acc: 0.9680\n",
      "Epoch 99/250\n",
      "1s - loss: 0.0826 - acc: 0.9686 - val_loss: 0.0837 - val_acc: 0.9682\n",
      "Epoch 100/250\n",
      "1s - loss: 0.0823 - acc: 0.9687 - val_loss: 0.0842 - val_acc: 0.9682\n",
      "Epoch 101/250\n",
      "1s - loss: 0.0824 - acc: 0.9685 - val_loss: 0.0841 - val_acc: 0.9680\n",
      "Epoch 102/250\n",
      "1s - loss: 0.0825 - acc: 0.9687 - val_loss: 0.0838 - val_acc: 0.9682\n",
      "Epoch 103/250\n",
      "1s - loss: 0.0823 - acc: 0.9687 - val_loss: 0.0839 - val_acc: 0.9681\n",
      "Epoch 104/250\n",
      "1s - loss: 0.0826 - acc: 0.9689 - val_loss: 0.0839 - val_acc: 0.9680\n",
      "Epoch 105/250\n",
      "1s - loss: 0.0824 - acc: 0.9689 - val_loss: 0.0838 - val_acc: 0.9680\n",
      "Epoch 106/250\n",
      "1s - loss: 0.0820 - acc: 0.9690 - val_loss: 0.0843 - val_acc: 0.9680\n",
      "Epoch 107/250\n",
      "1s - loss: 0.0823 - acc: 0.9687 - val_loss: 0.0838 - val_acc: 0.9681\n",
      "Epoch 108/250\n",
      "1s - loss: 0.0826 - acc: 0.9688 - val_loss: 0.0839 - val_acc: 0.9680\n",
      "Epoch 109/250\n",
      "1s - loss: 0.0822 - acc: 0.9688 - val_loss: 0.0835 - val_acc: 0.9681\n",
      "Epoch 110/250\n",
      "1s - loss: 0.0824 - acc: 0.9688 - val_loss: 0.0841 - val_acc: 0.9678\n",
      "Epoch 111/250\n",
      "1s - loss: 0.0822 - acc: 0.9686 - val_loss: 0.0838 - val_acc: 0.9677\n",
      "Epoch 112/250\n",
      "1s - loss: 0.0821 - acc: 0.9687 - val_loss: 0.0837 - val_acc: 0.9679\n",
      "Epoch 113/250\n",
      "1s - loss: 0.0822 - acc: 0.9688 - val_loss: 0.0838 - val_acc: 0.9681\n",
      "Epoch 114/250\n",
      "1s - loss: 0.0821 - acc: 0.9687 - val_loss: 0.0838 - val_acc: 0.9680\n",
      "Epoch 115/250\n",
      "1s - loss: 0.0821 - acc: 0.9690 - val_loss: 0.0842 - val_acc: 0.9680\n",
      "Epoch 116/250\n",
      "1s - loss: 0.0822 - acc: 0.9687 - val_loss: 0.0843 - val_acc: 0.9676\n",
      "Epoch 117/250\n",
      "1s - loss: 0.0818 - acc: 0.9690 - val_loss: 0.0837 - val_acc: 0.9681\n",
      "Epoch 118/250\n",
      "1s - loss: 0.0821 - acc: 0.9689 - val_loss: 0.0845 - val_acc: 0.9681\n",
      "Epoch 119/250\n",
      "1s - loss: 0.0820 - acc: 0.9690 - val_loss: 0.0841 - val_acc: 0.9680\n",
      "Epoch 120/250\n",
      "1s - loss: 0.0818 - acc: 0.9689 - val_loss: 0.0841 - val_acc: 0.9682\n",
      "Epoch 121/250\n",
      "1s - loss: 0.0819 - acc: 0.9689 - val_loss: 0.0842 - val_acc: 0.9680\n",
      "Epoch 122/250\n",
      "1s - loss: 0.0819 - acc: 0.9690 - val_loss: 0.0839 - val_acc: 0.9682\n",
      "Epoch 123/250\n",
      "1s - loss: 0.0817 - acc: 0.9690 - val_loss: 0.0842 - val_acc: 0.9680\n",
      "Epoch 124/250\n",
      "1s - loss: 0.0819 - acc: 0.9690 - val_loss: 0.0842 - val_acc: 0.9677\n",
      "Epoch 125/250\n",
      "1s - loss: 0.0820 - acc: 0.9688 - val_loss: 0.0844 - val_acc: 0.9679\n",
      "Epoch 126/250\n",
      "1s - loss: 0.0817 - acc: 0.9688 - val_loss: 0.0840 - val_acc: 0.9681\n",
      "Epoch 127/250\n",
      "1s - loss: 0.0818 - acc: 0.9691 - val_loss: 0.0842 - val_acc: 0.9677\n",
      "Epoch 128/250\n",
      "1s - loss: 0.0819 - acc: 0.9691 - val_loss: 0.0840 - val_acc: 0.9680\n",
      "Epoch 129/250\n",
      "1s - loss: 0.0817 - acc: 0.9690 - val_loss: 0.0840 - val_acc: 0.9679\n",
      "Epoch 130/250\n",
      "1s - loss: 0.0815 - acc: 0.9690 - val_loss: 0.0843 - val_acc: 0.9679\n",
      "Epoch 131/250\n",
      "1s - loss: 0.0817 - acc: 0.9688 - val_loss: 0.0844 - val_acc: 0.9677\n",
      "Epoch 132/250\n",
      "1s - loss: 0.0815 - acc: 0.9689 - val_loss: 0.0840 - val_acc: 0.9676\n",
      "Epoch 133/250\n",
      "0s - loss: 0.0817 - acc: 0.9689 - val_loss: 0.0840 - val_acc: 0.9680\n",
      "Epoch 134/250\n",
      "1s - loss: 0.0814 - acc: 0.9692 - val_loss: 0.0842 - val_acc: 0.9678\n",
      "Epoch 135/250\n",
      "1s - loss: 0.0814 - acc: 0.9690 - val_loss: 0.0843 - val_acc: 0.9678\n",
      "Epoch 136/250\n",
      "1s - loss: 0.0815 - acc: 0.9690 - val_loss: 0.0837 - val_acc: 0.9676\n",
      "Epoch 137/250\n",
      "1s - loss: 0.0814 - acc: 0.9689 - val_loss: 0.0845 - val_acc: 0.9679\n",
      "Epoch 138/250\n",
      "1s - loss: 0.0814 - acc: 0.9693 - val_loss: 0.0841 - val_acc: 0.9677\n",
      "Epoch 139/250\n",
      "1s - loss: 0.0816 - acc: 0.9690 - val_loss: 0.0843 - val_acc: 0.9676\n",
      "Epoch 140/250\n",
      "1s - loss: 0.0811 - acc: 0.9691 - val_loss: 0.0843 - val_acc: 0.9679\n",
      "Epoch 141/250\n",
      "1s - loss: 0.0813 - acc: 0.9691 - val_loss: 0.0843 - val_acc: 0.9678\n",
      "Epoch 142/250\n",
      "1s - loss: 0.0813 - acc: 0.9693 - val_loss: 0.0844 - val_acc: 0.9678\n",
      "Epoch 143/250\n",
      "1s - loss: 0.0813 - acc: 0.9691 - val_loss: 0.0847 - val_acc: 0.9677\n",
      "Epoch 144/250\n",
      "1s - loss: 0.0813 - acc: 0.9692 - val_loss: 0.0843 - val_acc: 0.9675\n",
      "Epoch 145/250\n",
      "1s - loss: 0.0812 - acc: 0.9692 - val_loss: 0.0841 - val_acc: 0.9678\n",
      "Epoch 146/250\n",
      "0s - loss: 0.0813 - acc: 0.9691 - val_loss: 0.0848 - val_acc: 0.9679\n",
      "Epoch 147/250\n",
      "1s - loss: 0.0811 - acc: 0.9692 - val_loss: 0.0844 - val_acc: 0.9678\n",
      "Epoch 148/250\n",
      "1s - loss: 0.0814 - acc: 0.9690 - val_loss: 0.0844 - val_acc: 0.9678\n",
      "Epoch 149/250\n",
      "1s - loss: 0.0812 - acc: 0.9691 - val_loss: 0.0842 - val_acc: 0.9679\n",
      "Epoch 150/250\n",
      "1s - loss: 0.0815 - acc: 0.9692 - val_loss: 0.0844 - val_acc: 0.9678\n",
      "Epoch 151/250\n",
      "1s - loss: 0.0813 - acc: 0.9690 - val_loss: 0.0845 - val_acc: 0.9678\n",
      "Epoch 152/250\n",
      "1s - loss: 0.0810 - acc: 0.9692 - val_loss: 0.0841 - val_acc: 0.9679\n",
      "Epoch 153/250\n",
      "1s - loss: 0.0813 - acc: 0.9692 - val_loss: 0.0842 - val_acc: 0.9680\n",
      "Epoch 154/250\n",
      "1s - loss: 0.0810 - acc: 0.9695 - val_loss: 0.0845 - val_acc: 0.9678\n",
      "Epoch 155/250\n",
      "1s - loss: 0.0813 - acc: 0.9690 - val_loss: 0.0840 - val_acc: 0.9680\n",
      "Epoch 156/250\n",
      "1s - loss: 0.0809 - acc: 0.9693 - val_loss: 0.0841 - val_acc: 0.9677\n",
      "Epoch 157/250\n",
      "1s - loss: 0.0811 - acc: 0.9690 - val_loss: 0.0839 - val_acc: 0.9679\n",
      "Epoch 158/250\n",
      "1s - loss: 0.0810 - acc: 0.9691 - val_loss: 0.0848 - val_acc: 0.9677\n",
      "Epoch 159/250\n",
      "1s - loss: 0.0812 - acc: 0.9692 - val_loss: 0.0843 - val_acc: 0.9681\n",
      "Epoch 160/250\n",
      "1s - loss: 0.0809 - acc: 0.9692 - val_loss: 0.0840 - val_acc: 0.9678\n",
      "Epoch 161/250\n",
      "1s - loss: 0.0807 - acc: 0.9692 - val_loss: 0.0841 - val_acc: 0.9677\n",
      "Epoch 162/250\n",
      "1s - loss: 0.0810 - acc: 0.9693 - val_loss: 0.0842 - val_acc: 0.9678\n",
      "Epoch 163/250\n",
      "1s - loss: 0.0808 - acc: 0.9694 - val_loss: 0.0845 - val_acc: 0.9677\n",
      "Epoch 164/250\n",
      "1s - loss: 0.0809 - acc: 0.9694 - val_loss: 0.0846 - val_acc: 0.9677\n",
      "Epoch 165/250\n",
      "1s - loss: 0.0809 - acc: 0.9693 - val_loss: 0.0842 - val_acc: 0.9679\n",
      "Epoch 166/250\n",
      "1s - loss: 0.0809 - acc: 0.9692 - val_loss: 0.0846 - val_acc: 0.9680\n",
      "Epoch 167/250\n",
      "1s - loss: 0.0807 - acc: 0.9693 - val_loss: 0.0846 - val_acc: 0.9679\n",
      "Epoch 168/250\n",
      "1s - loss: 0.0805 - acc: 0.9695 - val_loss: 0.0843 - val_acc: 0.9678\n",
      "Epoch 169/250\n",
      "1s - loss: 0.0806 - acc: 0.9695 - val_loss: 0.0843 - val_acc: 0.9675\n",
      "Epoch 170/250\n",
      "1s - loss: 0.0810 - acc: 0.9690 - val_loss: 0.0845 - val_acc: 0.9677\n",
      "Epoch 171/250\n",
      "1s - loss: 0.0807 - acc: 0.9692 - val_loss: 0.0844 - val_acc: 0.9680\n",
      "Epoch 172/250\n",
      "1s - loss: 0.0806 - acc: 0.9693 - val_loss: 0.0840 - val_acc: 0.9678\n",
      "Epoch 173/250\n",
      "1s - loss: 0.0807 - acc: 0.9692 - val_loss: 0.0843 - val_acc: 0.9678\n",
      "Epoch 174/250\n",
      "1s - loss: 0.0806 - acc: 0.9692 - val_loss: 0.0845 - val_acc: 0.9676\n",
      "Epoch 175/250\n",
      "1s - loss: 0.0805 - acc: 0.9694 - val_loss: 0.0847 - val_acc: 0.9677\n",
      "Epoch 176/250\n",
      "1s - loss: 0.0804 - acc: 0.9694 - val_loss: 0.0846 - val_acc: 0.9677\n",
      "Epoch 177/250\n",
      "1s - loss: 0.0804 - acc: 0.9692 - val_loss: 0.0848 - val_acc: 0.9679\n",
      "Epoch 178/250\n",
      "1s - loss: 0.0806 - acc: 0.9694 - val_loss: 0.0844 - val_acc: 0.9677\n",
      "Epoch 179/250\n",
      "1s - loss: 0.0804 - acc: 0.9693 - val_loss: 0.0846 - val_acc: 0.9680\n",
      "Epoch 180/250\n",
      "1s - loss: 0.0805 - acc: 0.9694 - val_loss: 0.0848 - val_acc: 0.9676\n",
      "Epoch 181/250\n",
      "1s - loss: 0.0803 - acc: 0.9694 - val_loss: 0.0844 - val_acc: 0.9679\n",
      "Epoch 182/250\n",
      "1s - loss: 0.0804 - acc: 0.9695 - val_loss: 0.0847 - val_acc: 0.9678\n",
      "Epoch 183/250\n",
      "1s - loss: 0.0806 - acc: 0.9692 - val_loss: 0.0848 - val_acc: 0.9677\n",
      "Epoch 184/250\n",
      "1s - loss: 0.0805 - acc: 0.9694 - val_loss: 0.0846 - val_acc: 0.9676\n",
      "Epoch 185/250\n",
      "1s - loss: 0.0804 - acc: 0.9694 - val_loss: 0.0843 - val_acc: 0.9679\n",
      "Epoch 186/250\n",
      "1s - loss: 0.0803 - acc: 0.9694 - val_loss: 0.0847 - val_acc: 0.9679\n",
      "Epoch 187/250\n",
      "1s - loss: 0.0805 - acc: 0.9691 - val_loss: 0.0846 - val_acc: 0.9679\n",
      "Epoch 188/250\n",
      "1s - loss: 0.0806 - acc: 0.9692 - val_loss: 0.0849 - val_acc: 0.9675\n",
      "Epoch 189/250\n",
      "1s - loss: 0.0804 - acc: 0.9693 - val_loss: 0.0846 - val_acc: 0.9678\n",
      "Epoch 190/250\n",
      "1s - loss: 0.0802 - acc: 0.9692 - val_loss: 0.0848 - val_acc: 0.9678\n",
      "Epoch 191/250\n",
      "1s - loss: 0.0804 - acc: 0.9696 - val_loss: 0.0848 - val_acc: 0.9677\n",
      "Epoch 192/250\n",
      "1s - loss: 0.0806 - acc: 0.9692 - val_loss: 0.0846 - val_acc: 0.9679\n",
      "Epoch 193/250\n",
      "1s - loss: 0.0800 - acc: 0.9695 - val_loss: 0.0843 - val_acc: 0.9678\n",
      "Epoch 194/250\n",
      "1s - loss: 0.0801 - acc: 0.9693 - val_loss: 0.0845 - val_acc: 0.9678\n",
      "Epoch 195/250\n",
      "1s - loss: 0.0803 - acc: 0.9694 - val_loss: 0.0847 - val_acc: 0.9677\n",
      "Epoch 196/250\n",
      "1s - loss: 0.0802 - acc: 0.9692 - val_loss: 0.0849 - val_acc: 0.9679\n",
      "Epoch 197/250\n",
      "1s - loss: 0.0802 - acc: 0.9695 - val_loss: 0.0847 - val_acc: 0.9676\n",
      "Epoch 198/250\n",
      "1s - loss: 0.0799 - acc: 0.9696 - val_loss: 0.0849 - val_acc: 0.9676\n",
      "Epoch 199/250\n",
      "1s - loss: 0.0797 - acc: 0.9696 - val_loss: 0.0848 - val_acc: 0.9679\n",
      "Epoch 200/250\n",
      "1s - loss: 0.0799 - acc: 0.9696 - val_loss: 0.0846 - val_acc: 0.9677\n",
      "Epoch 201/250\n",
      "1s - loss: 0.0801 - acc: 0.9696 - val_loss: 0.0850 - val_acc: 0.9677\n",
      "Epoch 202/250\n",
      "1s - loss: 0.0798 - acc: 0.9699 - val_loss: 0.0845 - val_acc: 0.9676\n",
      "Epoch 203/250\n",
      "1s - loss: 0.0801 - acc: 0.9694 - val_loss: 0.0848 - val_acc: 0.9678\n",
      "Epoch 204/250\n",
      "1s - loss: 0.0799 - acc: 0.9696 - val_loss: 0.0846 - val_acc: 0.9679\n",
      "Epoch 205/250\n",
      "1s - loss: 0.0802 - acc: 0.9695 - val_loss: 0.0848 - val_acc: 0.9675\n",
      "Epoch 206/250\n",
      "1s - loss: 0.0799 - acc: 0.9693 - val_loss: 0.0850 - val_acc: 0.9677\n",
      "Epoch 207/250\n",
      "1s - loss: 0.0799 - acc: 0.9695 - val_loss: 0.0848 - val_acc: 0.9678\n",
      "Epoch 208/250\n",
      "1s - loss: 0.0799 - acc: 0.9696 - val_loss: 0.0852 - val_acc: 0.9677\n",
      "Epoch 209/250\n",
      "1s - loss: 0.0801 - acc: 0.9693 - val_loss: 0.0853 - val_acc: 0.9674\n",
      "Epoch 210/250\n",
      "1s - loss: 0.0799 - acc: 0.9694 - val_loss: 0.0846 - val_acc: 0.9678\n",
      "Epoch 211/250\n",
      "1s - loss: 0.0798 - acc: 0.9694 - val_loss: 0.0851 - val_acc: 0.9677\n",
      "Epoch 212/250\n",
      "1s - loss: 0.0799 - acc: 0.9694 - val_loss: 0.0849 - val_acc: 0.9674\n",
      "Epoch 213/250\n",
      "1s - loss: 0.0800 - acc: 0.9693 - val_loss: 0.0848 - val_acc: 0.9676\n",
      "Epoch 214/250\n",
      "1s - loss: 0.0796 - acc: 0.9695 - val_loss: 0.0847 - val_acc: 0.9675\n",
      "Epoch 215/250\n",
      "1s - loss: 0.0797 - acc: 0.9695 - val_loss: 0.0846 - val_acc: 0.9674\n",
      "Epoch 216/250\n",
      "1s - loss: 0.0798 - acc: 0.9695 - val_loss: 0.0849 - val_acc: 0.9676\n",
      "Epoch 217/250\n",
      "1s - loss: 0.0799 - acc: 0.9695 - val_loss: 0.0849 - val_acc: 0.9673\n",
      "Epoch 218/250\n",
      "1s - loss: 0.0799 - acc: 0.9695 - val_loss: 0.0846 - val_acc: 0.9676\n",
      "Epoch 219/250\n",
      "1s - loss: 0.0798 - acc: 0.9695 - val_loss: 0.0849 - val_acc: 0.9677\n",
      "Epoch 220/250\n",
      "1s - loss: 0.0796 - acc: 0.9697 - val_loss: 0.0854 - val_acc: 0.9676\n",
      "Epoch 221/250\n",
      "1s - loss: 0.0796 - acc: 0.9697 - val_loss: 0.0850 - val_acc: 0.9677\n",
      "Epoch 222/250\n",
      "1s - loss: 0.0795 - acc: 0.9695 - val_loss: 0.0854 - val_acc: 0.9678\n",
      "Epoch 223/250\n",
      "1s - loss: 0.0800 - acc: 0.9694 - val_loss: 0.0850 - val_acc: 0.9678\n",
      "Epoch 224/250\n",
      "1s - loss: 0.0797 - acc: 0.9697 - val_loss: 0.0853 - val_acc: 0.9676\n",
      "Epoch 225/250\n",
      "1s - loss: 0.0796 - acc: 0.9697 - val_loss: 0.0848 - val_acc: 0.9675\n",
      "Epoch 226/250\n",
      "1s - loss: 0.0796 - acc: 0.9698 - val_loss: 0.0852 - val_acc: 0.9675\n",
      "Epoch 227/250\n",
      "1s - loss: 0.0797 - acc: 0.9696 - val_loss: 0.0851 - val_acc: 0.9681\n",
      "Epoch 228/250\n",
      "1s - loss: 0.0799 - acc: 0.9696 - val_loss: 0.0847 - val_acc: 0.9678\n",
      "Epoch 229/250\n",
      "1s - loss: 0.0796 - acc: 0.9695 - val_loss: 0.0852 - val_acc: 0.9675\n",
      "Epoch 230/250\n",
      "1s - loss: 0.0797 - acc: 0.9696 - val_loss: 0.0848 - val_acc: 0.9677\n",
      "Epoch 231/250\n",
      "1s - loss: 0.0796 - acc: 0.9695 - val_loss: 0.0853 - val_acc: 0.9679\n",
      "Epoch 232/250\n",
      "1s - loss: 0.0795 - acc: 0.9699 - val_loss: 0.0852 - val_acc: 0.9676\n",
      "Epoch 233/250\n",
      "1s - loss: 0.0793 - acc: 0.9697 - val_loss: 0.0857 - val_acc: 0.9679\n",
      "Epoch 234/250\n",
      "1s - loss: 0.0795 - acc: 0.9696 - val_loss: 0.0856 - val_acc: 0.9678\n",
      "Epoch 235/250\n",
      "1s - loss: 0.0794 - acc: 0.9697 - val_loss: 0.0851 - val_acc: 0.9679\n",
      "Epoch 236/250\n",
      "1s - loss: 0.0796 - acc: 0.9697 - val_loss: 0.0854 - val_acc: 0.9676\n",
      "Epoch 237/250\n",
      "1s - loss: 0.0796 - acc: 0.9696 - val_loss: 0.0851 - val_acc: 0.9675\n",
      "Epoch 238/250\n",
      "1s - loss: 0.0796 - acc: 0.9696 - val_loss: 0.0853 - val_acc: 0.9679\n",
      "Epoch 239/250\n",
      "1s - loss: 0.0794 - acc: 0.9696 - val_loss: 0.0850 - val_acc: 0.9675\n",
      "Epoch 240/250\n",
      "1s - loss: 0.0794 - acc: 0.9696 - val_loss: 0.0854 - val_acc: 0.9677\n",
      "Epoch 241/250\n",
      "1s - loss: 0.0796 - acc: 0.9696 - val_loss: 0.0853 - val_acc: 0.9677\n",
      "Epoch 242/250\n",
      "1s - loss: 0.0792 - acc: 0.9696 - val_loss: 0.0852 - val_acc: 0.9677\n",
      "Epoch 243/250\n",
      "1s - loss: 0.0795 - acc: 0.9698 - val_loss: 0.0852 - val_acc: 0.9676\n",
      "Epoch 244/250\n",
      "1s - loss: 0.0795 - acc: 0.9695 - val_loss: 0.0855 - val_acc: 0.9676\n",
      "Epoch 245/250\n",
      "1s - loss: 0.0791 - acc: 0.9697 - val_loss: 0.0854 - val_acc: 0.9675\n",
      "Epoch 246/250\n",
      "1s - loss: 0.0795 - acc: 0.9695 - val_loss: 0.0851 - val_acc: 0.9678\n",
      "Epoch 247/250\n",
      "1s - loss: 0.0793 - acc: 0.9699 - val_loss: 0.0854 - val_acc: 0.9677\n",
      "Epoch 248/250\n",
      "1s - loss: 0.0794 - acc: 0.9697 - val_loss: 0.0850 - val_acc: 0.9678\n",
      "Epoch 249/250\n",
      "1s - loss: 0.0797 - acc: 0.9695 - val_loss: 0.0849 - val_acc: 0.9676\n",
      "Epoch 250/250\n",
      "1s - loss: 0.0794 - acc: 0.9697 - val_loss: 0.0855 - val_acc: 0.9676\n",
      "0.927351369875\n",
      "Train on 32383 samples, validate on 8096 samples\n",
      "Epoch 1/250\n",
      "1s - loss: 0.1720 - acc: 0.9399 - val_loss: 0.1011 - val_acc: 0.9659\n",
      "Epoch 2/250\n",
      "1s - loss: 0.1057 - acc: 0.9645 - val_loss: 0.0939 - val_acc: 0.9672\n",
      "Epoch 3/250\n",
      "1s - loss: 0.1001 - acc: 0.9655 - val_loss: 0.0904 - val_acc: 0.9679\n",
      "Epoch 4/250\n",
      "1s - loss: 0.0969 - acc: 0.9661 - val_loss: 0.0894 - val_acc: 0.9677\n",
      "Epoch 5/250\n",
      "1s - loss: 0.0953 - acc: 0.9662 - val_loss: 0.0876 - val_acc: 0.9680\n",
      "Epoch 6/250\n",
      "1s - loss: 0.0941 - acc: 0.9663 - val_loss: 0.0875 - val_acc: 0.9680\n",
      "Epoch 7/250\n",
      "1s - loss: 0.0933 - acc: 0.9663 - val_loss: 0.0865 - val_acc: 0.9680\n",
      "Epoch 8/250\n",
      "1s - loss: 0.0925 - acc: 0.9664 - val_loss: 0.0861 - val_acc: 0.9680\n",
      "Epoch 9/250\n",
      "1s - loss: 0.0915 - acc: 0.9666 - val_loss: 0.0857 - val_acc: 0.9681\n",
      "Epoch 10/250\n",
      "1s - loss: 0.0911 - acc: 0.9666 - val_loss: 0.0853 - val_acc: 0.9684\n",
      "Epoch 11/250\n",
      "1s - loss: 0.0908 - acc: 0.9666 - val_loss: 0.0854 - val_acc: 0.9684\n",
      "Epoch 12/250\n",
      "1s - loss: 0.0905 - acc: 0.9666 - val_loss: 0.0851 - val_acc: 0.9682\n",
      "Epoch 13/250\n",
      "1s - loss: 0.0899 - acc: 0.9667 - val_loss: 0.0852 - val_acc: 0.9682\n",
      "Epoch 14/250\n",
      "1s - loss: 0.0898 - acc: 0.9669 - val_loss: 0.0844 - val_acc: 0.9681\n",
      "Epoch 15/250\n",
      "1s - loss: 0.0893 - acc: 0.9670 - val_loss: 0.0848 - val_acc: 0.9681\n",
      "Epoch 16/250\n",
      "1s - loss: 0.0889 - acc: 0.9669 - val_loss: 0.0844 - val_acc: 0.9683\n",
      "Epoch 17/250\n",
      "1s - loss: 0.0890 - acc: 0.9670 - val_loss: 0.0847 - val_acc: 0.9682\n",
      "Epoch 18/250\n",
      "1s - loss: 0.0884 - acc: 0.9671 - val_loss: 0.0844 - val_acc: 0.9683\n",
      "Epoch 19/250\n",
      "1s - loss: 0.0886 - acc: 0.9670 - val_loss: 0.0843 - val_acc: 0.9682\n",
      "Epoch 20/250\n",
      "1s - loss: 0.0879 - acc: 0.9671 - val_loss: 0.0845 - val_acc: 0.9684\n",
      "Epoch 21/250\n",
      "1s - loss: 0.0879 - acc: 0.9672 - val_loss: 0.0850 - val_acc: 0.9683\n",
      "Epoch 22/250\n",
      "1s - loss: 0.0878 - acc: 0.9674 - val_loss: 0.0843 - val_acc: 0.9682\n",
      "Epoch 23/250\n",
      "1s - loss: 0.0875 - acc: 0.9672 - val_loss: 0.0841 - val_acc: 0.9684\n",
      "Epoch 24/250\n",
      "1s - loss: 0.0871 - acc: 0.9673 - val_loss: 0.0847 - val_acc: 0.9682\n",
      "Epoch 25/250\n",
      "1s - loss: 0.0873 - acc: 0.9671 - val_loss: 0.0843 - val_acc: 0.9684\n",
      "Epoch 26/250\n",
      "1s - loss: 0.0872 - acc: 0.9674 - val_loss: 0.0841 - val_acc: 0.9684\n",
      "Epoch 27/250\n",
      "0s - loss: 0.0870 - acc: 0.9675 - val_loss: 0.0841 - val_acc: 0.9685\n",
      "Epoch 28/250\n",
      "1s - loss: 0.0870 - acc: 0.9673 - val_loss: 0.0841 - val_acc: 0.9682\n",
      "Epoch 29/250\n",
      "1s - loss: 0.0869 - acc: 0.9675 - val_loss: 0.0846 - val_acc: 0.9685\n",
      "Epoch 30/250\n",
      "1s - loss: 0.0865 - acc: 0.9674 - val_loss: 0.0843 - val_acc: 0.9684\n",
      "Epoch 31/250\n",
      "1s - loss: 0.0868 - acc: 0.9675 - val_loss: 0.0850 - val_acc: 0.9681\n",
      "Epoch 32/250\n",
      "1s - loss: 0.0863 - acc: 0.9676 - val_loss: 0.0837 - val_acc: 0.9684\n",
      "Epoch 33/250\n",
      "1s - loss: 0.0862 - acc: 0.9675 - val_loss: 0.0839 - val_acc: 0.9682\n",
      "Epoch 34/250\n",
      "1s - loss: 0.0862 - acc: 0.9678 - val_loss: 0.0842 - val_acc: 0.9683\n",
      "Epoch 35/250\n",
      "1s - loss: 0.0861 - acc: 0.9677 - val_loss: 0.0842 - val_acc: 0.9680\n",
      "Epoch 36/250\n",
      "1s - loss: 0.0861 - acc: 0.9674 - val_loss: 0.0840 - val_acc: 0.9683\n",
      "Epoch 37/250\n",
      "1s - loss: 0.0860 - acc: 0.9674 - val_loss: 0.0838 - val_acc: 0.9685\n",
      "Epoch 38/250\n",
      "1s - loss: 0.0860 - acc: 0.9677 - val_loss: 0.0841 - val_acc: 0.9686\n",
      "Epoch 39/250\n",
      "1s - loss: 0.0858 - acc: 0.9678 - val_loss: 0.0843 - val_acc: 0.9685\n",
      "Epoch 40/250\n",
      "1s - loss: 0.0863 - acc: 0.9674 - val_loss: 0.0844 - val_acc: 0.9684\n",
      "Epoch 41/250\n",
      "1s - loss: 0.0860 - acc: 0.9675 - val_loss: 0.0839 - val_acc: 0.9684\n",
      "Epoch 42/250\n",
      "1s - loss: 0.0854 - acc: 0.9678 - val_loss: 0.0843 - val_acc: 0.9681\n",
      "Epoch 43/250\n",
      "1s - loss: 0.0854 - acc: 0.9677 - val_loss: 0.0840 - val_acc: 0.9685\n",
      "Epoch 44/250\n",
      "1s - loss: 0.0851 - acc: 0.9678 - val_loss: 0.0844 - val_acc: 0.9686\n",
      "Epoch 45/250\n",
      "1s - loss: 0.0853 - acc: 0.9677 - val_loss: 0.0837 - val_acc: 0.9687\n",
      "Epoch 46/250\n",
      "1s - loss: 0.0852 - acc: 0.9676 - val_loss: 0.0838 - val_acc: 0.9685\n",
      "Epoch 47/250\n",
      "1s - loss: 0.0853 - acc: 0.9677 - val_loss: 0.0842 - val_acc: 0.9685\n",
      "Epoch 48/250\n",
      "1s - loss: 0.0854 - acc: 0.9678 - val_loss: 0.0842 - val_acc: 0.9684\n",
      "Epoch 49/250\n",
      "1s - loss: 0.0852 - acc: 0.9678 - val_loss: 0.0841 - val_acc: 0.9682\n",
      "Epoch 50/250\n",
      "1s - loss: 0.0850 - acc: 0.9678 - val_loss: 0.0843 - val_acc: 0.9687\n",
      "Epoch 51/250\n",
      "1s - loss: 0.0850 - acc: 0.9679 - val_loss: 0.0840 - val_acc: 0.9683\n",
      "Epoch 52/250\n",
      "1s - loss: 0.0848 - acc: 0.9681 - val_loss: 0.0842 - val_acc: 0.9681\n",
      "Epoch 53/250\n",
      "1s - loss: 0.0849 - acc: 0.9679 - val_loss: 0.0846 - val_acc: 0.9683\n",
      "Epoch 54/250\n",
      "1s - loss: 0.0848 - acc: 0.9679 - val_loss: 0.0839 - val_acc: 0.9683\n",
      "Epoch 55/250\n",
      "1s - loss: 0.0846 - acc: 0.9680 - val_loss: 0.0843 - val_acc: 0.9685\n",
      "Epoch 56/250\n",
      "1s - loss: 0.0847 - acc: 0.9678 - val_loss: 0.0840 - val_acc: 0.9683\n",
      "Epoch 57/250\n",
      "1s - loss: 0.0847 - acc: 0.9680 - val_loss: 0.0844 - val_acc: 0.9682\n",
      "Epoch 58/250\n",
      "1s - loss: 0.0847 - acc: 0.9681 - val_loss: 0.0838 - val_acc: 0.9684\n",
      "Epoch 59/250\n",
      "1s - loss: 0.0843 - acc: 0.9680 - val_loss: 0.0840 - val_acc: 0.9684\n",
      "Epoch 60/250\n",
      "1s - loss: 0.0839 - acc: 0.9683 - val_loss: 0.0841 - val_acc: 0.9684\n",
      "Epoch 61/250\n",
      "1s - loss: 0.0842 - acc: 0.9682 - val_loss: 0.0843 - val_acc: 0.9683\n",
      "Epoch 62/250\n",
      "1s - loss: 0.0845 - acc: 0.9679 - val_loss: 0.0845 - val_acc: 0.9681\n",
      "Epoch 63/250\n",
      "1s - loss: 0.0843 - acc: 0.9680 - val_loss: 0.0838 - val_acc: 0.9684\n",
      "Epoch 64/250\n",
      "1s - loss: 0.0841 - acc: 0.9681 - val_loss: 0.0843 - val_acc: 0.9683\n",
      "Epoch 65/250\n",
      "1s - loss: 0.0842 - acc: 0.9680 - val_loss: 0.0840 - val_acc: 0.9686\n",
      "Epoch 66/250\n",
      "1s - loss: 0.0837 - acc: 0.9683 - val_loss: 0.0841 - val_acc: 0.9685\n",
      "Epoch 67/250\n",
      "1s - loss: 0.0839 - acc: 0.9681 - val_loss: 0.0840 - val_acc: 0.9685\n",
      "Epoch 68/250\n",
      "1s - loss: 0.0840 - acc: 0.9682 - val_loss: 0.0844 - val_acc: 0.9683\n",
      "Epoch 69/250\n",
      "1s - loss: 0.0836 - acc: 0.9681 - val_loss: 0.0846 - val_acc: 0.9683\n",
      "Epoch 70/250\n",
      "1s - loss: 0.0838 - acc: 0.9683 - val_loss: 0.0841 - val_acc: 0.9685\n",
      "Epoch 71/250\n",
      "1s - loss: 0.0837 - acc: 0.9682 - val_loss: 0.0845 - val_acc: 0.9681\n",
      "Epoch 72/250\n",
      "1s - loss: 0.0841 - acc: 0.9682 - val_loss: 0.0843 - val_acc: 0.9681\n",
      "Epoch 73/250\n",
      "1s - loss: 0.0835 - acc: 0.9683 - val_loss: 0.0842 - val_acc: 0.9683\n",
      "Epoch 74/250\n",
      "1s - loss: 0.0835 - acc: 0.9684 - val_loss: 0.0839 - val_acc: 0.9682\n",
      "Epoch 75/250\n",
      "1s - loss: 0.0835 - acc: 0.9683 - val_loss: 0.0844 - val_acc: 0.9685\n",
      "Epoch 76/250\n",
      "1s - loss: 0.0833 - acc: 0.9685 - val_loss: 0.0842 - val_acc: 0.9684\n",
      "Epoch 77/250\n",
      "1s - loss: 0.0835 - acc: 0.9681 - val_loss: 0.0846 - val_acc: 0.9683\n",
      "Epoch 78/250\n",
      "1s - loss: 0.0834 - acc: 0.9685 - val_loss: 0.0843 - val_acc: 0.9683\n",
      "Epoch 79/250\n",
      "0s - loss: 0.0836 - acc: 0.9682 - val_loss: 0.0841 - val_acc: 0.9684\n",
      "Epoch 80/250\n",
      "1s - loss: 0.0833 - acc: 0.9682 - val_loss: 0.0840 - val_acc: 0.9682\n",
      "Epoch 81/250\n",
      "1s - loss: 0.0829 - acc: 0.9683 - val_loss: 0.0846 - val_acc: 0.9682\n",
      "Epoch 82/250\n",
      "1s - loss: 0.0831 - acc: 0.9683 - val_loss: 0.0841 - val_acc: 0.9682\n",
      "Epoch 83/250\n",
      "1s - loss: 0.0830 - acc: 0.9685 - val_loss: 0.0854 - val_acc: 0.9684\n",
      "Epoch 84/250\n",
      "1s - loss: 0.0831 - acc: 0.9683 - val_loss: 0.0845 - val_acc: 0.9682\n",
      "Epoch 85/250\n",
      "1s - loss: 0.0829 - acc: 0.9685 - val_loss: 0.0841 - val_acc: 0.9684\n",
      "Epoch 86/250\n",
      "1s - loss: 0.0831 - acc: 0.9682 - val_loss: 0.0842 - val_acc: 0.9687\n",
      "Epoch 87/250\n",
      "1s - loss: 0.0830 - acc: 0.9685 - val_loss: 0.0844 - val_acc: 0.9684\n",
      "Epoch 88/250\n",
      "1s - loss: 0.0830 - acc: 0.9684 - val_loss: 0.0844 - val_acc: 0.9681\n",
      "Epoch 89/250\n",
      "1s - loss: 0.0830 - acc: 0.9683 - val_loss: 0.0840 - val_acc: 0.9684\n",
      "Epoch 90/250\n",
      "1s - loss: 0.0828 - acc: 0.9687 - val_loss: 0.0844 - val_acc: 0.9683\n",
      "Epoch 91/250\n",
      "1s - loss: 0.0829 - acc: 0.9686 - val_loss: 0.0841 - val_acc: 0.9685\n",
      "Epoch 92/250\n",
      "1s - loss: 0.0827 - acc: 0.9684 - val_loss: 0.0841 - val_acc: 0.9683\n",
      "Epoch 93/250\n",
      "1s - loss: 0.0830 - acc: 0.9682 - val_loss: 0.0842 - val_acc: 0.9683\n",
      "Epoch 94/250\n",
      "1s - loss: 0.0823 - acc: 0.9687 - val_loss: 0.0847 - val_acc: 0.9682\n",
      "Epoch 95/250\n",
      "1s - loss: 0.0825 - acc: 0.9687 - val_loss: 0.0847 - val_acc: 0.9682\n",
      "Epoch 96/250\n",
      "1s - loss: 0.0824 - acc: 0.9684 - val_loss: 0.0842 - val_acc: 0.9683\n",
      "Epoch 97/250\n",
      "1s - loss: 0.0826 - acc: 0.9684 - val_loss: 0.0841 - val_acc: 0.9684\n",
      "Epoch 98/250\n",
      "1s - loss: 0.0825 - acc: 0.9685 - val_loss: 0.0841 - val_acc: 0.9683\n",
      "Epoch 99/250\n",
      "1s - loss: 0.0825 - acc: 0.9685 - val_loss: 0.0842 - val_acc: 0.9684\n",
      "Epoch 100/250\n",
      "1s - loss: 0.0827 - acc: 0.9688 - val_loss: 0.0841 - val_acc: 0.9683\n",
      "Epoch 101/250\n",
      "1s - loss: 0.0825 - acc: 0.9686 - val_loss: 0.0843 - val_acc: 0.9684\n",
      "Epoch 102/250\n",
      "1s - loss: 0.0826 - acc: 0.9684 - val_loss: 0.0845 - val_acc: 0.9685\n",
      "Epoch 103/250\n",
      "1s - loss: 0.0823 - acc: 0.9685 - val_loss: 0.0842 - val_acc: 0.9681\n",
      "Epoch 104/250\n",
      "1s - loss: 0.0824 - acc: 0.9685 - val_loss: 0.0844 - val_acc: 0.9684\n",
      "Epoch 105/250\n",
      "1s - loss: 0.0823 - acc: 0.9684 - val_loss: 0.0841 - val_acc: 0.9684\n",
      "Epoch 106/250\n",
      "1s - loss: 0.0820 - acc: 0.9687 - val_loss: 0.0847 - val_acc: 0.9681\n",
      "Epoch 107/250\n",
      "1s - loss: 0.0824 - acc: 0.9686 - val_loss: 0.0844 - val_acc: 0.9682\n",
      "Epoch 108/250\n",
      "1s - loss: 0.0823 - acc: 0.9686 - val_loss: 0.0848 - val_acc: 0.9680\n",
      "Epoch 109/250\n",
      "1s - loss: 0.0822 - acc: 0.9686 - val_loss: 0.0844 - val_acc: 0.9680\n",
      "Epoch 110/250\n",
      "1s - loss: 0.0823 - acc: 0.9685 - val_loss: 0.0846 - val_acc: 0.9682\n",
      "Epoch 111/250\n",
      "1s - loss: 0.0825 - acc: 0.9686 - val_loss: 0.0850 - val_acc: 0.9679\n",
      "Epoch 112/250\n",
      "1s - loss: 0.0822 - acc: 0.9686 - val_loss: 0.0843 - val_acc: 0.9684\n",
      "Epoch 113/250\n",
      "1s - loss: 0.0822 - acc: 0.9686 - val_loss: 0.0844 - val_acc: 0.9681\n",
      "Epoch 114/250\n",
      "1s - loss: 0.0820 - acc: 0.9687 - val_loss: 0.0842 - val_acc: 0.9683\n",
      "Epoch 115/250\n",
      "1s - loss: 0.0821 - acc: 0.9686 - val_loss: 0.0842 - val_acc: 0.9681\n",
      "Epoch 116/250\n",
      "1s - loss: 0.0822 - acc: 0.9687 - val_loss: 0.0844 - val_acc: 0.9681\n",
      "Epoch 117/250\n",
      "1s - loss: 0.0822 - acc: 0.9687 - val_loss: 0.0845 - val_acc: 0.9681\n",
      "Epoch 118/250\n",
      "1s - loss: 0.0820 - acc: 0.9685 - val_loss: 0.0842 - val_acc: 0.9684\n",
      "Epoch 119/250\n",
      "1s - loss: 0.0820 - acc: 0.9687 - val_loss: 0.0846 - val_acc: 0.9681\n",
      "Epoch 120/250\n",
      "1s - loss: 0.0818 - acc: 0.9689 - val_loss: 0.0847 - val_acc: 0.9680\n",
      "Epoch 121/250\n",
      "1s - loss: 0.0819 - acc: 0.9686 - val_loss: 0.0846 - val_acc: 0.9682\n",
      "Epoch 122/250\n",
      "1s - loss: 0.0820 - acc: 0.9686 - val_loss: 0.0845 - val_acc: 0.9680\n",
      "Epoch 123/250\n",
      "1s - loss: 0.0820 - acc: 0.9686 - val_loss: 0.0844 - val_acc: 0.9682\n",
      "Epoch 124/250\n",
      "1s - loss: 0.0818 - acc: 0.9688 - val_loss: 0.0846 - val_acc: 0.9682\n",
      "Epoch 125/250\n",
      "1s - loss: 0.0816 - acc: 0.9687 - val_loss: 0.0846 - val_acc: 0.9683\n",
      "Epoch 126/250\n",
      "1s - loss: 0.0819 - acc: 0.9687 - val_loss: 0.0847 - val_acc: 0.9680\n",
      "Epoch 127/250\n",
      "1s - loss: 0.0817 - acc: 0.9688 - val_loss: 0.0842 - val_acc: 0.9683\n",
      "Epoch 128/250\n",
      "1s - loss: 0.0817 - acc: 0.9687 - val_loss: 0.0845 - val_acc: 0.9681\n",
      "Epoch 129/250\n",
      "1s - loss: 0.0817 - acc: 0.9688 - val_loss: 0.0849 - val_acc: 0.9679\n",
      "Epoch 130/250\n",
      "1s - loss: 0.0817 - acc: 0.9687 - val_loss: 0.0847 - val_acc: 0.9680\n",
      "Epoch 131/250\n",
      "1s - loss: 0.0817 - acc: 0.9686 - val_loss: 0.0850 - val_acc: 0.9682\n",
      "Epoch 132/250\n",
      "1s - loss: 0.0814 - acc: 0.9690 - val_loss: 0.0849 - val_acc: 0.9681\n",
      "Epoch 133/250\n",
      "1s - loss: 0.0816 - acc: 0.9687 - val_loss: 0.0847 - val_acc: 0.9683\n",
      "Epoch 134/250\n",
      "1s - loss: 0.0813 - acc: 0.9686 - val_loss: 0.0852 - val_acc: 0.9682\n",
      "Epoch 135/250\n",
      "1s - loss: 0.0817 - acc: 0.9688 - val_loss: 0.0846 - val_acc: 0.9682\n",
      "Epoch 136/250\n",
      "1s - loss: 0.0814 - acc: 0.9689 - val_loss: 0.0844 - val_acc: 0.9681\n",
      "Epoch 137/250\n",
      "1s - loss: 0.0815 - acc: 0.9687 - val_loss: 0.0847 - val_acc: 0.9684\n",
      "Epoch 138/250\n",
      "1s - loss: 0.0815 - acc: 0.9688 - val_loss: 0.0851 - val_acc: 0.9683\n",
      "Epoch 139/250\n",
      "1s - loss: 0.0814 - acc: 0.9687 - val_loss: 0.0849 - val_acc: 0.9682\n",
      "Epoch 140/250\n",
      "1s - loss: 0.0812 - acc: 0.9690 - val_loss: 0.0850 - val_acc: 0.9681\n",
      "Epoch 141/250\n",
      "1s - loss: 0.0814 - acc: 0.9689 - val_loss: 0.0846 - val_acc: 0.9682\n",
      "Epoch 142/250\n",
      "1s - loss: 0.0814 - acc: 0.9688 - val_loss: 0.0849 - val_acc: 0.9678\n",
      "Epoch 143/250\n",
      "1s - loss: 0.0812 - acc: 0.9689 - val_loss: 0.0850 - val_acc: 0.9682\n",
      "Epoch 144/250\n",
      "1s - loss: 0.0812 - acc: 0.9689 - val_loss: 0.0849 - val_acc: 0.9681\n",
      "Epoch 145/250\n",
      "1s - loss: 0.0815 - acc: 0.9690 - val_loss: 0.0846 - val_acc: 0.9683\n",
      "Epoch 146/250\n",
      "1s - loss: 0.0814 - acc: 0.9685 - val_loss: 0.0849 - val_acc: 0.9677\n",
      "Epoch 147/250\n",
      "1s - loss: 0.0812 - acc: 0.9689 - val_loss: 0.0853 - val_acc: 0.9680\n",
      "Epoch 148/250\n",
      "1s - loss: 0.0812 - acc: 0.9688 - val_loss: 0.0851 - val_acc: 0.9680\n",
      "Epoch 149/250\n",
      "1s - loss: 0.0810 - acc: 0.9690 - val_loss: 0.0847 - val_acc: 0.9682\n",
      "Epoch 150/250\n",
      "1s - loss: 0.0812 - acc: 0.9690 - val_loss: 0.0846 - val_acc: 0.9682\n",
      "Epoch 151/250\n",
      "1s - loss: 0.0810 - acc: 0.9691 - val_loss: 0.0856 - val_acc: 0.9678\n",
      "Epoch 152/250\n",
      "1s - loss: 0.0809 - acc: 0.9689 - val_loss: 0.0848 - val_acc: 0.9681\n",
      "Epoch 153/250\n",
      "1s - loss: 0.0811 - acc: 0.9688 - val_loss: 0.0855 - val_acc: 0.9680\n",
      "Epoch 154/250\n",
      "1s - loss: 0.0808 - acc: 0.9691 - val_loss: 0.0850 - val_acc: 0.9679\n",
      "Epoch 155/250\n",
      "1s - loss: 0.0811 - acc: 0.9690 - val_loss: 0.0852 - val_acc: 0.9679\n",
      "Epoch 156/250\n",
      "1s - loss: 0.0811 - acc: 0.9691 - val_loss: 0.0844 - val_acc: 0.9682\n",
      "Epoch 157/250\n",
      "1s - loss: 0.0810 - acc: 0.9690 - val_loss: 0.0850 - val_acc: 0.9678\n",
      "Epoch 158/250\n",
      "1s - loss: 0.0812 - acc: 0.9687 - val_loss: 0.0852 - val_acc: 0.9682\n",
      "Epoch 159/250\n",
      "1s - loss: 0.0810 - acc: 0.9688 - val_loss: 0.0853 - val_acc: 0.9684\n",
      "Epoch 160/250\n",
      "1s - loss: 0.0808 - acc: 0.9689 - val_loss: 0.0853 - val_acc: 0.9680\n",
      "Epoch 161/250\n",
      "1s - loss: 0.0808 - acc: 0.9690 - val_loss: 0.0848 - val_acc: 0.9682\n",
      "Epoch 162/250\n",
      "1s - loss: 0.0810 - acc: 0.9689 - val_loss: 0.0849 - val_acc: 0.9680\n",
      "Epoch 163/250\n",
      "1s - loss: 0.0810 - acc: 0.9691 - val_loss: 0.0849 - val_acc: 0.9680\n",
      "Epoch 164/250\n",
      "1s - loss: 0.0808 - acc: 0.9689 - val_loss: 0.0850 - val_acc: 0.9681\n",
      "Epoch 165/250\n",
      "1s - loss: 0.0806 - acc: 0.9693 - val_loss: 0.0849 - val_acc: 0.9681\n",
      "Epoch 166/250\n",
      "1s - loss: 0.0806 - acc: 0.9693 - val_loss: 0.0851 - val_acc: 0.9680\n",
      "Epoch 167/250\n",
      "1s - loss: 0.0808 - acc: 0.9690 - val_loss: 0.0850 - val_acc: 0.9680\n",
      "Epoch 168/250\n",
      "1s - loss: 0.0811 - acc: 0.9689 - val_loss: 0.0852 - val_acc: 0.9678\n",
      "Epoch 169/250\n",
      "1s - loss: 0.0807 - acc: 0.9691 - val_loss: 0.0854 - val_acc: 0.9680\n",
      "Epoch 170/250\n",
      "1s - loss: 0.0806 - acc: 0.9692 - val_loss: 0.0848 - val_acc: 0.9680\n",
      "Epoch 171/250\n",
      "1s - loss: 0.0808 - acc: 0.9689 - val_loss: 0.0854 - val_acc: 0.9681\n",
      "Epoch 172/250\n",
      "1s - loss: 0.0807 - acc: 0.9689 - val_loss: 0.0853 - val_acc: 0.9681\n",
      "Epoch 173/250\n",
      "1s - loss: 0.0808 - acc: 0.9691 - val_loss: 0.0849 - val_acc: 0.9677\n",
      "Epoch 174/250\n",
      "0s - loss: 0.0806 - acc: 0.9691 - val_loss: 0.0849 - val_acc: 0.9681\n",
      "Epoch 175/250\n",
      "1s - loss: 0.0803 - acc: 0.9693 - val_loss: 0.0852 - val_acc: 0.9682\n",
      "Epoch 176/250\n",
      "1s - loss: 0.0806 - acc: 0.9691 - val_loss: 0.0849 - val_acc: 0.9682\n",
      "Epoch 177/250\n",
      "1s - loss: 0.0805 - acc: 0.9690 - val_loss: 0.0850 - val_acc: 0.9682\n",
      "Epoch 178/250\n",
      "1s - loss: 0.0807 - acc: 0.9690 - val_loss: 0.0850 - val_acc: 0.9680\n",
      "Epoch 179/250\n",
      "1s - loss: 0.0807 - acc: 0.9690 - val_loss: 0.0855 - val_acc: 0.9678\n",
      "Epoch 180/250\n",
      "1s - loss: 0.0806 - acc: 0.9691 - val_loss: 0.0856 - val_acc: 0.9682\n",
      "Epoch 181/250\n",
      "1s - loss: 0.0809 - acc: 0.9690 - val_loss: 0.0852 - val_acc: 0.9679\n",
      "Epoch 182/250\n",
      "1s - loss: 0.0805 - acc: 0.9692 - val_loss: 0.0849 - val_acc: 0.9681\n",
      "Epoch 183/250\n",
      "1s - loss: 0.0802 - acc: 0.9692 - val_loss: 0.0850 - val_acc: 0.9681\n",
      "Epoch 184/250\n",
      "1s - loss: 0.0804 - acc: 0.9693 - val_loss: 0.0851 - val_acc: 0.9682\n",
      "Epoch 185/250\n",
      "1s - loss: 0.0802 - acc: 0.9691 - val_loss: 0.0852 - val_acc: 0.9681\n",
      "Epoch 186/250\n",
      "1s - loss: 0.0805 - acc: 0.9690 - val_loss: 0.0851 - val_acc: 0.9681\n",
      "Epoch 187/250\n",
      "0s - loss: 0.0805 - acc: 0.9692 - val_loss: 0.0854 - val_acc: 0.9680\n",
      "Epoch 188/250\n",
      "1s - loss: 0.0802 - acc: 0.9691 - val_loss: 0.0857 - val_acc: 0.9680\n",
      "Epoch 189/250\n",
      "1s - loss: 0.0802 - acc: 0.9691 - val_loss: 0.0852 - val_acc: 0.9678\n",
      "Epoch 190/250\n",
      "0s - loss: 0.0804 - acc: 0.9688 - val_loss: 0.0851 - val_acc: 0.9681\n",
      "Epoch 191/250\n",
      "0s - loss: 0.0802 - acc: 0.9692 - val_loss: 0.0856 - val_acc: 0.9678\n",
      "Epoch 192/250\n",
      "1s - loss: 0.0802 - acc: 0.9693 - val_loss: 0.0852 - val_acc: 0.9679\n",
      "Epoch 193/250\n",
      "1s - loss: 0.0802 - acc: 0.9691 - val_loss: 0.0852 - val_acc: 0.9680\n",
      "Epoch 194/250\n",
      "1s - loss: 0.0802 - acc: 0.9691 - val_loss: 0.0855 - val_acc: 0.9680\n",
      "Epoch 195/250\n",
      "1s - loss: 0.0804 - acc: 0.9692 - val_loss: 0.0853 - val_acc: 0.9681\n",
      "Epoch 196/250\n",
      "1s - loss: 0.0800 - acc: 0.9692 - val_loss: 0.0857 - val_acc: 0.9679\n",
      "Epoch 197/250\n",
      "1s - loss: 0.0800 - acc: 0.9693 - val_loss: 0.0852 - val_acc: 0.9680\n",
      "Epoch 198/250\n",
      "1s - loss: 0.0800 - acc: 0.9692 - val_loss: 0.0856 - val_acc: 0.9679\n",
      "Epoch 199/250\n",
      "0s - loss: 0.0799 - acc: 0.9693 - val_loss: 0.0853 - val_acc: 0.9682\n",
      "Epoch 200/250\n",
      "1s - loss: 0.0801 - acc: 0.9693 - val_loss: 0.0856 - val_acc: 0.9677\n",
      "Epoch 201/250\n",
      "1s - loss: 0.0802 - acc: 0.9691 - val_loss: 0.0852 - val_acc: 0.9678\n",
      "Epoch 202/250\n",
      "1s - loss: 0.0801 - acc: 0.9693 - val_loss: 0.0853 - val_acc: 0.9682\n",
      "Epoch 203/250\n",
      "1s - loss: 0.0801 - acc: 0.9691 - val_loss: 0.0850 - val_acc: 0.9679\n",
      "Epoch 204/250\n",
      "1s - loss: 0.0800 - acc: 0.9692 - val_loss: 0.0853 - val_acc: 0.9679\n",
      "Epoch 205/250\n",
      "1s - loss: 0.0800 - acc: 0.9693 - val_loss: 0.0851 - val_acc: 0.9680\n",
      "Epoch 206/250\n",
      "1s - loss: 0.0803 - acc: 0.9692 - val_loss: 0.0857 - val_acc: 0.9680\n",
      "Epoch 207/250\n",
      "1s - loss: 0.0798 - acc: 0.9694 - val_loss: 0.0854 - val_acc: 0.9682\n",
      "Epoch 208/250\n",
      "1s - loss: 0.0801 - acc: 0.9693 - val_loss: 0.0850 - val_acc: 0.9681\n",
      "Epoch 209/250\n",
      "1s - loss: 0.0796 - acc: 0.9693 - val_loss: 0.0851 - val_acc: 0.9678\n",
      "Epoch 210/250\n",
      "1s - loss: 0.0800 - acc: 0.9692 - val_loss: 0.0852 - val_acc: 0.9680\n",
      "Epoch 211/250\n",
      "1s - loss: 0.0797 - acc: 0.9693 - val_loss: 0.0853 - val_acc: 0.9680\n",
      "Epoch 212/250\n",
      "1s - loss: 0.0799 - acc: 0.9693 - val_loss: 0.0855 - val_acc: 0.9682\n",
      "Epoch 213/250\n",
      "1s - loss: 0.0800 - acc: 0.9692 - val_loss: 0.0858 - val_acc: 0.9680\n",
      "Epoch 214/250\n",
      "1s - loss: 0.0797 - acc: 0.9695 - val_loss: 0.0858 - val_acc: 0.9678\n",
      "Epoch 215/250\n",
      "1s - loss: 0.0798 - acc: 0.9692 - val_loss: 0.0857 - val_acc: 0.9679\n",
      "Epoch 216/250\n",
      "1s - loss: 0.0799 - acc: 0.9693 - val_loss: 0.0857 - val_acc: 0.9680\n",
      "Epoch 217/250\n",
      "0s - loss: 0.0797 - acc: 0.9693 - val_loss: 0.0857 - val_acc: 0.9681\n",
      "Epoch 218/250\n",
      "1s - loss: 0.0795 - acc: 0.9694 - val_loss: 0.0855 - val_acc: 0.9681\n",
      "Epoch 219/250\n",
      "1s - loss: 0.0800 - acc: 0.9695 - val_loss: 0.0853 - val_acc: 0.9680\n",
      "Epoch 220/250\n",
      "1s - loss: 0.0798 - acc: 0.9693 - val_loss: 0.0858 - val_acc: 0.9679\n",
      "Epoch 221/250\n",
      "1s - loss: 0.0799 - acc: 0.9694 - val_loss: 0.0854 - val_acc: 0.9680\n",
      "Epoch 222/250\n",
      "1s - loss: 0.0797 - acc: 0.9693 - val_loss: 0.0849 - val_acc: 0.9684\n",
      "Epoch 223/250\n",
      "1s - loss: 0.0797 - acc: 0.9694 - val_loss: 0.0854 - val_acc: 0.9681\n",
      "Epoch 224/250\n",
      "1s - loss: 0.0798 - acc: 0.9693 - val_loss: 0.0854 - val_acc: 0.9682\n",
      "Epoch 225/250\n",
      "1s - loss: 0.0796 - acc: 0.9694 - val_loss: 0.0852 - val_acc: 0.9680\n",
      "Epoch 226/250\n",
      "1s - loss: 0.0801 - acc: 0.9691 - val_loss: 0.0855 - val_acc: 0.9680\n",
      "Epoch 227/250\n",
      "1s - loss: 0.0793 - acc: 0.9695 - val_loss: 0.0851 - val_acc: 0.9677\n",
      "Epoch 228/250\n",
      "1s - loss: 0.0797 - acc: 0.9693 - val_loss: 0.0855 - val_acc: 0.9681\n",
      "Epoch 229/250\n",
      "0s - loss: 0.0797 - acc: 0.9694 - val_loss: 0.0857 - val_acc: 0.9678\n",
      "Epoch 230/250\n",
      "1s - loss: 0.0797 - acc: 0.9694 - val_loss: 0.0856 - val_acc: 0.9680\n",
      "Epoch 231/250\n",
      "1s - loss: 0.0798 - acc: 0.9693 - val_loss: 0.0860 - val_acc: 0.9678\n",
      "Epoch 232/250\n",
      "1s - loss: 0.0796 - acc: 0.9695 - val_loss: 0.0856 - val_acc: 0.9677\n",
      "Epoch 233/250\n",
      "1s - loss: 0.0797 - acc: 0.9695 - val_loss: 0.0853 - val_acc: 0.9678\n",
      "Epoch 234/250\n",
      "1s - loss: 0.0796 - acc: 0.9693 - val_loss: 0.0854 - val_acc: 0.9680\n",
      "Epoch 235/250\n",
      "0s - loss: 0.0797 - acc: 0.9693 - val_loss: 0.0855 - val_acc: 0.9681\n",
      "Epoch 236/250\n",
      "1s - loss: 0.0797 - acc: 0.9692 - val_loss: 0.0857 - val_acc: 0.9678\n",
      "Epoch 237/250\n",
      "1s - loss: 0.0796 - acc: 0.9695 - val_loss: 0.0855 - val_acc: 0.9678\n",
      "Epoch 238/250\n",
      "1s - loss: 0.0796 - acc: 0.9694 - val_loss: 0.0857 - val_acc: 0.9678\n",
      "Epoch 239/250\n",
      "1s - loss: 0.0796 - acc: 0.9693 - val_loss: 0.0855 - val_acc: 0.9678\n",
      "Epoch 240/250\n",
      "1s - loss: 0.0793 - acc: 0.9694 - val_loss: 0.0851 - val_acc: 0.9676\n",
      "Epoch 241/250\n",
      "1s - loss: 0.0794 - acc: 0.9695 - val_loss: 0.0854 - val_acc: 0.9679\n",
      "Epoch 242/250\n",
      "1s - loss: 0.0793 - acc: 0.9694 - val_loss: 0.0853 - val_acc: 0.9677\n",
      "Epoch 243/250\n",
      "1s - loss: 0.0794 - acc: 0.9694 - val_loss: 0.0857 - val_acc: 0.9679\n",
      "Epoch 244/250\n",
      "1s - loss: 0.0796 - acc: 0.9693 - val_loss: 0.0855 - val_acc: 0.9680\n",
      "Epoch 245/250\n",
      "1s - loss: 0.0792 - acc: 0.9695 - val_loss: 0.0855 - val_acc: 0.9677\n",
      "Epoch 246/250\n",
      "1s - loss: 0.0792 - acc: 0.9695 - val_loss: 0.0855 - val_acc: 0.9680\n",
      "Epoch 247/250\n",
      "1s - loss: 0.0794 - acc: 0.9693 - val_loss: 0.0855 - val_acc: 0.9680\n",
      "Epoch 248/250\n",
      "1s - loss: 0.0793 - acc: 0.9695 - val_loss: 0.0861 - val_acc: 0.9678\n",
      "Epoch 249/250\n",
      "1s - loss: 0.0790 - acc: 0.9697 - val_loss: 0.0854 - val_acc: 0.9680\n",
      "Epoch 250/250\n",
      "1s - loss: 0.0793 - acc: 0.9694 - val_loss: 0.0856 - val_acc: 0.9677\n",
      "0.927572069696\n",
      "Train on 32383 samples, validate on 8096 samples\n",
      "Epoch 1/250\n",
      "1s - loss: 0.1704 - acc: 0.9426 - val_loss: 0.1006 - val_acc: 0.9659\n",
      "Epoch 2/250\n",
      "1s - loss: 0.1061 - acc: 0.9645 - val_loss: 0.0931 - val_acc: 0.9671\n",
      "Epoch 3/250\n",
      "1s - loss: 0.1002 - acc: 0.9654 - val_loss: 0.0896 - val_acc: 0.9678\n",
      "Epoch 4/250\n",
      "1s - loss: 0.0975 - acc: 0.9660 - val_loss: 0.0890 - val_acc: 0.9681\n",
      "Epoch 5/250\n",
      "1s - loss: 0.0957 - acc: 0.9663 - val_loss: 0.0876 - val_acc: 0.9680\n",
      "Epoch 6/250\n",
      "1s - loss: 0.0943 - acc: 0.9665 - val_loss: 0.0864 - val_acc: 0.9681\n",
      "Epoch 7/250\n",
      "1s - loss: 0.0931 - acc: 0.9665 - val_loss: 0.0858 - val_acc: 0.9685\n",
      "Epoch 8/250\n",
      "1s - loss: 0.0927 - acc: 0.9665 - val_loss: 0.0852 - val_acc: 0.9685\n",
      "Epoch 9/250\n",
      "1s - loss: 0.0916 - acc: 0.9666 - val_loss: 0.0852 - val_acc: 0.9681\n",
      "Epoch 10/250\n",
      "1s - loss: 0.0912 - acc: 0.9667 - val_loss: 0.0845 - val_acc: 0.9685\n",
      "Epoch 11/250\n",
      "1s - loss: 0.0909 - acc: 0.9666 - val_loss: 0.0842 - val_acc: 0.9685\n",
      "Epoch 12/250\n",
      "1s - loss: 0.0903 - acc: 0.9668 - val_loss: 0.0840 - val_acc: 0.9686\n",
      "Epoch 13/250\n",
      "1s - loss: 0.0898 - acc: 0.9671 - val_loss: 0.0838 - val_acc: 0.9686\n",
      "Epoch 14/250\n",
      "1s - loss: 0.0898 - acc: 0.9669 - val_loss: 0.0839 - val_acc: 0.9687\n",
      "Epoch 15/250\n",
      "1s - loss: 0.0897 - acc: 0.9669 - val_loss: 0.0842 - val_acc: 0.9681\n",
      "Epoch 16/250\n",
      "1s - loss: 0.0890 - acc: 0.9669 - val_loss: 0.0835 - val_acc: 0.9688\n",
      "Epoch 17/250\n",
      "1s - loss: 0.0891 - acc: 0.9668 - val_loss: 0.0833 - val_acc: 0.9685\n",
      "Epoch 18/250\n",
      "1s - loss: 0.0885 - acc: 0.9672 - val_loss: 0.0834 - val_acc: 0.9683\n",
      "Epoch 19/250\n",
      "1s - loss: 0.0884 - acc: 0.9671 - val_loss: 0.0834 - val_acc: 0.9686\n",
      "Epoch 20/250\n",
      "1s - loss: 0.0880 - acc: 0.9672 - val_loss: 0.0830 - val_acc: 0.9686\n",
      "Epoch 21/250\n",
      "1s - loss: 0.0883 - acc: 0.9670 - val_loss: 0.0833 - val_acc: 0.9684\n",
      "Epoch 22/250\n",
      "1s - loss: 0.0880 - acc: 0.9672 - val_loss: 0.0834 - val_acc: 0.9685\n",
      "Epoch 23/250\n",
      "1s - loss: 0.0877 - acc: 0.9675 - val_loss: 0.0830 - val_acc: 0.9684\n",
      "Epoch 24/250\n",
      "1s - loss: 0.0876 - acc: 0.9674 - val_loss: 0.0831 - val_acc: 0.9685\n",
      "Epoch 25/250\n",
      "1s - loss: 0.0876 - acc: 0.9673 - val_loss: 0.0829 - val_acc: 0.9688\n",
      "Epoch 26/250\n",
      "1s - loss: 0.0875 - acc: 0.9671 - val_loss: 0.0832 - val_acc: 0.9686\n",
      "Epoch 27/250\n",
      "1s - loss: 0.0874 - acc: 0.9675 - val_loss: 0.0832 - val_acc: 0.9686\n",
      "Epoch 28/250\n",
      "1s - loss: 0.0870 - acc: 0.9675 - val_loss: 0.0836 - val_acc: 0.9683\n",
      "Epoch 29/250\n",
      "1s - loss: 0.0872 - acc: 0.9673 - val_loss: 0.0830 - val_acc: 0.9686\n",
      "Epoch 30/250\n",
      "1s - loss: 0.0865 - acc: 0.9676 - val_loss: 0.0829 - val_acc: 0.9685\n",
      "Epoch 31/250\n",
      "1s - loss: 0.0870 - acc: 0.9675 - val_loss: 0.0828 - val_acc: 0.9685\n",
      "Epoch 32/250\n",
      "1s - loss: 0.0865 - acc: 0.9676 - val_loss: 0.0828 - val_acc: 0.9687\n",
      "Epoch 33/250\n",
      "1s - loss: 0.0867 - acc: 0.9675 - val_loss: 0.0827 - val_acc: 0.9684\n",
      "Epoch 34/250\n",
      "1s - loss: 0.0864 - acc: 0.9676 - val_loss: 0.0828 - val_acc: 0.9685\n",
      "Epoch 35/250\n",
      "1s - loss: 0.0860 - acc: 0.9677 - val_loss: 0.0829 - val_acc: 0.9685\n",
      "Epoch 36/250\n",
      "1s - loss: 0.0862 - acc: 0.9676 - val_loss: 0.0828 - val_acc: 0.9689\n",
      "Epoch 37/250\n",
      "1s - loss: 0.0863 - acc: 0.9677 - val_loss: 0.0830 - val_acc: 0.9687\n",
      "Epoch 38/250\n",
      "1s - loss: 0.0862 - acc: 0.9676 - val_loss: 0.0826 - val_acc: 0.9685\n",
      "Epoch 39/250\n",
      "1s - loss: 0.0863 - acc: 0.9675 - val_loss: 0.0833 - val_acc: 0.9687\n",
      "Epoch 40/250\n",
      "1s - loss: 0.0860 - acc: 0.9678 - val_loss: 0.0832 - val_acc: 0.9685\n",
      "Epoch 41/250\n",
      "1s - loss: 0.0859 - acc: 0.9678 - val_loss: 0.0833 - val_acc: 0.9684\n",
      "Epoch 42/250\n",
      "1s - loss: 0.0858 - acc: 0.9677 - val_loss: 0.0828 - val_acc: 0.9685\n",
      "Epoch 43/250\n",
      "1s - loss: 0.0859 - acc: 0.9675 - val_loss: 0.0831 - val_acc: 0.9685\n",
      "Epoch 44/250\n",
      "0s - loss: 0.0855 - acc: 0.9678 - val_loss: 0.0832 - val_acc: 0.9687\n",
      "Epoch 45/250\n",
      "1s - loss: 0.0854 - acc: 0.9679 - val_loss: 0.0833 - val_acc: 0.9684\n",
      "Epoch 46/250\n",
      "1s - loss: 0.0854 - acc: 0.9679 - val_loss: 0.0829 - val_acc: 0.9685\n",
      "Epoch 47/250\n",
      "1s - loss: 0.0851 - acc: 0.9680 - val_loss: 0.0828 - val_acc: 0.9686\n",
      "Epoch 48/250\n",
      "1s - loss: 0.0851 - acc: 0.9681 - val_loss: 0.0830 - val_acc: 0.9685\n",
      "Epoch 49/250\n",
      "1s - loss: 0.0852 - acc: 0.9679 - val_loss: 0.0826 - val_acc: 0.9685\n",
      "Epoch 50/250\n",
      "1s - loss: 0.0853 - acc: 0.9679 - val_loss: 0.0830 - val_acc: 0.9682\n",
      "Epoch 51/250\n",
      "1s - loss: 0.0851 - acc: 0.9682 - val_loss: 0.0833 - val_acc: 0.9684\n",
      "Epoch 52/250\n",
      "1s - loss: 0.0849 - acc: 0.9681 - val_loss: 0.0825 - val_acc: 0.9685\n",
      "Epoch 53/250\n",
      "1s - loss: 0.0847 - acc: 0.9682 - val_loss: 0.0827 - val_acc: 0.9686\n",
      "Epoch 54/250\n",
      "1s - loss: 0.0846 - acc: 0.9678 - val_loss: 0.0827 - val_acc: 0.9687\n",
      "Epoch 55/250\n",
      "1s - loss: 0.0846 - acc: 0.9681 - val_loss: 0.0830 - val_acc: 0.9685\n",
      "Epoch 56/250\n",
      "1s - loss: 0.0848 - acc: 0.9681 - val_loss: 0.0829 - val_acc: 0.9685\n",
      "Epoch 57/250\n",
      "1s - loss: 0.0845 - acc: 0.9681 - val_loss: 0.0825 - val_acc: 0.9686\n",
      "Epoch 58/250\n",
      "1s - loss: 0.0847 - acc: 0.9679 - val_loss: 0.0824 - val_acc: 0.9685\n",
      "Epoch 59/250\n",
      "1s - loss: 0.0849 - acc: 0.9682 - val_loss: 0.0829 - val_acc: 0.9685\n",
      "Epoch 60/250\n",
      "1s - loss: 0.0845 - acc: 0.9681 - val_loss: 0.0831 - val_acc: 0.9684\n",
      "Epoch 61/250\n",
      "1s - loss: 0.0843 - acc: 0.9680 - val_loss: 0.0828 - val_acc: 0.9686\n",
      "Epoch 62/250\n",
      "1s - loss: 0.0844 - acc: 0.9681 - val_loss: 0.0831 - val_acc: 0.9685\n",
      "Epoch 63/250\n",
      "1s - loss: 0.0845 - acc: 0.9683 - val_loss: 0.0829 - val_acc: 0.9688\n",
      "Epoch 64/250\n",
      "1s - loss: 0.0843 - acc: 0.9682 - val_loss: 0.0826 - val_acc: 0.9685\n",
      "Epoch 65/250\n",
      "1s - loss: 0.0839 - acc: 0.9682 - val_loss: 0.0830 - val_acc: 0.9684\n",
      "Epoch 66/250\n",
      "1s - loss: 0.0842 - acc: 0.9682 - val_loss: 0.0830 - val_acc: 0.9686\n",
      "Epoch 67/250\n",
      "1s - loss: 0.0843 - acc: 0.9683 - val_loss: 0.0824 - val_acc: 0.9686\n",
      "Epoch 68/250\n",
      "1s - loss: 0.0843 - acc: 0.9681 - val_loss: 0.0831 - val_acc: 0.9683\n",
      "Epoch 69/250\n",
      "1s - loss: 0.0839 - acc: 0.9681 - val_loss: 0.0831 - val_acc: 0.9686\n",
      "Epoch 70/250\n",
      "1s - loss: 0.0839 - acc: 0.9683 - val_loss: 0.0828 - val_acc: 0.9686\n",
      "Epoch 71/250\n",
      "1s - loss: 0.0840 - acc: 0.9683 - val_loss: 0.0830 - val_acc: 0.9687\n",
      "Epoch 72/250\n",
      "1s - loss: 0.0839 - acc: 0.9682 - val_loss: 0.0828 - val_acc: 0.9685\n",
      "Epoch 73/250\n",
      "1s - loss: 0.0839 - acc: 0.9681 - val_loss: 0.0828 - val_acc: 0.9687\n",
      "Epoch 74/250\n",
      "1s - loss: 0.0836 - acc: 0.9682 - val_loss: 0.0831 - val_acc: 0.9682\n",
      "Epoch 75/250\n",
      "1s - loss: 0.0835 - acc: 0.9685 - val_loss: 0.0830 - val_acc: 0.9686\n",
      "Epoch 76/250\n",
      "1s - loss: 0.0836 - acc: 0.9682 - val_loss: 0.0832 - val_acc: 0.9686\n",
      "Epoch 77/250\n",
      "1s - loss: 0.0837 - acc: 0.9682 - val_loss: 0.0829 - val_acc: 0.9687\n",
      "Epoch 78/250\n",
      "1s - loss: 0.0834 - acc: 0.9684 - val_loss: 0.0830 - val_acc: 0.9686\n",
      "Epoch 79/250\n",
      "0s - loss: 0.0834 - acc: 0.9683 - val_loss: 0.0831 - val_acc: 0.9686\n",
      "Epoch 80/250\n",
      "0s - loss: 0.0834 - acc: 0.9684 - val_loss: 0.0831 - val_acc: 0.9687\n",
      "Epoch 81/250\n",
      "1s - loss: 0.0837 - acc: 0.9682 - val_loss: 0.0833 - val_acc: 0.9682\n",
      "Epoch 82/250\n",
      "1s - loss: 0.0832 - acc: 0.9683 - val_loss: 0.0831 - val_acc: 0.9686\n",
      "Epoch 83/250\n",
      "1s - loss: 0.0833 - acc: 0.9683 - val_loss: 0.0833 - val_acc: 0.9684\n",
      "Epoch 84/250\n",
      "1s - loss: 0.0833 - acc: 0.9684 - val_loss: 0.0829 - val_acc: 0.9687\n",
      "Epoch 85/250\n",
      "1s - loss: 0.0834 - acc: 0.9686 - val_loss: 0.0828 - val_acc: 0.9683\n",
      "Epoch 86/250\n",
      "1s - loss: 0.0836 - acc: 0.9684 - val_loss: 0.0834 - val_acc: 0.9683\n",
      "Epoch 87/250\n",
      "0s - loss: 0.0831 - acc: 0.9686 - val_loss: 0.0829 - val_acc: 0.9687\n",
      "Epoch 88/250\n",
      "1s - loss: 0.0828 - acc: 0.9684 - val_loss: 0.0833 - val_acc: 0.9686\n",
      "Epoch 89/250\n",
      "1s - loss: 0.0829 - acc: 0.9685 - val_loss: 0.0831 - val_acc: 0.9687\n",
      "Epoch 90/250\n",
      "1s - loss: 0.0833 - acc: 0.9684 - val_loss: 0.0833 - val_acc: 0.9685\n",
      "Epoch 91/250\n",
      "1s - loss: 0.0828 - acc: 0.9685 - val_loss: 0.0829 - val_acc: 0.9685\n",
      "Epoch 92/250\n",
      "1s - loss: 0.0827 - acc: 0.9688 - val_loss: 0.0834 - val_acc: 0.9680\n",
      "Epoch 93/250\n",
      "1s - loss: 0.0830 - acc: 0.9685 - val_loss: 0.0827 - val_acc: 0.9687\n",
      "Epoch 94/250\n",
      "1s - loss: 0.0829 - acc: 0.9685 - val_loss: 0.0832 - val_acc: 0.9687\n",
      "Epoch 95/250\n",
      "1s - loss: 0.0830 - acc: 0.9687 - val_loss: 0.0832 - val_acc: 0.9685\n",
      "Epoch 96/250\n",
      "1s - loss: 0.0829 - acc: 0.9686 - val_loss: 0.0835 - val_acc: 0.9685\n",
      "Epoch 97/250\n",
      "1s - loss: 0.0827 - acc: 0.9687 - val_loss: 0.0831 - val_acc: 0.9686\n",
      "Epoch 98/250\n",
      "1s - loss: 0.0828 - acc: 0.9686 - val_loss: 0.0828 - val_acc: 0.9686\n",
      "Epoch 99/250\n",
      "1s - loss: 0.0829 - acc: 0.9687 - val_loss: 0.0833 - val_acc: 0.9684\n",
      "Epoch 100/250\n",
      "1s - loss: 0.0829 - acc: 0.9686 - val_loss: 0.0829 - val_acc: 0.9687\n",
      "Epoch 101/250\n",
      "1s - loss: 0.0827 - acc: 0.9687 - val_loss: 0.0831 - val_acc: 0.9684\n",
      "Epoch 102/250\n",
      "1s - loss: 0.0826 - acc: 0.9687 - val_loss: 0.0834 - val_acc: 0.9684\n",
      "Epoch 103/250\n",
      "1s - loss: 0.0825 - acc: 0.9685 - val_loss: 0.0832 - val_acc: 0.9685\n",
      "Epoch 104/250\n",
      "1s - loss: 0.0826 - acc: 0.9687 - val_loss: 0.0829 - val_acc: 0.9686\n",
      "Epoch 105/250\n",
      "0s - loss: 0.0827 - acc: 0.9687 - val_loss: 0.0828 - val_acc: 0.9683\n",
      "Epoch 106/250\n",
      "1s - loss: 0.0825 - acc: 0.9686 - val_loss: 0.0836 - val_acc: 0.9683\n",
      "Epoch 107/250\n",
      "1s - loss: 0.0825 - acc: 0.9687 - val_loss: 0.0834 - val_acc: 0.9683\n",
      "Epoch 108/250\n",
      "1s - loss: 0.0825 - acc: 0.9686 - val_loss: 0.0832 - val_acc: 0.9686\n",
      "Epoch 109/250\n",
      "1s - loss: 0.0826 - acc: 0.9684 - val_loss: 0.0834 - val_acc: 0.9686\n",
      "Epoch 110/250\n",
      "1s - loss: 0.0824 - acc: 0.9687 - val_loss: 0.0834 - val_acc: 0.9686\n",
      "Epoch 111/250\n",
      "1s - loss: 0.0821 - acc: 0.9688 - val_loss: 0.0831 - val_acc: 0.9684\n",
      "Epoch 112/250\n",
      "1s - loss: 0.0822 - acc: 0.9687 - val_loss: 0.0835 - val_acc: 0.9682\n",
      "Epoch 113/250\n",
      "1s - loss: 0.0823 - acc: 0.9686 - val_loss: 0.0834 - val_acc: 0.9685\n",
      "Epoch 114/250\n",
      "1s - loss: 0.0823 - acc: 0.9688 - val_loss: 0.0834 - val_acc: 0.9685\n",
      "Epoch 115/250\n",
      "1s - loss: 0.0824 - acc: 0.9687 - val_loss: 0.0833 - val_acc: 0.9683\n",
      "Epoch 116/250\n",
      "1s - loss: 0.0820 - acc: 0.9686 - val_loss: 0.0832 - val_acc: 0.9686\n",
      "Epoch 117/250\n",
      "1s - loss: 0.0820 - acc: 0.9689 - val_loss: 0.0833 - val_acc: 0.9687\n",
      "Epoch 118/250\n",
      "1s - loss: 0.0819 - acc: 0.9689 - val_loss: 0.0834 - val_acc: 0.9683\n",
      "Epoch 119/250\n",
      "1s - loss: 0.0820 - acc: 0.9688 - val_loss: 0.0833 - val_acc: 0.9684\n",
      "Epoch 120/250\n",
      "1s - loss: 0.0821 - acc: 0.9689 - val_loss: 0.0832 - val_acc: 0.9683\n",
      "Epoch 121/250\n",
      "1s - loss: 0.0821 - acc: 0.9687 - val_loss: 0.0833 - val_acc: 0.9683\n",
      "Epoch 122/250\n",
      "1s - loss: 0.0822 - acc: 0.9686 - val_loss: 0.0830 - val_acc: 0.9685\n",
      "Epoch 123/250\n",
      "1s - loss: 0.0822 - acc: 0.9687 - val_loss: 0.0836 - val_acc: 0.9685\n",
      "Epoch 124/250\n",
      "1s - loss: 0.0819 - acc: 0.9689 - val_loss: 0.0830 - val_acc: 0.9686\n",
      "Epoch 125/250\n",
      "1s - loss: 0.0819 - acc: 0.9689 - val_loss: 0.0833 - val_acc: 0.9686\n",
      "Epoch 126/250\n",
      "1s - loss: 0.0817 - acc: 0.9689 - val_loss: 0.0834 - val_acc: 0.9683\n",
      "Epoch 127/250\n",
      "1s - loss: 0.0821 - acc: 0.9689 - val_loss: 0.0831 - val_acc: 0.9684\n",
      "Epoch 128/250\n",
      "1s - loss: 0.0819 - acc: 0.9687 - val_loss: 0.0835 - val_acc: 0.9684\n",
      "Epoch 129/250\n",
      "1s - loss: 0.0820 - acc: 0.9687 - val_loss: 0.0829 - val_acc: 0.9688\n",
      "Epoch 130/250\n",
      "1s - loss: 0.0816 - acc: 0.9691 - val_loss: 0.0833 - val_acc: 0.9685\n",
      "Epoch 131/250\n",
      "1s - loss: 0.0818 - acc: 0.9689 - val_loss: 0.0831 - val_acc: 0.9686\n",
      "Epoch 132/250\n",
      "1s - loss: 0.0819 - acc: 0.9689 - val_loss: 0.0837 - val_acc: 0.9683\n",
      "Epoch 133/250\n",
      "1s - loss: 0.0818 - acc: 0.9689 - val_loss: 0.0832 - val_acc: 0.9684\n",
      "Epoch 134/250\n",
      "1s - loss: 0.0816 - acc: 0.9688 - val_loss: 0.0833 - val_acc: 0.9684\n",
      "Epoch 135/250\n",
      "1s - loss: 0.0818 - acc: 0.9689 - val_loss: 0.0833 - val_acc: 0.9685\n",
      "Epoch 136/250\n",
      "1s - loss: 0.0816 - acc: 0.9690 - val_loss: 0.0833 - val_acc: 0.9684\n",
      "Epoch 137/250\n",
      "1s - loss: 0.0812 - acc: 0.9688 - val_loss: 0.0842 - val_acc: 0.9682\n",
      "Epoch 138/250\n",
      "1s - loss: 0.0817 - acc: 0.9688 - val_loss: 0.0835 - val_acc: 0.9685\n",
      "Epoch 139/250\n",
      "1s - loss: 0.0815 - acc: 0.9689 - val_loss: 0.0834 - val_acc: 0.9685\n",
      "Epoch 140/250\n",
      "1s - loss: 0.0816 - acc: 0.9689 - val_loss: 0.0832 - val_acc: 0.9686\n",
      "Epoch 141/250\n",
      "1s - loss: 0.0815 - acc: 0.9689 - val_loss: 0.0835 - val_acc: 0.9682\n",
      "Epoch 142/250\n",
      "1s - loss: 0.0817 - acc: 0.9690 - val_loss: 0.0834 - val_acc: 0.9684\n",
      "Epoch 143/250\n",
      "1s - loss: 0.0814 - acc: 0.9690 - val_loss: 0.0834 - val_acc: 0.9684\n",
      "Epoch 144/250\n",
      "1s - loss: 0.0814 - acc: 0.9691 - val_loss: 0.0832 - val_acc: 0.9684\n",
      "Epoch 145/250\n",
      "1s - loss: 0.0812 - acc: 0.9689 - val_loss: 0.0838 - val_acc: 0.9682\n",
      "Epoch 146/250\n",
      "1s - loss: 0.0811 - acc: 0.9689 - val_loss: 0.0833 - val_acc: 0.9686\n",
      "Epoch 147/250\n",
      "1s - loss: 0.0810 - acc: 0.9692 - val_loss: 0.0836 - val_acc: 0.9682\n",
      "Epoch 148/250\n",
      "1s - loss: 0.0812 - acc: 0.9691 - val_loss: 0.0831 - val_acc: 0.9684\n",
      "Epoch 149/250\n",
      "1s - loss: 0.0811 - acc: 0.9690 - val_loss: 0.0833 - val_acc: 0.9684\n",
      "Epoch 150/250\n",
      "1s - loss: 0.0813 - acc: 0.9689 - val_loss: 0.0834 - val_acc: 0.9685\n",
      "Epoch 151/250\n",
      "1s - loss: 0.0811 - acc: 0.9691 - val_loss: 0.0836 - val_acc: 0.9684\n",
      "Epoch 152/250\n",
      "1s - loss: 0.0811 - acc: 0.9690 - val_loss: 0.0834 - val_acc: 0.9684\n",
      "Epoch 153/250\n",
      "0s - loss: 0.0810 - acc: 0.9691 - val_loss: 0.0834 - val_acc: 0.9684\n",
      "Epoch 154/250\n",
      "1s - loss: 0.0808 - acc: 0.9692 - val_loss: 0.0832 - val_acc: 0.9683\n",
      "Epoch 155/250\n",
      "1s - loss: 0.0811 - acc: 0.9692 - val_loss: 0.0832 - val_acc: 0.9684\n",
      "Epoch 156/250\n",
      "1s - loss: 0.0812 - acc: 0.9691 - val_loss: 0.0832 - val_acc: 0.9686\n",
      "Epoch 157/250\n",
      "1s - loss: 0.0810 - acc: 0.9693 - val_loss: 0.0839 - val_acc: 0.9681\n",
      "Epoch 158/250\n",
      "1s - loss: 0.0809 - acc: 0.9692 - val_loss: 0.0837 - val_acc: 0.9684\n",
      "Epoch 159/250\n",
      "1s - loss: 0.0812 - acc: 0.9690 - val_loss: 0.0838 - val_acc: 0.9682\n",
      "Epoch 160/250\n",
      "1s - loss: 0.0810 - acc: 0.9691 - val_loss: 0.0834 - val_acc: 0.9683\n",
      "Epoch 161/250\n",
      "1s - loss: 0.0808 - acc: 0.9693 - val_loss: 0.0834 - val_acc: 0.9684\n",
      "Epoch 162/250\n",
      "1s - loss: 0.0811 - acc: 0.9690 - val_loss: 0.0838 - val_acc: 0.9683\n",
      "Epoch 163/250\n",
      "1s - loss: 0.0809 - acc: 0.9692 - val_loss: 0.0838 - val_acc: 0.9683\n",
      "Epoch 164/250\n",
      "1s - loss: 0.0811 - acc: 0.9689 - val_loss: 0.0838 - val_acc: 0.9685\n",
      "Epoch 165/250\n",
      "1s - loss: 0.0810 - acc: 0.9692 - val_loss: 0.0833 - val_acc: 0.9687\n",
      "Epoch 166/250\n",
      "1s - loss: 0.0809 - acc: 0.9691 - val_loss: 0.0836 - val_acc: 0.9681\n",
      "Epoch 167/250\n",
      "1s - loss: 0.0811 - acc: 0.9690 - val_loss: 0.0837 - val_acc: 0.9683\n",
      "Epoch 168/250\n",
      "1s - loss: 0.0805 - acc: 0.9692 - val_loss: 0.0837 - val_acc: 0.9683\n",
      "Epoch 169/250\n",
      "1s - loss: 0.0810 - acc: 0.9692 - val_loss: 0.0836 - val_acc: 0.9682\n",
      "Epoch 170/250\n",
      "1s - loss: 0.0807 - acc: 0.9691 - val_loss: 0.0836 - val_acc: 0.9682\n",
      "Epoch 171/250\n",
      "1s - loss: 0.0808 - acc: 0.9691 - val_loss: 0.0836 - val_acc: 0.9684\n",
      "Epoch 172/250\n",
      "1s - loss: 0.0809 - acc: 0.9692 - val_loss: 0.0836 - val_acc: 0.9682\n",
      "Epoch 173/250\n",
      "1s - loss: 0.0806 - acc: 0.9691 - val_loss: 0.0834 - val_acc: 0.9683\n",
      "Epoch 174/250\n",
      "1s - loss: 0.0807 - acc: 0.9692 - val_loss: 0.0835 - val_acc: 0.9685\n",
      "Epoch 175/250\n",
      "1s - loss: 0.0808 - acc: 0.9690 - val_loss: 0.0837 - val_acc: 0.9682\n",
      "Epoch 176/250\n",
      "1s - loss: 0.0809 - acc: 0.9691 - val_loss: 0.0835 - val_acc: 0.9683\n",
      "Epoch 177/250\n",
      "1s - loss: 0.0806 - acc: 0.9690 - val_loss: 0.0840 - val_acc: 0.9682\n",
      "Epoch 178/250\n",
      "1s - loss: 0.0806 - acc: 0.9692 - val_loss: 0.0837 - val_acc: 0.9685\n",
      "Epoch 179/250\n",
      "1s - loss: 0.0806 - acc: 0.9691 - val_loss: 0.0835 - val_acc: 0.9680\n",
      "Epoch 180/250\n",
      "1s - loss: 0.0804 - acc: 0.9692 - val_loss: 0.0834 - val_acc: 0.9687\n",
      "Epoch 181/250\n",
      "1s - loss: 0.0803 - acc: 0.9692 - val_loss: 0.0834 - val_acc: 0.9684\n",
      "Epoch 182/250\n",
      "1s - loss: 0.0808 - acc: 0.9692 - val_loss: 0.0836 - val_acc: 0.9685\n",
      "Epoch 183/250\n",
      "1s - loss: 0.0811 - acc: 0.9692 - val_loss: 0.0838 - val_acc: 0.9683\n",
      "Epoch 184/250\n",
      "1s - loss: 0.0803 - acc: 0.9693 - val_loss: 0.0836 - val_acc: 0.9686\n",
      "Epoch 185/250\n",
      "1s - loss: 0.0806 - acc: 0.9693 - val_loss: 0.0839 - val_acc: 0.9682\n",
      "Epoch 186/250\n",
      "1s - loss: 0.0803 - acc: 0.9693 - val_loss: 0.0835 - val_acc: 0.9685\n",
      "Epoch 187/250\n",
      "1s - loss: 0.0804 - acc: 0.9693 - val_loss: 0.0836 - val_acc: 0.9682\n",
      "Epoch 188/250\n",
      "1s - loss: 0.0804 - acc: 0.9694 - val_loss: 0.0837 - val_acc: 0.9684\n",
      "Epoch 189/250\n",
      "1s - loss: 0.0803 - acc: 0.9692 - val_loss: 0.0834 - val_acc: 0.9685\n",
      "Epoch 190/250\n",
      "1s - loss: 0.0804 - acc: 0.9695 - val_loss: 0.0839 - val_acc: 0.9683\n",
      "Epoch 191/250\n",
      "1s - loss: 0.0806 - acc: 0.9691 - val_loss: 0.0834 - val_acc: 0.9682\n",
      "Epoch 192/250\n",
      "1s - loss: 0.0807 - acc: 0.9693 - val_loss: 0.0838 - val_acc: 0.9684\n",
      "Epoch 193/250\n",
      "1s - loss: 0.0807 - acc: 0.9693 - val_loss: 0.0836 - val_acc: 0.9685\n",
      "Epoch 194/250\n",
      "1s - loss: 0.0803 - acc: 0.9692 - val_loss: 0.0841 - val_acc: 0.9683\n",
      "Epoch 195/250\n",
      "1s - loss: 0.0804 - acc: 0.9693 - val_loss: 0.0837 - val_acc: 0.9682\n",
      "Epoch 196/250\n",
      "1s - loss: 0.0805 - acc: 0.9691 - val_loss: 0.0838 - val_acc: 0.9682\n",
      "Epoch 197/250\n",
      "1s - loss: 0.0802 - acc: 0.9693 - val_loss: 0.0841 - val_acc: 0.9683\n",
      "Epoch 198/250\n",
      "1s - loss: 0.0802 - acc: 0.9694 - val_loss: 0.0838 - val_acc: 0.9685\n",
      "Epoch 199/250\n",
      "1s - loss: 0.0801 - acc: 0.9693 - val_loss: 0.0840 - val_acc: 0.9681\n",
      "Epoch 200/250\n",
      "1s - loss: 0.0801 - acc: 0.9694 - val_loss: 0.0838 - val_acc: 0.9685\n",
      "Epoch 201/250\n",
      "1s - loss: 0.0801 - acc: 0.9693 - val_loss: 0.0839 - val_acc: 0.9685\n",
      "Epoch 202/250\n",
      "1s - loss: 0.0804 - acc: 0.9694 - val_loss: 0.0842 - val_acc: 0.9681\n",
      "Epoch 203/250\n",
      "1s - loss: 0.0802 - acc: 0.9694 - val_loss: 0.0840 - val_acc: 0.9680\n",
      "Epoch 204/250\n",
      "1s - loss: 0.0802 - acc: 0.9694 - val_loss: 0.0836 - val_acc: 0.9682\n",
      "Epoch 205/250\n",
      "1s - loss: 0.0804 - acc: 0.9693 - val_loss: 0.0839 - val_acc: 0.9683\n",
      "Epoch 206/250\n",
      "1s - loss: 0.0802 - acc: 0.9693 - val_loss: 0.0838 - val_acc: 0.9683\n",
      "Epoch 207/250\n",
      "1s - loss: 0.0804 - acc: 0.9694 - val_loss: 0.0836 - val_acc: 0.9683\n",
      "Epoch 208/250\n",
      "1s - loss: 0.0801 - acc: 0.9693 - val_loss: 0.0838 - val_acc: 0.9684\n",
      "Epoch 209/250\n",
      "1s - loss: 0.0802 - acc: 0.9693 - val_loss: 0.0840 - val_acc: 0.9681\n",
      "Epoch 210/250\n",
      "1s - loss: 0.0801 - acc: 0.9696 - val_loss: 0.0837 - val_acc: 0.9682\n",
      "Epoch 211/250\n",
      "1s - loss: 0.0804 - acc: 0.9696 - val_loss: 0.0843 - val_acc: 0.9680\n",
      "Epoch 212/250\n",
      "1s - loss: 0.0801 - acc: 0.9691 - val_loss: 0.0843 - val_acc: 0.9684\n",
      "Epoch 213/250\n",
      "1s - loss: 0.0804 - acc: 0.9693 - val_loss: 0.0839 - val_acc: 0.9686\n",
      "Epoch 214/250\n",
      "1s - loss: 0.0802 - acc: 0.9694 - val_loss: 0.0839 - val_acc: 0.9681\n",
      "Epoch 215/250\n",
      "1s - loss: 0.0798 - acc: 0.9695 - val_loss: 0.0841 - val_acc: 0.9684\n",
      "Epoch 216/250\n",
      "1s - loss: 0.0800 - acc: 0.9693 - val_loss: 0.0840 - val_acc: 0.9681\n",
      "Epoch 217/250\n",
      "1s - loss: 0.0798 - acc: 0.9695 - val_loss: 0.0845 - val_acc: 0.9682\n",
      "Epoch 218/250\n",
      "1s - loss: 0.0799 - acc: 0.9695 - val_loss: 0.0844 - val_acc: 0.9681\n",
      "Epoch 219/250\n",
      "1s - loss: 0.0799 - acc: 0.9694 - val_loss: 0.0843 - val_acc: 0.9681\n",
      "Epoch 220/250\n",
      "1s - loss: 0.0801 - acc: 0.9695 - val_loss: 0.0846 - val_acc: 0.9683\n",
      "Epoch 221/250\n",
      "1s - loss: 0.0798 - acc: 0.9695 - val_loss: 0.0839 - val_acc: 0.9679\n",
      "Epoch 222/250\n",
      "1s - loss: 0.0799 - acc: 0.9695 - val_loss: 0.0843 - val_acc: 0.9683\n",
      "Epoch 223/250\n",
      "1s - loss: 0.0797 - acc: 0.9694 - val_loss: 0.0839 - val_acc: 0.9685\n",
      "Epoch 224/250\n",
      "1s - loss: 0.0799 - acc: 0.9694 - val_loss: 0.0842 - val_acc: 0.9682\n",
      "Epoch 225/250\n",
      "1s - loss: 0.0799 - acc: 0.9695 - val_loss: 0.0839 - val_acc: 0.9683\n",
      "Epoch 226/250\n",
      "1s - loss: 0.0798 - acc: 0.9695 - val_loss: 0.0840 - val_acc: 0.9681\n",
      "Epoch 227/250\n",
      "1s - loss: 0.0802 - acc: 0.9693 - val_loss: 0.0840 - val_acc: 0.9683\n",
      "Epoch 228/250\n",
      "1s - loss: 0.0798 - acc: 0.9695 - val_loss: 0.0839 - val_acc: 0.9685\n",
      "Epoch 229/250\n",
      "1s - loss: 0.0797 - acc: 0.9695 - val_loss: 0.0838 - val_acc: 0.9684\n",
      "Epoch 230/250\n",
      "1s - loss: 0.0798 - acc: 0.9694 - val_loss: 0.0843 - val_acc: 0.9684\n",
      "Epoch 231/250\n",
      "1s - loss: 0.0796 - acc: 0.9696 - val_loss: 0.0843 - val_acc: 0.9681\n",
      "Epoch 232/250\n",
      "1s - loss: 0.0795 - acc: 0.9696 - val_loss: 0.0839 - val_acc: 0.9682\n",
      "Epoch 233/250\n",
      "1s - loss: 0.0797 - acc: 0.9695 - val_loss: 0.0848 - val_acc: 0.9682\n",
      "Epoch 234/250\n",
      "1s - loss: 0.0796 - acc: 0.9698 - val_loss: 0.0843 - val_acc: 0.9681\n",
      "Epoch 235/250\n",
      "1s - loss: 0.0798 - acc: 0.9694 - val_loss: 0.0843 - val_acc: 0.9683\n",
      "Epoch 236/250\n",
      "1s - loss: 0.0797 - acc: 0.9695 - val_loss: 0.0842 - val_acc: 0.9684\n",
      "Epoch 237/250\n",
      "1s - loss: 0.0800 - acc: 0.9694 - val_loss: 0.0843 - val_acc: 0.9682\n",
      "Epoch 238/250\n",
      "1s - loss: 0.0797 - acc: 0.9696 - val_loss: 0.0838 - val_acc: 0.9684\n",
      "Epoch 239/250\n",
      "1s - loss: 0.0794 - acc: 0.9694 - val_loss: 0.0841 - val_acc: 0.9681\n",
      "Epoch 240/250\n",
      "1s - loss: 0.0797 - acc: 0.9693 - val_loss: 0.0841 - val_acc: 0.9681\n",
      "Epoch 241/250\n",
      "1s - loss: 0.0792 - acc: 0.9697 - val_loss: 0.0842 - val_acc: 0.9682\n",
      "Epoch 242/250\n",
      "1s - loss: 0.0800 - acc: 0.9692 - val_loss: 0.0839 - val_acc: 0.9682\n",
      "Epoch 243/250\n",
      "1s - loss: 0.0793 - acc: 0.9694 - val_loss: 0.0839 - val_acc: 0.9681\n",
      "Epoch 244/250\n",
      "1s - loss: 0.0792 - acc: 0.9697 - val_loss: 0.0842 - val_acc: 0.9683\n",
      "Epoch 245/250\n",
      "1s - loss: 0.0795 - acc: 0.9696 - val_loss: 0.0845 - val_acc: 0.9681\n",
      "Epoch 246/250\n",
      "1s - loss: 0.0795 - acc: 0.9697 - val_loss: 0.0842 - val_acc: 0.9683\n",
      "Epoch 247/250\n",
      "1s - loss: 0.0793 - acc: 0.9696 - val_loss: 0.0844 - val_acc: 0.9678\n",
      "Epoch 248/250\n",
      "1s - loss: 0.0795 - acc: 0.9697 - val_loss: 0.0843 - val_acc: 0.9681\n",
      "Epoch 249/250\n",
      "1s - loss: 0.0795 - acc: 0.9696 - val_loss: 0.0847 - val_acc: 0.9679\n",
      "Epoch 250/250\n",
      "1s - loss: 0.0793 - acc: 0.9696 - val_loss: 0.0842 - val_acc: 0.9681\n",
      "0.927571987525\n",
      "Train on 32384 samples, validate on 8095 samples\n",
      "Epoch 1/250\n",
      "1s - loss: 0.1679 - acc: 0.9428 - val_loss: 0.0983 - val_acc: 0.9664\n",
      "Epoch 2/250\n",
      "1s - loss: 0.1060 - acc: 0.9646 - val_loss: 0.0912 - val_acc: 0.9679\n",
      "Epoch 3/250\n",
      "1s - loss: 0.1007 - acc: 0.9653 - val_loss: 0.0884 - val_acc: 0.9678\n",
      "Epoch 4/250\n",
      "1s - loss: 0.0974 - acc: 0.9660 - val_loss: 0.0868 - val_acc: 0.9683\n",
      "Epoch 5/250\n",
      "1s - loss: 0.0956 - acc: 0.9663 - val_loss: 0.0855 - val_acc: 0.9685\n",
      "Epoch 6/250\n",
      "1s - loss: 0.0944 - acc: 0.9662 - val_loss: 0.0850 - val_acc: 0.9685\n",
      "Epoch 7/250\n",
      "1s - loss: 0.0938 - acc: 0.9663 - val_loss: 0.0845 - val_acc: 0.9685\n",
      "Epoch 8/250\n",
      "1s - loss: 0.0924 - acc: 0.9666 - val_loss: 0.0844 - val_acc: 0.9686\n",
      "Epoch 9/250\n",
      "1s - loss: 0.0921 - acc: 0.9665 - val_loss: 0.0838 - val_acc: 0.9686\n",
      "Epoch 10/250\n",
      "1s - loss: 0.0915 - acc: 0.9667 - val_loss: 0.0837 - val_acc: 0.9689\n",
      "Epoch 11/250\n",
      "1s - loss: 0.0913 - acc: 0.9666 - val_loss: 0.0831 - val_acc: 0.9689\n",
      "Epoch 12/250\n",
      "1s - loss: 0.0909 - acc: 0.9667 - val_loss: 0.0830 - val_acc: 0.9689\n",
      "Epoch 13/250\n",
      "1s - loss: 0.0905 - acc: 0.9668 - val_loss: 0.0830 - val_acc: 0.9687\n",
      "Epoch 14/250\n",
      "1s - loss: 0.0897 - acc: 0.9667 - val_loss: 0.0828 - val_acc: 0.9687\n",
      "Epoch 15/250\n",
      "1s - loss: 0.0898 - acc: 0.9667 - val_loss: 0.0827 - val_acc: 0.9686\n",
      "Epoch 16/250\n",
      "1s - loss: 0.0892 - acc: 0.9669 - val_loss: 0.0826 - val_acc: 0.9689\n",
      "Epoch 17/250\n",
      "1s - loss: 0.0893 - acc: 0.9668 - val_loss: 0.0827 - val_acc: 0.9687\n",
      "Epoch 18/250\n",
      "1s - loss: 0.0892 - acc: 0.9668 - val_loss: 0.0822 - val_acc: 0.9689\n",
      "Epoch 19/250\n",
      "1s - loss: 0.0889 - acc: 0.9669 - val_loss: 0.0827 - val_acc: 0.9687\n",
      "Epoch 20/250\n",
      "1s - loss: 0.0882 - acc: 0.9674 - val_loss: 0.0820 - val_acc: 0.9691\n",
      "Epoch 21/250\n",
      "1s - loss: 0.0884 - acc: 0.9670 - val_loss: 0.0823 - val_acc: 0.9690\n",
      "Epoch 22/250\n",
      "1s - loss: 0.0881 - acc: 0.9670 - val_loss: 0.0826 - val_acc: 0.9687\n",
      "Epoch 23/250\n",
      "1s - loss: 0.0884 - acc: 0.9670 - val_loss: 0.0821 - val_acc: 0.9690\n",
      "Epoch 24/250\n",
      "1s - loss: 0.0879 - acc: 0.9671 - val_loss: 0.0826 - val_acc: 0.9687\n",
      "Epoch 25/250\n",
      "1s - loss: 0.0879 - acc: 0.9672 - val_loss: 0.0826 - val_acc: 0.9684\n",
      "Epoch 26/250\n",
      "1s - loss: 0.0879 - acc: 0.9672 - val_loss: 0.0821 - val_acc: 0.9689\n",
      "Epoch 27/250\n",
      "1s - loss: 0.0873 - acc: 0.9673 - val_loss: 0.0823 - val_acc: 0.9689\n",
      "Epoch 28/250\n",
      "1s - loss: 0.0876 - acc: 0.9674 - val_loss: 0.0823 - val_acc: 0.9690\n",
      "Epoch 29/250\n",
      "1s - loss: 0.0873 - acc: 0.9673 - val_loss: 0.0820 - val_acc: 0.9690\n",
      "Epoch 30/250\n",
      "1s - loss: 0.0872 - acc: 0.9673 - val_loss: 0.0823 - val_acc: 0.9688\n",
      "Epoch 31/250\n",
      "1s - loss: 0.0874 - acc: 0.9673 - val_loss: 0.0822 - val_acc: 0.9688\n",
      "Epoch 32/250\n",
      "1s - loss: 0.0866 - acc: 0.9676 - val_loss: 0.0822 - val_acc: 0.9688\n",
      "Epoch 33/250\n",
      "1s - loss: 0.0869 - acc: 0.9674 - val_loss: 0.0822 - val_acc: 0.9689\n",
      "Epoch 34/250\n",
      "1s - loss: 0.0867 - acc: 0.9676 - val_loss: 0.0827 - val_acc: 0.9685\n",
      "Epoch 35/250\n",
      "1s - loss: 0.0867 - acc: 0.9674 - val_loss: 0.0818 - val_acc: 0.9688\n",
      "Epoch 36/250\n",
      "1s - loss: 0.0866 - acc: 0.9676 - val_loss: 0.0821 - val_acc: 0.9689\n",
      "Epoch 37/250\n",
      "1s - loss: 0.0862 - acc: 0.9675 - val_loss: 0.0823 - val_acc: 0.9689\n",
      "Epoch 38/250\n",
      "1s - loss: 0.0862 - acc: 0.9677 - val_loss: 0.0826 - val_acc: 0.9685\n",
      "Epoch 39/250\n",
      "1s - loss: 0.0860 - acc: 0.9678 - val_loss: 0.0824 - val_acc: 0.9686\n",
      "Epoch 40/250\n",
      "1s - loss: 0.0863 - acc: 0.9676 - val_loss: 0.0822 - val_acc: 0.9686\n",
      "Epoch 41/250\n",
      "1s - loss: 0.0862 - acc: 0.9677 - val_loss: 0.0821 - val_acc: 0.9688\n",
      "Epoch 42/250\n",
      "1s - loss: 0.0857 - acc: 0.9676 - val_loss: 0.0824 - val_acc: 0.9686\n",
      "Epoch 43/250\n",
      "1s - loss: 0.0861 - acc: 0.9675 - val_loss: 0.0823 - val_acc: 0.9689\n",
      "Epoch 44/250\n",
      "1s - loss: 0.0857 - acc: 0.9680 - val_loss: 0.0826 - val_acc: 0.9689\n",
      "Epoch 45/250\n",
      "1s - loss: 0.0858 - acc: 0.9678 - val_loss: 0.0821 - val_acc: 0.9687\n",
      "Epoch 46/250\n",
      "1s - loss: 0.0855 - acc: 0.9678 - val_loss: 0.0819 - val_acc: 0.9687\n",
      "Epoch 47/250\n",
      "1s - loss: 0.0857 - acc: 0.9679 - val_loss: 0.0824 - val_acc: 0.9682\n",
      "Epoch 48/250\n",
      "1s - loss: 0.0855 - acc: 0.9680 - val_loss: 0.0829 - val_acc: 0.9685\n",
      "Epoch 49/250\n",
      "1s - loss: 0.0855 - acc: 0.9677 - val_loss: 0.0819 - val_acc: 0.9689\n",
      "Epoch 50/250\n",
      "1s - loss: 0.0853 - acc: 0.9680 - val_loss: 0.0821 - val_acc: 0.9691\n",
      "Epoch 51/250\n",
      "1s - loss: 0.0855 - acc: 0.9678 - val_loss: 0.0822 - val_acc: 0.9688\n",
      "Epoch 52/250\n",
      "1s - loss: 0.0851 - acc: 0.9679 - val_loss: 0.0821 - val_acc: 0.9690\n",
      "Epoch 53/250\n",
      "1s - loss: 0.0853 - acc: 0.9678 - val_loss: 0.0817 - val_acc: 0.9689\n",
      "Epoch 54/250\n",
      "1s - loss: 0.0849 - acc: 0.9680 - val_loss: 0.0826 - val_acc: 0.9687\n",
      "Epoch 55/250\n",
      "1s - loss: 0.0850 - acc: 0.9679 - val_loss: 0.0822 - val_acc: 0.9687\n",
      "Epoch 56/250\n",
      "1s - loss: 0.0851 - acc: 0.9682 - val_loss: 0.0824 - val_acc: 0.9687\n",
      "Epoch 57/250\n",
      "1s - loss: 0.0848 - acc: 0.9680 - val_loss: 0.0824 - val_acc: 0.9685\n",
      "Epoch 58/250\n",
      "1s - loss: 0.0850 - acc: 0.9679 - val_loss: 0.0822 - val_acc: 0.9688\n",
      "Epoch 59/250\n",
      "1s - loss: 0.0847 - acc: 0.9680 - val_loss: 0.0823 - val_acc: 0.9690\n",
      "Epoch 60/250\n",
      "1s - loss: 0.0846 - acc: 0.9680 - val_loss: 0.0824 - val_acc: 0.9688\n",
      "Epoch 61/250\n",
      "1s - loss: 0.0846 - acc: 0.9681 - val_loss: 0.0827 - val_acc: 0.9685\n",
      "Epoch 62/250\n",
      "1s - loss: 0.0848 - acc: 0.9682 - val_loss: 0.0823 - val_acc: 0.9687\n",
      "Epoch 63/250\n",
      "1s - loss: 0.0848 - acc: 0.9679 - val_loss: 0.0825 - val_acc: 0.9687\n",
      "Epoch 64/250\n",
      "1s - loss: 0.0846 - acc: 0.9682 - val_loss: 0.0826 - val_acc: 0.9687\n",
      "Epoch 65/250\n",
      "1s - loss: 0.0844 - acc: 0.9680 - val_loss: 0.0825 - val_acc: 0.9689\n",
      "Epoch 66/250\n",
      "1s - loss: 0.0844 - acc: 0.9679 - val_loss: 0.0825 - val_acc: 0.9686\n",
      "Epoch 67/250\n",
      "1s - loss: 0.0845 - acc: 0.9680 - val_loss: 0.0823 - val_acc: 0.9690\n",
      "Epoch 68/250\n",
      "1s - loss: 0.0844 - acc: 0.9682 - val_loss: 0.0823 - val_acc: 0.9689\n",
      "Epoch 69/250\n",
      "1s - loss: 0.0842 - acc: 0.9680 - val_loss: 0.0825 - val_acc: 0.9687\n",
      "Epoch 70/250\n",
      "1s - loss: 0.0842 - acc: 0.9683 - val_loss: 0.0822 - val_acc: 0.9687\n",
      "Epoch 71/250\n",
      "1s - loss: 0.0844 - acc: 0.9683 - val_loss: 0.0824 - val_acc: 0.9687\n",
      "Epoch 72/250\n",
      "1s - loss: 0.0843 - acc: 0.9682 - val_loss: 0.0823 - val_acc: 0.9687\n",
      "Epoch 73/250\n",
      "1s - loss: 0.0840 - acc: 0.9683 - val_loss: 0.0822 - val_acc: 0.9688\n",
      "Epoch 74/250\n",
      "1s - loss: 0.0841 - acc: 0.9683 - val_loss: 0.0822 - val_acc: 0.9689\n",
      "Epoch 75/250\n",
      "1s - loss: 0.0841 - acc: 0.9682 - val_loss: 0.0823 - val_acc: 0.9689\n",
      "Epoch 76/250\n",
      "1s - loss: 0.0838 - acc: 0.9684 - val_loss: 0.0827 - val_acc: 0.9685\n",
      "Epoch 77/250\n",
      "1s - loss: 0.0836 - acc: 0.9684 - val_loss: 0.0825 - val_acc: 0.9687\n",
      "Epoch 78/250\n",
      "1s - loss: 0.0839 - acc: 0.9682 - val_loss: 0.0826 - val_acc: 0.9688\n",
      "Epoch 79/250\n",
      "1s - loss: 0.0838 - acc: 0.9683 - val_loss: 0.0823 - val_acc: 0.9688\n",
      "Epoch 80/250\n",
      "1s - loss: 0.0836 - acc: 0.9684 - val_loss: 0.0827 - val_acc: 0.9689\n",
      "Epoch 81/250\n",
      "1s - loss: 0.0837 - acc: 0.9682 - val_loss: 0.0827 - val_acc: 0.9688\n",
      "Epoch 82/250\n",
      "1s - loss: 0.0834 - acc: 0.9683 - val_loss: 0.0824 - val_acc: 0.9689\n",
      "Epoch 83/250\n",
      "1s - loss: 0.0837 - acc: 0.9684 - val_loss: 0.0826 - val_acc: 0.9689\n",
      "Epoch 84/250\n",
      "1s - loss: 0.0837 - acc: 0.9684 - val_loss: 0.0827 - val_acc: 0.9687\n",
      "Epoch 85/250\n",
      "1s - loss: 0.0835 - acc: 0.9684 - val_loss: 0.0826 - val_acc: 0.9688\n",
      "Epoch 86/250\n",
      "1s - loss: 0.0833 - acc: 0.9684 - val_loss: 0.0825 - val_acc: 0.9685\n",
      "Epoch 87/250\n",
      "1s - loss: 0.0837 - acc: 0.9682 - val_loss: 0.0823 - val_acc: 0.9688\n",
      "Epoch 88/250\n",
      "1s - loss: 0.0834 - acc: 0.9687 - val_loss: 0.0830 - val_acc: 0.9686\n",
      "Epoch 89/250\n",
      "1s - loss: 0.0834 - acc: 0.9684 - val_loss: 0.0827 - val_acc: 0.9689\n",
      "Epoch 90/250\n",
      "1s - loss: 0.0835 - acc: 0.9682 - val_loss: 0.0826 - val_acc: 0.9688\n",
      "Epoch 91/250\n",
      "1s - loss: 0.0831 - acc: 0.9685 - val_loss: 0.0828 - val_acc: 0.9684\n",
      "Epoch 92/250\n",
      "1s - loss: 0.0831 - acc: 0.9684 - val_loss: 0.0832 - val_acc: 0.9686\n",
      "Epoch 93/250\n",
      "1s - loss: 0.0832 - acc: 0.9683 - val_loss: 0.0825 - val_acc: 0.9687\n",
      "Epoch 94/250\n",
      "1s - loss: 0.0837 - acc: 0.9682 - val_loss: 0.0827 - val_acc: 0.9686\n",
      "Epoch 95/250\n",
      "1s - loss: 0.0830 - acc: 0.9684 - val_loss: 0.0826 - val_acc: 0.9687\n",
      "Epoch 96/250\n",
      "1s - loss: 0.0831 - acc: 0.9686 - val_loss: 0.0826 - val_acc: 0.9688\n",
      "Epoch 97/250\n",
      "1s - loss: 0.0832 - acc: 0.9682 - val_loss: 0.0828 - val_acc: 0.9688\n",
      "Epoch 98/250\n",
      "1s - loss: 0.0829 - acc: 0.9685 - val_loss: 0.0831 - val_acc: 0.9683\n",
      "Epoch 99/250\n",
      "1s - loss: 0.0829 - acc: 0.9684 - val_loss: 0.0830 - val_acc: 0.9687\n",
      "Epoch 100/250\n",
      "1s - loss: 0.0828 - acc: 0.9687 - val_loss: 0.0828 - val_acc: 0.9685\n",
      "Epoch 101/250\n",
      "1s - loss: 0.0830 - acc: 0.9686 - val_loss: 0.0828 - val_acc: 0.9687\n",
      "Epoch 102/250\n",
      "1s - loss: 0.0829 - acc: 0.9686 - val_loss: 0.0827 - val_acc: 0.9688\n",
      "Epoch 103/250\n",
      "1s - loss: 0.0828 - acc: 0.9687 - val_loss: 0.0828 - val_acc: 0.9687\n",
      "Epoch 104/250\n",
      "1s - loss: 0.0831 - acc: 0.9682 - val_loss: 0.0829 - val_acc: 0.9685\n",
      "Epoch 105/250\n",
      "1s - loss: 0.0829 - acc: 0.9686 - val_loss: 0.0826 - val_acc: 0.9687\n",
      "Epoch 106/250\n",
      "1s - loss: 0.0826 - acc: 0.9686 - val_loss: 0.0825 - val_acc: 0.9689\n",
      "Epoch 107/250\n",
      "1s - loss: 0.0827 - acc: 0.9685 - val_loss: 0.0831 - val_acc: 0.9687\n",
      "Epoch 108/250\n",
      "1s - loss: 0.0828 - acc: 0.9685 - val_loss: 0.0826 - val_acc: 0.9687\n",
      "Epoch 109/250\n",
      "1s - loss: 0.0827 - acc: 0.9684 - val_loss: 0.0829 - val_acc: 0.9686\n",
      "Epoch 110/250\n",
      "1s - loss: 0.0827 - acc: 0.9684 - val_loss: 0.0833 - val_acc: 0.9686\n",
      "Epoch 111/250\n",
      "1s - loss: 0.0828 - acc: 0.9687 - val_loss: 0.0827 - val_acc: 0.9686\n",
      "Epoch 112/250\n",
      "1s - loss: 0.0824 - acc: 0.9686 - val_loss: 0.0824 - val_acc: 0.9688\n",
      "Epoch 113/250\n",
      "1s - loss: 0.0825 - acc: 0.9687 - val_loss: 0.0828 - val_acc: 0.9687\n",
      "Epoch 114/250\n",
      "1s - loss: 0.0826 - acc: 0.9686 - val_loss: 0.0829 - val_acc: 0.9688\n",
      "Epoch 115/250\n",
      "1s - loss: 0.0824 - acc: 0.9686 - val_loss: 0.0827 - val_acc: 0.9686\n",
      "Epoch 116/250\n",
      "1s - loss: 0.0824 - acc: 0.9688 - val_loss: 0.0827 - val_acc: 0.9687\n",
      "Epoch 117/250\n",
      "1s - loss: 0.0825 - acc: 0.9688 - val_loss: 0.0824 - val_acc: 0.9689\n",
      "Epoch 118/250\n",
      "1s - loss: 0.0824 - acc: 0.9687 - val_loss: 0.0827 - val_acc: 0.9687\n",
      "Epoch 119/250\n",
      "1s - loss: 0.0826 - acc: 0.9686 - val_loss: 0.0830 - val_acc: 0.9687\n",
      "Epoch 120/250\n",
      "1s - loss: 0.0824 - acc: 0.9688 - val_loss: 0.0832 - val_acc: 0.9686\n",
      "Epoch 121/250\n",
      "1s - loss: 0.0824 - acc: 0.9688 - val_loss: 0.0829 - val_acc: 0.9687\n",
      "Epoch 122/250\n",
      "1s - loss: 0.0823 - acc: 0.9687 - val_loss: 0.0826 - val_acc: 0.9687\n",
      "Epoch 123/250\n",
      "1s - loss: 0.0820 - acc: 0.9689 - val_loss: 0.0834 - val_acc: 0.9688\n",
      "Epoch 124/250\n",
      "1s - loss: 0.0822 - acc: 0.9687 - val_loss: 0.0828 - val_acc: 0.9690\n",
      "Epoch 125/250\n",
      "1s - loss: 0.0824 - acc: 0.9686 - val_loss: 0.0826 - val_acc: 0.9690\n",
      "Epoch 126/250\n",
      "1s - loss: 0.0824 - acc: 0.9685 - val_loss: 0.0830 - val_acc: 0.9688\n",
      "Epoch 127/250\n",
      "1s - loss: 0.0821 - acc: 0.9689 - val_loss: 0.0828 - val_acc: 0.9687\n",
      "Epoch 128/250\n",
      "1s - loss: 0.0821 - acc: 0.9687 - val_loss: 0.0829 - val_acc: 0.9686\n",
      "Epoch 129/250\n",
      "1s - loss: 0.0824 - acc: 0.9685 - val_loss: 0.0828 - val_acc: 0.9688\n",
      "Epoch 130/250\n",
      "1s - loss: 0.0819 - acc: 0.9687 - val_loss: 0.0831 - val_acc: 0.9687\n",
      "Epoch 131/250\n",
      "1s - loss: 0.0820 - acc: 0.9687 - val_loss: 0.0831 - val_acc: 0.9687\n",
      "Epoch 132/250\n",
      "1s - loss: 0.0821 - acc: 0.9687 - val_loss: 0.0830 - val_acc: 0.9688\n",
      "Epoch 133/250\n",
      "1s - loss: 0.0819 - acc: 0.9688 - val_loss: 0.0829 - val_acc: 0.9688\n",
      "Epoch 134/250\n",
      "1s - loss: 0.0820 - acc: 0.9686 - val_loss: 0.0831 - val_acc: 0.9686\n",
      "Epoch 135/250\n",
      "1s - loss: 0.0822 - acc: 0.9687 - val_loss: 0.0831 - val_acc: 0.9686\n",
      "Epoch 136/250\n",
      "1s - loss: 0.0818 - acc: 0.9690 - val_loss: 0.0833 - val_acc: 0.9687\n",
      "Epoch 137/250\n",
      "1s - loss: 0.0819 - acc: 0.9687 - val_loss: 0.0823 - val_acc: 0.9688\n",
      "Epoch 138/250\n",
      "1s - loss: 0.0819 - acc: 0.9687 - val_loss: 0.0833 - val_acc: 0.9686\n",
      "Epoch 139/250\n",
      "1s - loss: 0.0818 - acc: 0.9687 - val_loss: 0.0831 - val_acc: 0.9690\n",
      "Epoch 140/250\n",
      "1s - loss: 0.0817 - acc: 0.9690 - val_loss: 0.0828 - val_acc: 0.9685\n",
      "Epoch 141/250\n",
      "1s - loss: 0.0820 - acc: 0.9687 - val_loss: 0.0831 - val_acc: 0.9686\n",
      "Epoch 142/250\n",
      "1s - loss: 0.0821 - acc: 0.9689 - val_loss: 0.0831 - val_acc: 0.9688\n",
      "Epoch 143/250\n",
      "1s - loss: 0.0819 - acc: 0.9688 - val_loss: 0.0829 - val_acc: 0.9688\n",
      "Epoch 144/250\n",
      "1s - loss: 0.0816 - acc: 0.9688 - val_loss: 0.0833 - val_acc: 0.9688\n",
      "Epoch 145/250\n",
      "1s - loss: 0.0817 - acc: 0.9689 - val_loss: 0.0834 - val_acc: 0.9686\n",
      "Epoch 146/250\n",
      "1s - loss: 0.0820 - acc: 0.9687 - val_loss: 0.0830 - val_acc: 0.9686\n",
      "Epoch 147/250\n",
      "1s - loss: 0.0818 - acc: 0.9690 - val_loss: 0.0833 - val_acc: 0.9689\n",
      "Epoch 148/250\n",
      "1s - loss: 0.0821 - acc: 0.9687 - val_loss: 0.0831 - val_acc: 0.9689\n",
      "Epoch 149/250\n",
      "1s - loss: 0.0814 - acc: 0.9690 - val_loss: 0.0831 - val_acc: 0.9690\n",
      "Epoch 150/250\n",
      "1s - loss: 0.0815 - acc: 0.9690 - val_loss: 0.0832 - val_acc: 0.9686\n",
      "Epoch 151/250\n",
      "1s - loss: 0.0818 - acc: 0.9686 - val_loss: 0.0829 - val_acc: 0.9686\n",
      "Epoch 152/250\n",
      "1s - loss: 0.0817 - acc: 0.9687 - val_loss: 0.0835 - val_acc: 0.9689\n",
      "Epoch 153/250\n",
      "1s - loss: 0.0815 - acc: 0.9690 - val_loss: 0.0833 - val_acc: 0.9686\n",
      "Epoch 154/250\n",
      "1s - loss: 0.0815 - acc: 0.9689 - val_loss: 0.0828 - val_acc: 0.9688\n",
      "Epoch 155/250\n",
      "1s - loss: 0.0813 - acc: 0.9691 - val_loss: 0.0834 - val_acc: 0.9687\n",
      "Epoch 156/250\n",
      "1s - loss: 0.0818 - acc: 0.9688 - val_loss: 0.0829 - val_acc: 0.9687\n",
      "Epoch 157/250\n",
      "1s - loss: 0.0817 - acc: 0.9689 - val_loss: 0.0837 - val_acc: 0.9685\n",
      "Epoch 158/250\n",
      "1s - loss: 0.0816 - acc: 0.9687 - val_loss: 0.0833 - val_acc: 0.9685\n",
      "Epoch 159/250\n",
      "1s - loss: 0.0815 - acc: 0.9689 - val_loss: 0.0835 - val_acc: 0.9689\n",
      "Epoch 160/250\n",
      "1s - loss: 0.0814 - acc: 0.9689 - val_loss: 0.0835 - val_acc: 0.9687\n",
      "Epoch 161/250\n",
      "1s - loss: 0.0816 - acc: 0.9692 - val_loss: 0.0834 - val_acc: 0.9686\n",
      "Epoch 162/250\n",
      "1s - loss: 0.0813 - acc: 0.9689 - val_loss: 0.0836 - val_acc: 0.9686\n",
      "Epoch 163/250\n",
      "1s - loss: 0.0813 - acc: 0.9689 - val_loss: 0.0834 - val_acc: 0.9687\n",
      "Epoch 164/250\n",
      "1s - loss: 0.0813 - acc: 0.9689 - val_loss: 0.0833 - val_acc: 0.9687\n",
      "Epoch 165/250\n",
      "1s - loss: 0.0816 - acc: 0.9690 - val_loss: 0.0835 - val_acc: 0.9687\n",
      "Epoch 166/250\n",
      "1s - loss: 0.0812 - acc: 0.9691 - val_loss: 0.0833 - val_acc: 0.9688\n",
      "Epoch 167/250\n",
      "1s - loss: 0.0815 - acc: 0.9690 - val_loss: 0.0833 - val_acc: 0.9689\n",
      "Epoch 168/250\n",
      "1s - loss: 0.0812 - acc: 0.9689 - val_loss: 0.0833 - val_acc: 0.9686\n",
      "Epoch 169/250\n",
      "1s - loss: 0.0812 - acc: 0.9691 - val_loss: 0.0834 - val_acc: 0.9687\n",
      "Epoch 170/250\n",
      "1s - loss: 0.0812 - acc: 0.9688 - val_loss: 0.0831 - val_acc: 0.9686\n",
      "Epoch 171/250\n",
      "1s - loss: 0.0811 - acc: 0.9688 - val_loss: 0.0835 - val_acc: 0.9687\n",
      "Epoch 172/250\n",
      "1s - loss: 0.0812 - acc: 0.9689 - val_loss: 0.0838 - val_acc: 0.9686\n",
      "Epoch 173/250\n",
      "1s - loss: 0.0812 - acc: 0.9692 - val_loss: 0.0838 - val_acc: 0.9689\n",
      "Epoch 174/250\n",
      "1s - loss: 0.0812 - acc: 0.9689 - val_loss: 0.0836 - val_acc: 0.9687\n",
      "Epoch 175/250\n",
      "1s - loss: 0.0811 - acc: 0.9691 - val_loss: 0.0834 - val_acc: 0.9687\n",
      "Epoch 176/250\n",
      "1s - loss: 0.0812 - acc: 0.9690 - val_loss: 0.0836 - val_acc: 0.9687\n",
      "Epoch 177/250\n",
      "1s - loss: 0.0812 - acc: 0.9689 - val_loss: 0.0829 - val_acc: 0.9687\n",
      "Epoch 178/250\n",
      "1s - loss: 0.0812 - acc: 0.9689 - val_loss: 0.0836 - val_acc: 0.9688\n",
      "Epoch 179/250\n",
      "1s - loss: 0.0808 - acc: 0.9691 - val_loss: 0.0835 - val_acc: 0.9687\n",
      "Epoch 180/250\n",
      "1s - loss: 0.0811 - acc: 0.9691 - val_loss: 0.0833 - val_acc: 0.9688\n",
      "Epoch 181/250\n",
      "1s - loss: 0.0809 - acc: 0.9691 - val_loss: 0.0832 - val_acc: 0.9687\n",
      "Epoch 182/250\n",
      "1s - loss: 0.0810 - acc: 0.9692 - val_loss: 0.0834 - val_acc: 0.9689\n",
      "Epoch 183/250\n",
      "1s - loss: 0.0809 - acc: 0.9689 - val_loss: 0.0833 - val_acc: 0.9690\n",
      "Epoch 184/250\n",
      "1s - loss: 0.0814 - acc: 0.9689 - val_loss: 0.0835 - val_acc: 0.9686\n",
      "Epoch 185/250\n",
      "1s - loss: 0.0809 - acc: 0.9692 - val_loss: 0.0834 - val_acc: 0.9687\n",
      "Epoch 186/250\n",
      "1s - loss: 0.0809 - acc: 0.9692 - val_loss: 0.0834 - val_acc: 0.9688\n",
      "Epoch 187/250\n",
      "1s - loss: 0.0806 - acc: 0.9690 - val_loss: 0.0835 - val_acc: 0.9686\n",
      "Epoch 188/250\n",
      "1s - loss: 0.0812 - acc: 0.9691 - val_loss: 0.0835 - val_acc: 0.9688\n",
      "Epoch 189/250\n",
      "1s - loss: 0.0811 - acc: 0.9690 - val_loss: 0.0834 - val_acc: 0.9685\n",
      "Epoch 190/250\n",
      "1s - loss: 0.0810 - acc: 0.9691 - val_loss: 0.0836 - val_acc: 0.9685\n",
      "Epoch 191/250\n",
      "1s - loss: 0.0807 - acc: 0.9691 - val_loss: 0.0834 - val_acc: 0.9687\n",
      "Epoch 192/250\n",
      "1s - loss: 0.0807 - acc: 0.9691 - val_loss: 0.0831 - val_acc: 0.9689\n",
      "Epoch 193/250\n",
      "1s - loss: 0.0810 - acc: 0.9692 - val_loss: 0.0840 - val_acc: 0.9688\n",
      "Epoch 194/250\n",
      "1s - loss: 0.0808 - acc: 0.9693 - val_loss: 0.0836 - val_acc: 0.9685\n",
      "Epoch 195/250\n",
      "1s - loss: 0.0806 - acc: 0.9691 - val_loss: 0.0839 - val_acc: 0.9685\n",
      "Epoch 196/250\n",
      "1s - loss: 0.0808 - acc: 0.9691 - val_loss: 0.0835 - val_acc: 0.9687\n",
      "Epoch 197/250\n",
      "1s - loss: 0.0807 - acc: 0.9690 - val_loss: 0.0838 - val_acc: 0.9686\n",
      "Epoch 198/250\n",
      "1s - loss: 0.0807 - acc: 0.9693 - val_loss: 0.0839 - val_acc: 0.9685\n",
      "Epoch 199/250\n",
      "1s - loss: 0.0808 - acc: 0.9690 - val_loss: 0.0841 - val_acc: 0.9685\n",
      "Epoch 200/250\n",
      "1s - loss: 0.0804 - acc: 0.9691 - val_loss: 0.0836 - val_acc: 0.9687\n",
      "Epoch 201/250\n",
      "1s - loss: 0.0804 - acc: 0.9692 - val_loss: 0.0840 - val_acc: 0.9686\n",
      "Epoch 202/250\n",
      "1s - loss: 0.0806 - acc: 0.9693 - val_loss: 0.0838 - val_acc: 0.9686\n",
      "Epoch 203/250\n",
      "1s - loss: 0.0806 - acc: 0.9690 - val_loss: 0.0834 - val_acc: 0.9686\n",
      "Epoch 204/250\n",
      "1s - loss: 0.0806 - acc: 0.9692 - val_loss: 0.0835 - val_acc: 0.9686\n",
      "Epoch 205/250\n",
      "1s - loss: 0.0806 - acc: 0.9693 - val_loss: 0.0839 - val_acc: 0.9684\n",
      "Epoch 206/250\n",
      "1s - loss: 0.0808 - acc: 0.9690 - val_loss: 0.0838 - val_acc: 0.9686\n",
      "Epoch 207/250\n",
      "1s - loss: 0.0803 - acc: 0.9692 - val_loss: 0.0836 - val_acc: 0.9688\n",
      "Epoch 208/250\n",
      "1s - loss: 0.0806 - acc: 0.9693 - val_loss: 0.0838 - val_acc: 0.9686\n",
      "Epoch 209/250\n",
      "1s - loss: 0.0809 - acc: 0.9689 - val_loss: 0.0838 - val_acc: 0.9688\n",
      "Epoch 210/250\n",
      "1s - loss: 0.0806 - acc: 0.9692 - val_loss: 0.0843 - val_acc: 0.9687\n",
      "Epoch 211/250\n",
      "1s - loss: 0.0807 - acc: 0.9690 - val_loss: 0.0841 - val_acc: 0.9686\n",
      "Epoch 212/250\n",
      "1s - loss: 0.0802 - acc: 0.9693 - val_loss: 0.0840 - val_acc: 0.9687\n",
      "Epoch 213/250\n",
      "1s - loss: 0.0806 - acc: 0.9693 - val_loss: 0.0837 - val_acc: 0.9686\n",
      "Epoch 214/250\n",
      "1s - loss: 0.0806 - acc: 0.9692 - val_loss: 0.0840 - val_acc: 0.9687\n",
      "Epoch 215/250\n",
      "1s - loss: 0.0804 - acc: 0.9695 - val_loss: 0.0838 - val_acc: 0.9687\n",
      "Epoch 216/250\n",
      "1s - loss: 0.0800 - acc: 0.9694 - val_loss: 0.0838 - val_acc: 0.9688\n",
      "Epoch 217/250\n",
      "1s - loss: 0.0803 - acc: 0.9692 - val_loss: 0.0834 - val_acc: 0.9687\n",
      "Epoch 218/250\n",
      "1s - loss: 0.0805 - acc: 0.9692 - val_loss: 0.0836 - val_acc: 0.9689\n",
      "Epoch 219/250\n",
      "1s - loss: 0.0802 - acc: 0.9692 - val_loss: 0.0836 - val_acc: 0.9688\n",
      "Epoch 220/250\n",
      "1s - loss: 0.0802 - acc: 0.9693 - val_loss: 0.0841 - val_acc: 0.9685\n",
      "Epoch 221/250\n",
      "1s - loss: 0.0803 - acc: 0.9694 - val_loss: 0.0839 - val_acc: 0.9688\n",
      "Epoch 222/250\n",
      "1s - loss: 0.0802 - acc: 0.9691 - val_loss: 0.0840 - val_acc: 0.9684\n",
      "Epoch 223/250\n",
      "1s - loss: 0.0801 - acc: 0.9694 - val_loss: 0.0840 - val_acc: 0.9686\n",
      "Epoch 224/250\n",
      "1s - loss: 0.0799 - acc: 0.9692 - val_loss: 0.0841 - val_acc: 0.9685\n",
      "Epoch 225/250\n",
      "1s - loss: 0.0803 - acc: 0.9694 - val_loss: 0.0841 - val_acc: 0.9687\n",
      "Epoch 226/250\n",
      "1s - loss: 0.0805 - acc: 0.9692 - val_loss: 0.0844 - val_acc: 0.9686\n",
      "Epoch 227/250\n",
      "1s - loss: 0.0801 - acc: 0.9695 - val_loss: 0.0841 - val_acc: 0.9685\n",
      "Epoch 228/250\n",
      "1s - loss: 0.0804 - acc: 0.9692 - val_loss: 0.0842 - val_acc: 0.9687\n",
      "Epoch 229/250\n",
      "1s - loss: 0.0801 - acc: 0.9696 - val_loss: 0.0839 - val_acc: 0.9686\n",
      "Epoch 230/250\n",
      "1s - loss: 0.0800 - acc: 0.9693 - val_loss: 0.0836 - val_acc: 0.9686\n",
      "Epoch 231/250\n",
      "1s - loss: 0.0801 - acc: 0.9692 - val_loss: 0.0841 - val_acc: 0.9688\n",
      "Epoch 232/250\n",
      "1s - loss: 0.0800 - acc: 0.9693 - val_loss: 0.0844 - val_acc: 0.9687\n",
      "Epoch 233/250\n",
      "1s - loss: 0.0801 - acc: 0.9694 - val_loss: 0.0839 - val_acc: 0.9686\n",
      "Epoch 234/250\n",
      "1s - loss: 0.0804 - acc: 0.9690 - val_loss: 0.0841 - val_acc: 0.9684\n",
      "Epoch 235/250\n",
      "1s - loss: 0.0802 - acc: 0.9692 - val_loss: 0.0839 - val_acc: 0.9686\n",
      "Epoch 236/250\n",
      "1s - loss: 0.0800 - acc: 0.9693 - val_loss: 0.0838 - val_acc: 0.9687\n",
      "Epoch 237/250\n",
      "1s - loss: 0.0801 - acc: 0.9692 - val_loss: 0.0843 - val_acc: 0.9687\n",
      "Epoch 238/250\n",
      "1s - loss: 0.0800 - acc: 0.9693 - val_loss: 0.0839 - val_acc: 0.9684\n",
      "Epoch 239/250\n",
      "1s - loss: 0.0801 - acc: 0.9694 - val_loss: 0.0841 - val_acc: 0.9687\n",
      "Epoch 240/250\n",
      "1s - loss: 0.0803 - acc: 0.9693 - val_loss: 0.0841 - val_acc: 0.9687\n",
      "Epoch 241/250\n",
      "1s - loss: 0.0801 - acc: 0.9694 - val_loss: 0.0839 - val_acc: 0.9685\n",
      "Epoch 242/250\n",
      "1s - loss: 0.0800 - acc: 0.9695 - val_loss: 0.0842 - val_acc: 0.9685\n",
      "Epoch 243/250\n",
      "1s - loss: 0.0800 - acc: 0.9695 - val_loss: 0.0844 - val_acc: 0.9686\n",
      "Epoch 244/250\n",
      "1s - loss: 0.0800 - acc: 0.9693 - val_loss: 0.0840 - val_acc: 0.9687\n",
      "Epoch 245/250\n",
      "1s - loss: 0.0800 - acc: 0.9694 - val_loss: 0.0839 - val_acc: 0.9686\n",
      "Epoch 246/250\n",
      "1s - loss: 0.0799 - acc: 0.9694 - val_loss: 0.0844 - val_acc: 0.9687\n",
      "Epoch 247/250\n",
      "1s - loss: 0.0797 - acc: 0.9694 - val_loss: 0.0843 - val_acc: 0.9685\n",
      "Epoch 248/250\n",
      "1s - loss: 0.0797 - acc: 0.9695 - val_loss: 0.0844 - val_acc: 0.9684\n",
      "Epoch 249/250\n",
      "1s - loss: 0.0800 - acc: 0.9693 - val_loss: 0.0838 - val_acc: 0.9684\n",
      "Epoch 250/250\n",
      "1s - loss: 0.0799 - acc: 0.9693 - val_loss: 0.0844 - val_acc: 0.9686\n",
      "0.928125594621\n",
      "XGB: 0.9281705 +- 0.0010619\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, AveragePooling1D\n",
    "\n",
    "pred_train = np.zeros((len(y_all), 17))\n",
    "sc,sc_mean = [],[]\n",
    "\n",
    "for itr, ite in cv_folds:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=106))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(64, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(17, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(tr_conc.ix[itr].values, y_all.ix[itr].values,\n",
    "              validation_data=(tr_conc.ix[ite].values, y_all.ix[ite].values),\n",
    "              batch_size=100, nb_epoch=250, verbose=2)\n",
    "    pred_train[ite] = model.predict(tr_conc.ix[ite].values)\n",
    "    loc_sc = fbeta_score(y_all.ix[ite, :], pred_train[ite] > 0.2, beta=2, average='samples')\n",
    "    print(loc_sc)\n",
    "    sc.append(loc_sc)\n",
    "print('XGB: {:.7f} +- {:.7f}'.format(np.mean(sc), np.std(sc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9260916"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9301398\n",
    "0.9281668\n",
    "0.9283114\n",
    "0.9260916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([    0,     1,     2, ..., 40475, 40477, 40478]),\n",
       " array([    5,     8,    10, ..., 40467, 40468, 40476])]"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_folds[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.99929190e-01,   6.28942676e-17,   2.41957451e-07, ...,\n",
       "          4.46432103e-14,   7.57077373e-07,   4.85096782e-01],\n",
       "       [  9.99942660e-01,   3.24663233e-05,   7.39261625e-13, ...,\n",
       "          7.32313236e-03,   1.13315070e-02,   9.99699116e-01],\n",
       "       [  9.99994755e-01,   1.49332941e-16,   2.31145729e-17, ...,\n",
       "          4.05182797e-11,   2.48834181e-06,   9.99659657e-01],\n",
       "       ..., \n",
       "       [  7.98553050e-01,   2.66852612e-06,   1.14681531e-09, ...,\n",
       "          2.55176425e-03,   4.97260273e-01,   9.99344051e-01],\n",
       "       [  9.06705201e-01,   2.78187304e-08,   1.21807426e-08, ...,\n",
       "          3.74744704e-04,   3.46397818e-03,   8.91566396e-01],\n",
       "       [  9.99999881e-01,   4.51274786e-08,   7.94609534e-09, ...,\n",
       "          6.08505234e-02,   1.83584113e-02,   6.85956271e-04]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.928760026482\n"
     ]
    }
   ],
   "source": [
    "ma = 0\n",
    "q=[0.2]*17\n",
    "for t in range(17):\n",
    "    ib = 0.2\n",
    "    for i in np.linspace(0.05, 0.3, 50):\n",
    "        q[t] = i\n",
    "        cur = fbeta_score(y_all, np.greater(np.array(pred_train), q), beta=2, average='samples')\n",
    "        if (cur > ma):\n",
    "            ma = cur\n",
    "            ib = i\n",
    "    q[t] = ib\n",
    "print(ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.930672899448\n",
    "0.929144586029\n",
    "0.929116884774\n",
    "0.927450949963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.22346938775510206,\n",
       " 0.11632653061224489,\n",
       " 0.075510204081632656,\n",
       " 0.16224489795918368,\n",
       " 0.24897959183673468,\n",
       " 0.2,\n",
       " 0.23367346938775507,\n",
       " 0.18265306122448977,\n",
       " 0.16734693877551021,\n",
       " 0.21836734693877552,\n",
       " 0.21326530612244898,\n",
       " 0.21326530612244898,\n",
       " 0.18265306122448977,\n",
       " 0.22857142857142854,\n",
       " 0.16224489795918368,\n",
       " 0.16224489795918368,\n",
       " 0.17755102040816328]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## голосование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vote = te1[0] * 0\n",
    "for i in range(4):\n",
    "    vote = vote + (np.array(te1[i]) > q_all[0]) * 1.0\n",
    "    \n",
    "for i in range(4):\n",
    "    vote = vote + (np.array(te2[i]) > q_all[1]) * 1.0    \n",
    "\n",
    "for i in range(4):\n",
    "    vote = vote + (np.array(te3[i]) > q_all[2]) * 1.0 \n",
    "    \n",
    "for i in range(5):\n",
    "    vote = vote + (np.array(te4[i]) > q_all[3]) * 1.0 \n",
    "    \n",
    "vote = vote + (np.array(ave5) > 0.2) * 1.0 \n",
    "vote = vote + (np.array(ave6) > 0.2) * 1.0 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61161</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61162</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61163</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61164</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61165</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61166</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61167</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61168</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61169</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61170</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61171</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61172</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61173</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61174</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61175</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61176</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61177</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61178</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61179</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61180</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61181</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61182</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61183</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61184</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61185</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61186</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61187</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61188</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61189</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61190</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61191 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1     2    3    4     5     6     7    8    9    10    11  \\\n",
       "0      19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "1      19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "2      19.0  0.0   0.0  0.0  0.0  19.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "3      19.0  0.0   0.0  0.0  0.0   5.0   0.0   1.0  0.0  0.0  13.0  19.0   \n",
       "4      18.0  0.0  16.0  0.0  0.0  19.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "5      19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "6      19.0  0.0   0.0  0.0  0.0  13.0   3.0   8.0  0.0  0.0  19.0  19.0   \n",
       "7      18.0  0.0   0.0  0.0  0.0   0.0  19.0   0.0  0.0  0.0   0.0  12.0   \n",
       "8      19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "9      19.0  0.0   0.0  0.0  0.0   1.0   0.0   0.0  0.0  0.0  12.0  19.0   \n",
       "10     19.0  0.0   0.0  0.0  0.0  19.0   0.0  10.0  0.0  0.0   0.0   4.0   \n",
       "11     19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0  19.0  19.0   \n",
       "12      0.0  0.0  19.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "13     19.0  0.0   0.0  0.0  0.0   0.0  13.0   2.0  0.0  0.0   9.0  19.0   \n",
       "14     19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0  16.0  16.0   \n",
       "15     19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   4.0   \n",
       "16     19.0  0.0   0.0  0.0  0.0   0.0  19.0   1.0  0.0  0.0  19.0  19.0   \n",
       "17     19.0  0.0   0.0  0.0  0.0  19.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "18     19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "19     19.0  0.0   0.0  0.0  0.0   0.0  19.0  19.0  2.0  0.0   0.0   0.0   \n",
       "20     19.0  0.0   0.0  0.0  0.0   0.0   9.0  19.0  0.0  0.0   2.0  17.0   \n",
       "21     19.0  0.0   0.0  0.0  0.0   0.0   3.0  19.0  0.0  0.0   0.0  19.0   \n",
       "22     19.0  0.0   0.0  0.0  0.0   0.0  19.0   2.0  0.0  0.0  13.0  19.0   \n",
       "23      2.0  0.0  19.0  0.0  0.0   0.0   0.0   1.0  0.0  0.0   0.0   0.0   \n",
       "24     19.0  0.0   0.0  0.0  0.0  19.0  19.0  14.0  0.0  0.0   0.0  19.0   \n",
       "25     19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "26     19.0  0.0   0.0  0.0  0.0  19.0  19.0   0.0  0.0  0.0   0.0  19.0   \n",
       "27     19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   1.0  19.0   \n",
       "28     19.0  0.0   0.0  0.0  0.0  19.0  19.0   0.0  0.0  0.0   0.0  19.0   \n",
       "29     19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "...     ...  ...   ...  ...  ...   ...   ...   ...  ...  ...   ...   ...   \n",
       "61161  19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0  18.0  17.0   \n",
       "61162  19.0  0.0   0.0  0.0  0.0   0.0  19.0   0.0  0.0  0.0  19.0  19.0   \n",
       "61163  19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "61164  19.0  0.0   0.0  0.0  0.0  16.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "61165  19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0  19.0  19.0   \n",
       "61166  19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "61167  19.0  0.0   0.0  0.0  0.0  19.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "61168  19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "61169  19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "61170  19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "61171  19.0  0.0   0.0  0.0  0.0   0.0   8.0   0.0  0.0  0.0  19.0  19.0   \n",
       "61172  19.0  0.0   0.0  0.0  0.0   0.0  19.0   0.0  0.0  0.0  10.0  19.0   \n",
       "61173  19.0  0.0   0.0  4.0  0.0   0.0   0.0   1.0  0.0  0.0   0.0   0.0   \n",
       "61174  19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "61175  19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "61176  19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "61177  19.0  0.0   0.0  0.0  0.0   0.0  19.0   0.0  0.0  0.0   3.0  19.0   \n",
       "61178  19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "61179  19.0  0.0   0.0  0.0  0.0   0.0  19.0   1.0  0.0  0.0   0.0  19.0   \n",
       "61180  19.0  0.0   0.0  0.0  0.0   0.0  19.0   0.0  0.0  0.0   0.0  19.0   \n",
       "61181  19.0  0.0   0.0  1.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "61182  19.0  0.0   0.0  0.0  0.0   0.0  19.0   0.0  0.0  0.0  19.0  19.0   \n",
       "61183  19.0  0.0   0.0  0.0  0.0   0.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "61184  19.0  0.0   0.0  0.0  0.0   0.0   0.0  19.0  0.0  0.0   0.0   1.0   \n",
       "61185  19.0  0.0  14.0  0.0  0.0  19.0   0.0  19.0  0.0  0.0   0.0  14.0   \n",
       "61186  10.0  0.0  19.0  0.0  0.0   9.0   0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "61187  19.0  0.0   0.0  0.0  0.0   0.0   0.0  19.0  0.0  0.0   0.0   2.0   \n",
       "61188  19.0  0.0   0.0  0.0  0.0   0.0  14.0  13.0  0.0  0.0   0.0   2.0   \n",
       "61189   1.0  0.0  19.0  0.0  0.0   0.0   0.0   1.0  0.0  0.0   0.0   0.0   \n",
       "61190  19.0  0.0   0.0  0.0  0.0   0.0  19.0   2.0  0.0  0.0   0.0   4.0   \n",
       "\n",
       "         12    13   14   15    16  \n",
       "0       0.0   0.0  0.0  0.0  19.0  \n",
       "1       0.0   0.0  0.0  0.0  19.0  \n",
       "2       0.0   0.0  0.0  0.0   0.0  \n",
       "3       0.0   0.0  0.0  0.0  19.0  \n",
       "4       0.0   0.0  0.0  0.0   0.0  \n",
       "5       0.0   0.0  0.0  0.0  19.0  \n",
       "6       6.0  19.0  0.0  0.0  16.0  \n",
       "7       0.0  19.0  0.0  0.0  19.0  \n",
       "8       0.0   0.0  0.0  0.0  19.0  \n",
       "9      19.0   0.0  0.0  0.0  16.0  \n",
       "10      0.0   0.0  0.0  0.0   1.0  \n",
       "11      0.0   0.0  0.0  0.0  19.0  \n",
       "12      1.0   0.0  0.0  0.0   0.0  \n",
       "13      0.0   0.0  0.0  0.0  19.0  \n",
       "14      3.0   0.0  0.0  0.0  19.0  \n",
       "15      0.0   0.0  0.0  0.0  19.0  \n",
       "16      0.0  19.0  0.0  0.0  19.0  \n",
       "17      0.0   0.0  0.0  0.0   7.0  \n",
       "18     16.0   0.0  0.0  0.0  19.0  \n",
       "19      0.0   0.0  0.0  0.0  19.0  \n",
       "20      0.0   0.0  0.0  0.0  19.0  \n",
       "21      0.0   4.0  0.0  0.0  19.0  \n",
       "22      0.0  19.0  0.0  0.0  19.0  \n",
       "23      1.0   0.0  0.0  0.0   0.0  \n",
       "24      0.0   0.0  0.0  0.0   0.0  \n",
       "25      0.0   0.0  0.0  0.0  19.0  \n",
       "26      0.0  10.0  0.0  0.0   6.0  \n",
       "27      0.0   0.0  0.0  5.0  19.0  \n",
       "28      0.0   8.0  0.0  0.0  13.0  \n",
       "29      0.0   0.0  0.0  0.0  19.0  \n",
       "...     ...   ...  ...  ...   ...  \n",
       "61161   0.0   0.0  0.0  0.0  19.0  \n",
       "61162   0.0  19.0  0.0  0.0  19.0  \n",
       "61163   0.0   0.0  0.0  0.0  19.0  \n",
       "61164   0.0   0.0  0.0  0.0  19.0  \n",
       "61165   0.0   0.0  0.0  0.0  19.0  \n",
       "61166   0.0   0.0  0.0  0.0  19.0  \n",
       "61167   0.0   0.0  0.0  0.0   0.0  \n",
       "61168   0.0   0.0  0.0  0.0  19.0  \n",
       "61169   0.0   0.0  0.0  0.0  19.0  \n",
       "61170   0.0   0.0  0.0  0.0  19.0  \n",
       "61171   0.0   4.0  0.0  0.0  19.0  \n",
       "61172   0.0  19.0  0.0  0.0  19.0  \n",
       "61173   0.0   0.0  0.0  0.0  19.0  \n",
       "61174   0.0   0.0  0.0  0.0  19.0  \n",
       "61175   0.0   0.0  0.0  0.0  19.0  \n",
       "61176   0.0   0.0  0.0  0.0  19.0  \n",
       "61177   0.0  19.0  0.0  0.0  19.0  \n",
       "61178   0.0   0.0  0.0  0.0  19.0  \n",
       "61179   0.0  19.0  0.0  0.0  19.0  \n",
       "61180   0.0   9.0  0.0  2.0  19.0  \n",
       "61181   0.0   0.0  0.0  0.0  19.0  \n",
       "61182   0.0   2.0  0.0  0.0  19.0  \n",
       "61183   0.0   0.0  0.0  0.0  19.0  \n",
       "61184   0.0   0.0  0.0  0.0  19.0  \n",
       "61185   0.0   0.0  0.0  0.0   1.0  \n",
       "61186   0.0   0.0  0.0  0.0   0.0  \n",
       "61187   0.0   0.0  0.0  0.0  19.0  \n",
       "61188   0.0   0.0  0.0  0.0  19.0  \n",
       "61189   0.0   0.0  0.0  0.0   0.0  \n",
       "61190   0.0  19.0  0.0  0.0  19.0  \n",
       "\n",
       "[61191 rows x 17 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6232\n",
      "6844\n",
      "9594\n",
      "16282\n",
      "784888\n"
     ]
    }
   ],
   "source": [
    "print((vote == 4.0).sum().sum())\n",
    "print((vote == 3.0).sum().sum())\n",
    "print((vote == 2.0).sum().sum())\n",
    "print((vote == 1.0).sum().sum())\n",
    "print((vote == 0.0).sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_multilabel = OneVsRestClassifier(XGBClassifier(max_depth=2,\n",
    "                                                       learning_rate=0.1,\n",
    "                                                       n_estimators=100,\n",
    "                                                       min_child_weight=1))\n",
    "    #clf_multilabel = OneVsRestClassifier(LogisticRegression())\n",
    "clf_multilabel.fit(tr_conc.ix[:].values, y_all.ix[:].values)\n",
    "pr_xgb = clf_multilabel.predict_proba(te_conc.ix[:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_test = pr_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fao3864/anaconda3/lib/python3.6/site-packages/keras/models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    }
   ],
   "source": [
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=106))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(64, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(17, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(tr_conc.ix[:].values, y_all.ix[:].values,\n",
    "              validation_data=(tr_conc.ix[ite].values, y_all.ix[ite].values),\n",
    "              batch_size=500, nb_epoch=250, verbose=0)\n",
    "    nn_test2 = model.predict(te_conc.ix[:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_multilabel = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "clf_multilabel.fit(tr_conc.ix[:].values, y_all.ix[:].values)\n",
    "lin_test = clf_multilabel.predict_proba(te_conc.ix[:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_multilabel = OneVsRestClassifier(ExtraTreesClassifier(n_estimators=500, criterion='gini', max_depth=None))\n",
    "\n",
    "clf_multilabel.fit(tr_conc.ix[:].values, y_all.ix[:].values)\n",
    "ext_test = clf_multilabel.predict_proba(te_conc.ix[:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_multilabel = OneVsRestClassifier(lgb.sklearn.LGBMClassifier(n_estimators=150, seed=0, **params)) \n",
    "\n",
    "clf_multilabel.fit(tr_conc.ix[:].values, y_all.ix[:].values)\n",
    "lgb_test = clf_multilabel.predict_proba(te_conc.ix[:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_test = (lin_test*0.14 + xgb_test*0.3+ ext_test*0.14+lgb_test*0.14+nn_test*0.14+nn_test2*0.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_test =  (lgb_test+ext_test+lin_test+nn_test+xgb_test)/5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bl = pd.DataFrame(pd.DataFrame(my_test).values * 0.65 + pd.DataFrame(ave4).values * 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_test = np.array(bl) > q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_test = np.array(ave5) > 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in p_test[:]:\n",
    "    if (i.sum() == 0):\n",
    "        print(i.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     58256\n",
       "1       681\n",
       "2      4787\n",
       "3       553\n",
       "4       203\n",
       "5     12841\n",
       "6     15319\n",
       "7     14456\n",
       "8       610\n",
       "9       208\n",
       "10    11105\n",
       "11    23717\n",
       "12     5503\n",
       "13     7837\n",
       "14      251\n",
       "15     2218\n",
       "16    45478\n",
       "dtype: int64"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204023"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pt = pd.DataFrame(p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203940"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in pt.index:\n",
    "    if pt.ix[i,:].sum() == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_test = model.predict(te_conc.ix[:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_multilabel.fit(tr_conc.ix[:].values, y_all.ix[:].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_test = clf_multilabel.predict_proba(te_conc.ix[:].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_test = np.array(vote) > 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_test = np.array(ave) > q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29999999999999999,\n",
       " 0.13673469387755102,\n",
       " 0.16734693877551021,\n",
       " 0.27959183673469384,\n",
       " 0.12142857142857143,\n",
       " 0.22857142857142854,\n",
       " 0.24897959183673468,\n",
       " 0.23877551020408161,\n",
       " 0.13163265306122449,\n",
       " 0.090816326530612251,\n",
       " 0.21326530612244898,\n",
       " 0.19795918367346937,\n",
       " 0.21326530612244898,\n",
       " 0.19285714285714284,\n",
       " 0.1010204081632653,\n",
       " 0.16734693877551021,\n",
       " 0.24387755102040815]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_all[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61191"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61191"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_test = np.array(blend) > q_all[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_test  = np.array(p_test) > 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agriculture': 11,\n",
       " 'artisinal_mine': 1,\n",
       " 'bare_ground': 15,\n",
       " 'blooming': 3,\n",
       " 'blow_down': 9,\n",
       " 'clear': 16,\n",
       " 'cloudy': 2,\n",
       " 'conventional_mine': 4,\n",
       " 'cultivation': 10,\n",
       " 'habitation': 13,\n",
       " 'haze': 12,\n",
       " 'partly_cloudy': 5,\n",
       " 'primary': 0,\n",
       " 'road': 6,\n",
       " 'selective_logging': 8,\n",
       " 'slash_burn': 14,\n",
       " 'water': 7}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = np.array(['primary', 'artisinal_mine', 'cloudy','blooming','conventional_mine',\n",
    "                       'partly_cloudy','road','water','selective_logging','blow_down',\n",
    "                       'cultivation','agriculture','haze','habitation',\n",
    "                       'slash_burn','bare_ground','clear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../data/sample_submission_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61191"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(len(p_test)):\n",
    "    res += [' '.join(categories[p_test[i]])]\n",
    "    if res[i] == ' ':\n",
    "        res[i] = 'primary'\n",
    "        print('primary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_submission['tags'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>primary partly_cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>primary cultivation agriculture clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>primary cloudy partly_cloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                   tags\n",
       "0     test_0                          primary clear\n",
       "1     test_1                          primary clear\n",
       "2     test_2                  primary partly_cloudy\n",
       "3     test_3  primary cultivation agriculture clear\n",
       "4     test_4           primary cloudy partly_cloudy"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>primary partly_cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>primary cultivation agriculture clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>primary cloudy partly_cloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                   tags\n",
       "0     test_0                          primary clear\n",
       "1     test_1                          primary clear\n",
       "2     test_2                  primary partly_cloudy\n",
       "3     test_3  primary cultivation agriculture clear\n",
       "4     test_4           primary cloudy partly_cloudy"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"stack_3.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
