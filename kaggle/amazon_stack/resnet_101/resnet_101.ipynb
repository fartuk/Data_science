{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.4/dist-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['shuffle']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn import manifold, decomposition, linear_model, ensemble, neighbors, cross_validation\n",
    "import sklearn\n",
    "from sklearn import pipeline, preprocessing, feature_extraction\n",
    "#import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "#import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, auc, roc_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#import lightgbm as lgbm\n",
    "import scipy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cv2\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                       tags\n",
       "0    train_0                               haze primary\n",
       "1    train_1            agriculture clear primary water\n",
       "2    train_2                              clear primary\n",
       "3    train_3                              clear primary\n",
       "4    train_4  agriculture clear habitation primary road"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/train_v2.csv')\n",
    "labels = df_train['tags'].apply(lambda x: x.split(' '))\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission_v2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_labels = ['clear', 'partly_cloudy', 'haze', 'cloudy']\n",
    "weather = []\n",
    "\n",
    "for i in [l.split(' ') for l in df_train['tags'].values]:\n",
    "    for j in i:\n",
    "        if j in weather_labels:\n",
    "            weather += [j]\n",
    "\n",
    "df_train['weather'] = weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(4, shuffle=True, random_state=1)\n",
    "folds = []\n",
    "for itr, ite in skf.split(df_train, df_train.weather):\n",
    "    folds += [[itr, ite]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([    0,     2,     4, ..., 40476, 40477, 40478]),\n",
       "  array([    1,     3,    16, ..., 40469, 40470, 40475])],\n",
       " [array([    0,     1,     2, ..., 40476, 40477, 40478]),\n",
       "  array([    4,     6,     8, ..., 40471, 40472, 40473])],\n",
       " [array([    0,     1,     3, ..., 40473, 40475, 40476]),\n",
       "  array([    2,     5,    12, ..., 40474, 40477, 40478])],\n",
       " [array([    1,     2,     3, ..., 40475, 40477, 40478]),\n",
       "  array([    0,     7,    13, ..., 40464, 40466, 40476])]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation, Lambda, GlobalAveragePooling2D, Merge\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import initializations\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras import backend as K\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(3000)\n",
    "\n",
    "class Scale(Layer):\n",
    "    '''Learns a set of weights and biases used for scaling the input data.\n",
    "    the output consists simply in an element-wise multiplication of the input\n",
    "    and a sum of a set of constants:\n",
    "        out = in * gamma + beta,\n",
    "    where 'gamma' and 'beta' are the weights and biases larned.\n",
    "    # Arguments\n",
    "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
    "            if your input tensor has shape (samples, channels, rows, cols),\n",
    "            set axis to 1 to normalize per feature map (channels axis).\n",
    "        momentum: momentum in the computation of the\n",
    "            exponential average of the mean and standard deviation\n",
    "            of the data, for feature-wise normalization.\n",
    "        weights: Initialization weights.\n",
    "            List of 2 Numpy arrays, with shapes:\n",
    "            `[(input_shape,), (input_shape,)]`\n",
    "        beta_init: name of initialization function for shift parameter\n",
    "            (see [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_init: name of initialization function for scale parameter (see\n",
    "            [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "                        This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_init: name of initialization function for scale parameter (see\n",
    "            [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "    '''\n",
    "    def __init__(self, weights=None, axis=-1, momentum = 0.9, beta_init='zero', gamma_init='one', **kwargs):\n",
    "        self.momentum = momentum\n",
    "        self.axis = axis\n",
    "        self.beta_init = initializations.get(beta_init)\n",
    "        self.gamma_init = initializations.get(gamma_init)\n",
    "        self.initial_weights = weights\n",
    "        super(Scale, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (int(input_shape[self.axis]),)\n",
    "\n",
    "        self.gamma = self.gamma_init(shape, name='{}_gamma'.format(self.name))\n",
    "        self.beta = self.beta_init(shape, name='{}_beta'.format(self.name))\n",
    "        self.trainable_weights = [self.gamma, self.beta]\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "        del self.initial_weights\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "        out = K.reshape(self.gamma, broadcast_shape) * x + K.reshape(self.beta, broadcast_shape)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"momentum\": self.momentum, \"axis\": self.axis}\n",
    "        base_config = super(Scale, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    '''The identity_block is the block that has no conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    scale_name_base = 'scale' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a', bias=False)(input_tensor)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2a_relu')(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)\n",
    "    x = Convolution2D(nb_filter2, kernel_size, kernel_size,\n",
    "                      name=conv_name_base + '2b', bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2b_relu')(x)\n",
    "\n",
    "    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c', bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)\n",
    "\n",
    "    x = merge([x, input_tensor], mode='sum', name='res' + str(stage) + block)\n",
    "    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    '''conv_block is the block that has a conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n",
    "    And the shortcut should have subsample=(2,2) as well\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    scale_name_base = 'scale' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Convolution2D(nb_filter1, 1, 1, subsample=strides,\n",
    "                      name=conv_name_base + '2a', bias=False)(input_tensor)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2a_relu')(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)\n",
    "    x = Convolution2D(nb_filter2, kernel_size, kernel_size,\n",
    "                      name=conv_name_base + '2b', bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2b_relu')(x)\n",
    "\n",
    "    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c', bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = Convolution2D(nb_filter3, 1, 1, subsample=strides,\n",
    "                             name=conv_name_base + '1', bias=False)(input_tensor)\n",
    "    shortcut = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "    shortcut = Scale(axis=bn_axis, name=scale_name_base + '1')(shortcut)\n",
    "\n",
    "    x = merge([x, shortcut], mode='sum', name='res' + str(stage) + block)\n",
    "    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)\n",
    "    return x\n",
    "\n",
    "def resnet101_model(weights_path=None):\n",
    "    '''Instantiate the ResNet101 architecture,\n",
    "    # Arguments\n",
    "        weights_path: path to pretrained weight file\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "\n",
    "    # Handle Dimension Ordering for different backends\n",
    "    global bn_axis\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "      bn_axis = 3\n",
    "      img_input = Input(shape=(224, 224, 3), name='data')\n",
    "    else:\n",
    "      bn_axis = 1\n",
    "      img_input = Input(shape=(3, 224, 224), name='data')\n",
    "\n",
    "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
    "    x = Convolution2D(64, 7, 7, subsample=(2, 2), name='conv1', bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = Scale(axis=bn_axis, name='scale_conv1')(x)\n",
    "    x = Activation('relu', name='conv1_relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    for i in range(1,3):\n",
    "      x = identity_block(x, 3, [128, 128, 512], stage=3, block='b'+str(i))\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    for i in range(1,23):\n",
    "      x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b'+str(i))\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    x_fc = AveragePooling2D((7, 7), name='avg_pool')(x)\n",
    "    x_fc = Flatten()(x_fc)\n",
    "    #x_fc = Dense(1000, activation='softmax', name='fc1000')(x_fc)\n",
    "\n",
    "    model = Model(img_input, x_fc)\n",
    "    \n",
    "    # load weights\n",
    "    if weights_path:\n",
    "      model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "channel = 3\n",
    "num_classes = 17 \n",
    "\n",
    "    # Load Cifar10 data. Please implement your own load_data() module for your own dataset\n",
    "    #X_train, Y_train, X_valid, Y_valid = load_cifar10_data(img_rows, img_cols)\n",
    "\n",
    "    # Load our model\n",
    "#model = resnet152_model(img_rows, img_cols, channel, num_classes)\n",
    "#model = resnet152_model()\n",
    "weights_path = 'resnet101_weights_tf.h5'\n",
    "\n",
    "\n",
    "  # Test pretrained model\n",
    "base_model = resnet101_model(weights_path)\n",
    "#sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "#x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "#x = Dense(512, activation='relu')(x)\n",
    "\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(17, activation='sigmoid', name='last_layer')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(loss='binary_crossentropy', # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('resnet_101.h5', save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = np.array(['primary', 'artisinal_mine', 'cloudy','blooming','conventional_mine',\n",
    "                       'partly_cloudy','road','water','selective_logging','blow_down',\n",
    "                       'cultivation','agriculture','haze','habitation',\n",
    "                       'slash_burn','bare_ground','clear'])\n",
    "\n",
    "label_map = {l: i for i, l in enumerate(categories)}\n",
    "\n",
    "\n",
    "#labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
    "#label_map = {l: i for i, l in enumerate(labels)}\n",
    "weather_label_map = {l: i for i, l in enumerate(weather_labels)}\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_generator(tr, batch_size):\n",
    "    while(1):\n",
    "        shuffle(tr)\n",
    "        batches = []\n",
    "        cnt = 0\n",
    "        tmp_arr = []\n",
    "        for i in tr:\n",
    "            tmp_arr += [i]\n",
    "            if cnt % batch_size == 0:\n",
    "                batches += [tmp_arr]\n",
    "                tmp_arr = []\n",
    "            cnt += 1\n",
    "        batches += [tmp_arr]\n",
    "        print(len(batches))\n",
    "        batch_num = 0\n",
    "        for i in batches:\n",
    "            batch_num += 1\n",
    "            if batch_num % 50 == 0:\n",
    "                print(batch_num)\n",
    "            x_train = []\n",
    "            y_train = []\n",
    "\n",
    "            for f, tags, weather in df_train.ix[i, :].values:\n",
    "                img = cv2.imread('data/train-jpg/{}.jpg'.format(f))\n",
    "                targets = np.zeros(17)\n",
    "                for t in tags.split(' '):\n",
    "                    targets[label_map[t]] = 1\n",
    "                #targets[weather_label_map[weather]] = 1\n",
    "                img = cv2.resize(img, (224, 224))\n",
    "                num_rows, num_cols = img.shape[:2]\n",
    "                \n",
    "                rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), random.choice([0, 90, 180, 270]), 1)\n",
    "                img = cv2.warpAffine(img, rotation_matrix, (num_cols, num_rows))\n",
    "                \n",
    "                if random.choice([0, 1]) == 1:\n",
    "                    img = cv2.flip( img, 0  )\n",
    "\n",
    "                #1\n",
    "                x_train.append(img)\n",
    "                y_train.append(targets)  \n",
    "            \n",
    "            \n",
    "            y_train = np.array(y_train, np.uint8)\n",
    "            x_train = np.array(x_train, np.float16) / 255.0\n",
    "            yield (x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def val_generator(tr, batch_size, seed=0, shuf=1):\n",
    "    while(1):\n",
    "        if shuf==1:\n",
    "            shuffle(tr)\n",
    "        batches = []\n",
    "        cnt = 0\n",
    "        tmp_arr = []\n",
    "        for i in tr:\n",
    "            tmp_arr += [i]\n",
    "            if cnt % batch_size == 0:\n",
    "                batches += [tmp_arr]\n",
    "                tmp_arr = []\n",
    "            cnt += 1\n",
    "        batches += [tmp_arr]\n",
    "        \n",
    "        for i in batches:\n",
    "                \n",
    "            x_train = []\n",
    "            y_train = []\n",
    "\n",
    "            for f, tags, weather in df_train.ix[i, :].values:\n",
    "                img = cv2.imread('data/train-jpg/{}.jpg'.format(f))\n",
    "                targets = np.zeros(17)\n",
    "                for t in tags.split(' '):\n",
    "                    targets[label_map[t]] = 1\n",
    "                #targets[weather_label_map[weather]] = 1\n",
    "                img = cv2.resize(img, (224, 224))\n",
    "                num_rows, num_cols = img.shape[:2]\n",
    "                \n",
    "                rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), (seed % 4)*90, 1)\n",
    "                img = cv2.warpAffine(img, rotation_matrix, (num_cols, num_rows))\n",
    "                \n",
    "                if (seed % 2 == 1):\n",
    "                    img = cv2.flip( img, 0  )\n",
    "                #1\n",
    "                x_train.append(img)\n",
    "                y_train.append(targets)  \n",
    "\n",
    "            \n",
    "            y_train = np.array(y_train, np.uint8)\n",
    "            x_train = np.array(x_train, np.float16) / 255.0\n",
    "            yield (x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel_launcher.py:23: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/Keras-1.2.2-py3.4.egg/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545s - loss: 0.1637 - acc: 0.9429 - val_loss: 0.7616 - val_acc: 0.8308\n",
      "Epoch 2/10\n",
      "150\n",
      "200\n",
      "250\n",
      "235s - loss: 0.1271 - acc: 0.9523 - val_loss: 1.3609 - val_acc: 0.9055\n",
      "Epoch 3/10\n",
      "300\n",
      "350\n",
      "236s - loss: 0.1232 - acc: 0.9528 - val_loss: 1.3080 - val_acc: 0.9048\n",
      "Epoch 4/10\n",
      "400\n",
      "450\n",
      "476\n",
      "236s - loss: 0.1202 - acc: 0.9543 - val_loss: 1.2704 - val_acc: 0.8992\n",
      "Epoch 5/10\n",
      "50\n",
      "100\n",
      "150\n",
      "236s - loss: 0.1135 - acc: 0.9565 - val_loss: 1.2189 - val_acc: 0.8511\n",
      "Epoch 6/10\n",
      "200\n",
      "250\n",
      "235s - loss: 0.1123 - acc: 0.9568 - val_loss: 0.7749 - val_acc: 0.8541\n",
      "Epoch 7/10\n",
      "300\n",
      "350\n",
      "400\n",
      "281s - loss: 0.1132 - acc: 0.9570 - val_loss: 0.5978 - val_acc: 0.8286\n",
      "Epoch 8/10\n",
      "450\n",
      "476\n",
      "50\n",
      "284s - loss: 0.1118 - acc: 0.9566 - val_loss: 0.4126 - val_acc: 0.8613\n",
      "Epoch 9/10\n",
      "100\n",
      "150\n",
      "284s - loss: 0.1100 - acc: 0.9576 - val_loss: 0.2122 - val_acc: 0.9261\n",
      "Epoch 10/10\n",
      "200\n",
      "250\n",
      "300\n",
      "253s - loss: 0.1086 - acc: 0.9576 - val_loss: 0.1587 - val_acc: 0.9426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6bea3ebe10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit_generator(all_generator(folds[0][0], batch_size),\n",
    "                    samples_per_epoch=8000,\n",
    "                    validation_data=val_generator(folds[0][1], batch_size),\n",
    "                    nb_val_samples=5000,\n",
    "                    nb_epoch=10,\n",
    "                    verbose=2,\n",
    "                    callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476Epoch 1/4\n",
      "\n",
      "50\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/Keras-1.2.2-py3.4.egg/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256s - loss: 0.1095 - acc: 0.9572 - val_loss: 0.1145 - val_acc: 0.9570\n",
      "Epoch 2/4\n",
      "150\n",
      "200\n",
      "250\n",
      "251s - loss: 0.1053 - acc: 0.9590 - val_loss: 0.1079 - val_acc: 0.9592\n",
      "Epoch 3/4\n",
      "300\n",
      "350\n",
      "252s - loss: 0.1065 - acc: 0.9587 - val_loss: 0.1061 - val_acc: 0.9604\n",
      "Epoch 4/4\n",
      "400\n",
      "450\n",
      "476\n",
      "235s - loss: 0.1087 - acc: 0.9579 - val_loss: 0.1167 - val_acc: 0.9584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6bea23fdd8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit_generator(all_generator(folds[0][0], batch_size),\n",
    "                    samples_per_epoch=8000,\n",
    "                    validation_data=val_generator(folds[0][1], batch_size),\n",
    "                    nb_val_samples=5000,\n",
    "                    nb_epoch=4,\n",
    "                    verbose=2,\n",
    "                    callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('resnet152_stage_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for layer in model.layers[:500]:\n",
    "#    layer.trainable = False\n",
    "for layer in model.layers[:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def fbeta(y_true, y_pred, threshold_shift=0):\n",
    "    beta = 2\n",
    "\n",
    "    # just in case of hipster activation at the final layer\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "\n",
    "    # shifting the prediction threshold from .5 if needed\n",
    "    y_pred_bin = K.round(y_pred + threshold_shift)\n",
    "\n",
    "    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n",
    "    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    beta_squared = beta ** 2\n",
    "    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "adam = keras.optimizers.Adam(lr=0.000001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.000001, momentum=0.0, decay=0.0, nesterov=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='binary_crossentropy', # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n",
    "              optimizer=adam,\n",
    "              metrics=[fbeta])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model_checkpoint = ModelCheckpoint('resnet101_stage_3.h5', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1899\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/Keras-1.2.2-py3.4.egg/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706s - loss: 0.0874 - fbeta: 0.8830 - val_loss: 0.0934 - val_fbeta: 0.8797\n",
      "Epoch 2/5\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "672s - loss: 0.0862 - fbeta: 0.8861 - val_loss: 0.0917 - val_fbeta: 0.8803\n",
      "Epoch 3/5\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "670s - loss: 0.0868 - fbeta: 0.8857 - val_loss: 0.0940 - val_fbeta: 0.8779\n",
      "Epoch 4/5\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1899\n",
      "50\n",
      "100\n",
      "670s - loss: 0.0853 - fbeta: 0.8864 - val_loss: 0.0922 - val_fbeta: 0.8808\n",
      "Epoch 5/5\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "669s - loss: 0.0861 - fbeta: 0.8835 - val_loss: 0.0943 - val_fbeta: 0.8781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6b3980fc88>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "model.fit_generator(all_generator(folds[0][0], batch_size),\n",
    "                    samples_per_epoch=8000,\n",
    "                    validation_data=val_generator(folds[0][1], batch_size),\n",
    "                    nb_val_samples=5000,\n",
    "                    nb_epoch=5,\n",
    "                    verbose=2,\n",
    "                    callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/51899\n",
      "\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/Keras-1.2.2-py3.4.egg/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659s - loss: 0.0874 - fbeta: 0.8860 - val_loss: 0.0943 - val_fbeta: 0.8783\n",
      "Epoch 2/5\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "633s - loss: 0.0874 - fbeta: 0.8850 - val_loss: 0.0920 - val_fbeta: 0.8828\n",
      "Epoch 3/5\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "630s - loss: 0.0864 - fbeta: 0.8853 - val_loss: 0.0927 - val_fbeta: 0.8816\n",
      "Epoch 4/5\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1899\n",
      "50\n",
      "100\n",
      "630s - loss: 0.0892 - fbeta: 0.8822 - val_loss: 0.0939 - val_fbeta: 0.8775\n",
      "Epoch 5/5\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "629s - loss: 0.0883 - fbeta: 0.8828 - val_loss: 0.0949 - val_fbeta: 0.8780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6b4f4bec50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "model.fit_generator(all_generator(folds[0][0], batch_size),\n",
    "                    samples_per_epoch=8000,\n",
    "                    validation_data=val_generator(folds[0][1], batch_size),\n",
    "                    nb_val_samples=5000,\n",
    "                    nb_epoch=5,\n",
    "                    verbose=2,\n",
    "                    callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('fold1/resnet101.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([15980, 29233,  4392, ..., 18421, 23480, 15172]),\n",
       "  array([  301, 20036, 30432, ..., 19143,  3925, 36324])],\n",
       " [array([    0,     1,     2, ..., 40476, 40477, 40478]),\n",
       "  array([    4,     6,     8, ..., 40471, 40472, 40473])],\n",
       " [array([    0,     1,     3, ..., 40473, 40475, 40476]),\n",
       "  array([    2,     5,    12, ..., 40474, 40477, 40478])],\n",
       " [array([    1,     2,     3, ..., 40475, 40477, 40478]),\n",
       "  array([    0,     7,    13, ..., 40464, 40466, 40476])]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "val_pred = model.predict_generator(val_generator(folds[0][1], batch_size, shuf=0, seed=0), val_samples = len(folds[0][1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pred = val_pred * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "for i in range(8):\n",
    "    val_pred_tmp = model.predict_generator(val_generator(folds[0][1], batch_size, shuf=0, seed=i), val_samples = len(folds[0][1]) )\n",
    "    val_pred = val_pred + val_pred_tmp\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pred = val_pred / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred['id'] = folds[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = val_pred.sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del val_pred['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pred = pd.DataFrame(val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred.to_csv(\"fold1/train.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v=pd.read_csv('fold4/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13494"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13494"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_all = []\n",
    "for f, tags, weather in df_train.values:\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1\n",
    "    y_all += [targets]\n",
    "y_all = pd.DataFrame(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40479, 17)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10122"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_all.ix[folds[0][1].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.580099944106\n"
     ]
    }
   ],
   "source": [
    "          \n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "#print(y_valid)\n",
    "#print(p_valid)\n",
    "print(fbeta_score(y_val, np.array(val_pred) > 0.2, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.922365582803\n",
    "0.917041863094\n",
    "0.913685914092\n",
    "0.899545374775\n",
    "0.89285374022\n",
    "0.892722591666\n",
    "0.889811292029\n",
    "0.887170607068\n",
    "0.863779970799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61191"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61191"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61191"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(range(61191))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_generator(tr, batch_size, seed=0):\n",
    "    while(1):\n",
    "        batches = []\n",
    "        cnt = 0\n",
    "        tmp_arr = []\n",
    "        for i in tr:\n",
    "            tmp_arr += [i]\n",
    "            if cnt % batch_size == 0:\n",
    "                batches += [tmp_arr]\n",
    "                tmp_arr = []\n",
    "            cnt += 1\n",
    "        batches += [tmp_arr]\n",
    "        \n",
    "        for i in batches:\n",
    "                \n",
    "            x_test = []\n",
    "\n",
    "            for f in sample_submission.ix[i, 'image_name'].values:\n",
    "                img = cv2.imread('data/test-jpg/{}.jpg'.format(f))\n",
    "                img = cv2.resize(img, (224, 224))\n",
    "                num_rows, num_cols = img.shape[:2]\n",
    "                \n",
    "                rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), (seed % 4)*90, 1)\n",
    "                img = cv2.warpAffine(img, rotation_matrix, (num_cols, num_rows))\n",
    "                \n",
    "                if (seed % 2 == 1):\n",
    "                    img = cv2.flip( img, 0  )\n",
    "                #1\n",
    "                x_test.append(img)\n",
    "\n",
    "            \n",
    "            x_test = np.array(x_test, np.float16) / 255.0\n",
    "            yield x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "test_pred = model.predict_generator(test_generator(range(61191), batch_size, seed=0), val_samples = 61191)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_pred = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_test = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5924"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "for i in range(1,8):\n",
    "    test_pred = model.predict_generator(test_generator(range(61191), batch_size, seed=i), val_samples = 61191 )\n",
    "    result_pred = test_pred + result_pred\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_pred = result_pred / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_pred = pd.DataFrame(result_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_pred.to_csv(\"fold1/test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_test = np.array(p_test) > 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True],\n",
       "       [ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True],\n",
       "       [ True, False, False, False, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agriculture': 11,\n",
       " 'artisinal_mine': 1,\n",
       " 'bare_ground': 15,\n",
       " 'blooming': 3,\n",
       " 'blow_down': 9,\n",
       " 'clear': 16,\n",
       " 'cloudy': 2,\n",
       " 'conventional_mine': 4,\n",
       " 'cultivation': 10,\n",
       " 'habitation': 13,\n",
       " 'haze': 12,\n",
       " 'partly_cloudy': 5,\n",
       " 'primary': 0,\n",
       " 'road': 6,\n",
       " 'selective_logging': 8,\n",
       " 'slash_burn': 14,\n",
       " 'water': 7}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = np.array(['primary', 'artisinal_mine', 'cloudy','blooming','conventional_mine',\n",
    "                       'partly_cloudy','road','water','selective_logging','blow_down',\n",
    "                       'cultivation','agriculture','haze','habitation',\n",
    "                       'slash_burn','bare_ground','clear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(len(p_test)):\n",
    "    res += [' '.join(categories[p_test[i]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61191"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission['tags'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>primary partly_cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>primary cultivation agriculture clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>primary cloudy partly_cloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                   tags\n",
       "0     test_0                          primary clear\n",
       "1     test_1                          primary clear\n",
       "2     test_2                  primary partly_cloudy\n",
       "3     test_3  primary cultivation agriculture clear\n",
       "4     test_4           primary cloudy partly_cloudy"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>primary partly_cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>primary partly_cloudy cultivation agriculture ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>primary cloudy partly_cloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                               tags\n",
       "0     test_0                                      primary clear\n",
       "1     test_1                                      primary clear\n",
       "2     test_2                              primary partly_cloudy\n",
       "3     test_3  primary partly_cloudy cultivation agriculture ...\n",
       "4     test_4                       primary cloudy partly_cloudy"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61186</th>\n",
       "      <td>file_9995</td>\n",
       "      <td>primary partly_cloudy road water agriculture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61187</th>\n",
       "      <td>file_9996</td>\n",
       "      <td>primary road water cultivation agriculture hab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61188</th>\n",
       "      <td>file_9997</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61189</th>\n",
       "      <td>file_9998</td>\n",
       "      <td>primary cloudy haze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61190</th>\n",
       "      <td>file_9999</td>\n",
       "      <td>primary partly_cloudy road water agriculture h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_name                                               tags\n",
       "61186  file_9995       primary partly_cloudy road water agriculture\n",
       "61187  file_9996  primary road water cultivation agriculture hab...\n",
       "61188  file_9997                                      primary clear\n",
       "61189  file_9998                                primary cloudy haze\n",
       "61190  file_9999  primary partly_cloudy road water agriculture h..."
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"predictions/tmp13.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
