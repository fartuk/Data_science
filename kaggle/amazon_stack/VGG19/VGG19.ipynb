{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['shuffle']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn import manifold, decomposition, linear_model, ensemble, neighbors, cross_validation\n",
    "import sklearn\n",
    "from sklearn import pipeline, preprocessing, feature_extraction\n",
    "#import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "#import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, auc, roc_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#import lightgbm as lgbm\n",
    "import scipy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cv2\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                       tags\n",
       "0    train_0                               haze primary\n",
       "1    train_1            agriculture clear primary water\n",
       "2    train_2                              clear primary\n",
       "3    train_3                              clear primary\n",
       "4    train_4  agriculture clear habitation primary road"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/train_v2.csv')\n",
    "labels = df_train['tags'].apply(lambda x: x.split(' '))\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission_v2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_labels = ['clear', 'partly_cloudy', 'haze', 'cloudy']\n",
    "weather = []\n",
    "\n",
    "for i in [l.split(' ') for l in df_train['tags'].values]:\n",
    "    for j in i:\n",
    "        if j in weather_labels:\n",
    "            weather += [j]\n",
    "\n",
    "df_train['weather'] = weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(4, shuffle=True, random_state=1)\n",
    "folds = []\n",
    "for itr, ite in skf.split(df_train, df_train.weather):\n",
    "    folds += [[itr, ite]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(17, activation='sigmoid')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = np.array(['primary', 'artisinal_mine', 'cloudy','blooming','conventional_mine',\n",
    "                       'partly_cloudy','road','water','selective_logging','blow_down',\n",
    "                       'cultivation','agriculture','haze','habitation',\n",
    "                       'slash_burn','bare_ground','clear'])\n",
    "\n",
    "label_map = {l: i for i, l in enumerate(categories)}\n",
    "\n",
    "\n",
    "#labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
    "#label_map = {l: i for i, l in enumerate(labels)}\n",
    "weather_label_map = {l: i for i, l in enumerate(weather_labels)}\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_generator(tr, batch_size):\n",
    "    while(1):\n",
    "        shuffle(tr)\n",
    "        batches = []\n",
    "        cnt = 0\n",
    "        tmp_arr = []\n",
    "        for i in tr:\n",
    "            tmp_arr += [i]\n",
    "            if cnt % batch_size == 0:\n",
    "                batches += [tmp_arr]\n",
    "                tmp_arr = []\n",
    "            cnt += 1\n",
    "        batches += [tmp_arr]\n",
    "        print(len(batches))\n",
    "        batch_num = 0\n",
    "        for i in batches:\n",
    "            batch_num += 1\n",
    "            if batch_num % 50 == 0:\n",
    "                print(batch_num)\n",
    "            x_train = []\n",
    "            y_train = []\n",
    "\n",
    "            for f, tags, weather in df_train.ix[i, :].values:\n",
    "                img = cv2.imread('data/train-jpg/{}.jpg'.format(f))\n",
    "                targets = np.zeros(17)\n",
    "                for t in tags.split(' '):\n",
    "                    targets[label_map[t]] = 1\n",
    "                #targets[weather_label_map[weather]] = 1\n",
    "                img = cv2.resize(img, (224, 224))\n",
    "                num_rows, num_cols = img.shape[:2]\n",
    "                \n",
    "                rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), random.choice([0, 90, 180, 270]), 1)\n",
    "                img = cv2.warpAffine(img, rotation_matrix, (num_cols, num_rows))\n",
    "                \n",
    "                if random.choice([0, 1]) == 1:\n",
    "                    img = cv2.flip( img, 0  )\n",
    "\n",
    "                #1\n",
    "                x_train.append(img)\n",
    "                y_train.append(targets)  \n",
    "            \n",
    "            \n",
    "            y_train = np.array(y_train, np.uint8)\n",
    "            x_train = np.array(x_train, np.float16) / 255.0\n",
    "            yield (x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def val_generator(tr, batch_size, seed=0, shuf=1):\n",
    "    while(1):\n",
    "        if shuf==1:\n",
    "            shuffle(tr)\n",
    "        batches = []\n",
    "        cnt = 0\n",
    "        tmp_arr = []\n",
    "        for i in tr:\n",
    "            tmp_arr += [i]\n",
    "            if cnt % batch_size == 0:\n",
    "                batches += [tmp_arr]\n",
    "                tmp_arr = []\n",
    "            cnt += 1\n",
    "        batches += [tmp_arr]\n",
    "        \n",
    "        for i in batches:\n",
    "                \n",
    "            x_train = []\n",
    "            y_train = []\n",
    "\n",
    "            for f, tags, weather in df_train.ix[i, :].values:\n",
    "                img = cv2.imread('data/train-jpg/{}.jpg'.format(f))\n",
    "                targets = np.zeros(17)\n",
    "                for t in tags.split(' '):\n",
    "                    targets[label_map[t]] = 1\n",
    "                #targets[weather_label_map[weather]] = 1\n",
    "                img = cv2.resize(img, (224, 224))\n",
    "                num_rows, num_cols = img.shape[:2]\n",
    "                \n",
    "                rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), (seed % 4)*90, 1)\n",
    "                img = cv2.warpAffine(img, rotation_matrix, (num_cols, num_rows))\n",
    "                \n",
    "                if (seed % 2 == 1):\n",
    "                    img = cv2.flip( img, 0  )\n",
    "                #1\n",
    "                x_train.append(img)\n",
    "                y_train.append(targets)  \n",
    "\n",
    "            \n",
    "            y_train = np.array(y_train, np.uint8)\n",
    "            x_train = np.array(x_train, np.float16) / 255.0\n",
    "            yield (x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(loss='binary_crossentropy', # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('VGG19.h5', save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40239\n",
      "\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/Keras-1.2.2-py3.4.egg/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256s - loss: 0.4745 - acc: 0.7925 - val_loss: 0.3145 - val_acc: 0.9052\n",
      "Epoch 2/40\n",
      "100\n",
      "150\n",
      "254s - loss: 0.3180 - acc: 0.8986 - val_loss: 0.2784 - val_acc: 0.9058\n",
      "Epoch 3/40\n",
      "200\n",
      "239\n",
      "254s - loss: 0.2873 - acc: 0.9047 - val_loss: 0.2682 - val_acc: 0.9049\n",
      "Epoch 4/40\n",
      "50\n",
      "256s - loss: 0.2774 - acc: 0.9047 - val_loss: 0.2612 - val_acc: 0.9048\n",
      "Epoch 5/40\n",
      "100\n",
      "150\n",
      "254s - loss: 0.2697 - acc: 0.9057 - val_loss: 0.2582 - val_acc: 0.9060\n",
      "Epoch 6/40\n",
      "200\n",
      "239\n",
      "254s - loss: 0.2673 - acc: 0.9048 - val_loss: 0.2551 - val_acc: 0.9056\n",
      "Epoch 7/40\n",
      "50\n",
      "253s - loss: 0.2647 - acc: 0.9049 - val_loss: 0.2529 - val_acc: 0.9052\n",
      "Epoch 8/40\n",
      "100\n",
      "150\n",
      "254s - loss: 0.2607 - acc: 0.9057 - val_loss: 0.2507 - val_acc: 0.9062\n",
      "Epoch 9/40\n",
      "200\n",
      "239\n",
      "254s - loss: 0.2567 - acc: 0.9064 - val_loss: 0.2485 - val_acc: 0.9064\n",
      "Epoch 10/40\n",
      "50\n",
      "253s - loss: 0.2570 - acc: 0.9051 - val_loss: 0.2498 - val_acc: 0.9052\n",
      "Epoch 11/40\n",
      "100\n",
      "150\n",
      "255s - loss: 0.2573 - acc: 0.9052 - val_loss: 0.2471 - val_acc: 0.9057\n",
      "Epoch 12/40\n",
      "200\n",
      "239\n",
      "254s - loss: 0.2497 - acc: 0.9076 - val_loss: 0.2474 - val_acc: 0.9054\n",
      "Epoch 13/40\n",
      "50\n",
      "253s - loss: 0.2519 - acc: 0.9069 - val_loss: 0.2449 - val_acc: 0.9067\n",
      "Epoch 14/40\n",
      "100\n",
      "150\n",
      "255s - loss: 0.2519 - acc: 0.9059 - val_loss: 0.2463 - val_acc: 0.9056\n",
      "Epoch 15/40\n",
      "200\n",
      "239\n",
      "255s - loss: 0.2500 - acc: 0.9066 - val_loss: 0.2416 - val_acc: 0.9078\n",
      "Epoch 16/40\n",
      "50\n",
      "253s - loss: 0.2499 - acc: 0.9063 - val_loss: 0.2451 - val_acc: 0.9051\n",
      "Epoch 17/40\n",
      "100\n",
      "150\n",
      "254s - loss: 0.2488 - acc: 0.9069 - val_loss: 0.2436 - val_acc: 0.9061\n",
      "Epoch 18/40\n",
      "200\n",
      "239\n",
      "255s - loss: 0.2459 - acc: 0.9071 - val_loss: 0.2403 - val_acc: 0.9072\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-d8d9c5047859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     callbacks=[model_checkpoint])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/Keras-1.2.2-py3.4.egg/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1555\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1556\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/Keras-1.2.2-py3.4.egg/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/Keras-1.2.2-py3.4.egg/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1941\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1943\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1944\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit_generator(all_generator(folds[3][0], batch_size),\n",
    "                    samples_per_epoch=10000,\n",
    "                    validation_data=val_generator(folds[3][1], batch_size),\n",
    "                    nb_val_samples=5000,\n",
    "                    nb_epoch=40,\n",
    "                    verbose=2,\n",
    "                    callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('resnet152_stage_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for layer in model.layers[:500]:\n",
    "#    layer.trainable = False\n",
    "for layer in model.layers[:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "adam = keras.optimizers.Adam(lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.000001, momentum=0.0, decay=0.0, nesterov=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='binary_crossentropy', # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model_checkpoint = ModelCheckpoint('VGG19.h5', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "476\n",
      "50\n",
      "100\n",
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/Keras-1.2.2-py3.4.egg/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "667s - loss: 0.1890 - acc: 0.9268 - val_loss: 0.1442 - val_acc: 0.9437\n",
      "Epoch 2/60\n",
      "200\n",
      "250\n",
      "300\n",
      "666s - loss: 0.1464 - acc: 0.9441 - val_loss: 0.1311 - val_acc: 0.9504\n",
      "Epoch 3/60\n",
      "350\n",
      "400\n",
      "450\n",
      "476\n",
      "666s - loss: 0.1281 - acc: 0.9523 - val_loss: 0.1286 - val_acc: 0.9492\n",
      "Epoch 4/60\n",
      "50\n",
      "100\n",
      "150\n",
      "668s - loss: 0.1354 - acc: 0.9494 - val_loss: 0.1225 - val_acc: 0.9546\n",
      "Epoch 5/60\n",
      "200\n",
      "250\n",
      "300\n",
      "665s - loss: 0.1203 - acc: 0.9555 - val_loss: 0.1238 - val_acc: 0.9530\n",
      "Epoch 6/60\n",
      "350\n",
      "400\n",
      "450\n",
      "476\n",
      "666s - loss: 0.1168 - acc: 0.9564 - val_loss: 0.1131 - val_acc: 0.9581\n",
      "Epoch 7/60\n",
      "50\n",
      "100\n",
      "150\n",
      "663s - loss: 0.1150 - acc: 0.9574 - val_loss: 0.1047 - val_acc: 0.9607\n",
      "Epoch 8/60\n",
      "200\n",
      "250\n",
      "300\n",
      "665s - loss: 0.1089 - acc: 0.9596 - val_loss: 0.1097 - val_acc: 0.9583\n",
      "Epoch 9/60\n",
      "350\n",
      "400\n",
      "450\n",
      "664s - loss: 0.1110 - acc: 0.9594 - val_loss: 0.1098 - val_acc: 0.9577\n",
      "Epoch 10/60\n",
      "476\n",
      "50\n",
      "100\n",
      "150\n",
      "663s - loss: 0.1068 - acc: 0.9607 - val_loss: 0.1035 - val_acc: 0.9608\n",
      "Epoch 11/60\n",
      "200\n",
      "250\n",
      "300\n",
      "664s - loss: 0.1063 - acc: 0.9602 - val_loss: 0.1041 - val_acc: 0.9610\n",
      "Epoch 12/60\n",
      "350\n",
      "400\n",
      "450\n",
      "664s - loss: 0.1061 - acc: 0.9606 - val_loss: 0.1045 - val_acc: 0.9604\n",
      "Epoch 13/60\n",
      "476\n",
      "50\n",
      "100\n",
      "150\n",
      "663s - loss: 0.1074 - acc: 0.9601 - val_loss: 0.1025 - val_acc: 0.9610\n",
      "Epoch 14/60\n",
      "200\n",
      "250\n",
      "300\n",
      "664s - loss: 0.1035 - acc: 0.9611 - val_loss: 0.0992 - val_acc: 0.9621\n",
      "Epoch 15/60\n",
      "350\n",
      "400\n",
      "450\n",
      "665s - loss: 0.1023 - acc: 0.9619 - val_loss: 0.0978 - val_acc: 0.9628\n",
      "Epoch 16/60\n",
      "476\n",
      "50\n",
      "100\n",
      "663s - loss: 0.1016 - acc: 0.9625 - val_loss: 0.1001 - val_acc: 0.9627\n",
      "Epoch 17/60\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "665s - loss: 0.1010 - acc: 0.9624 - val_loss: 0.1024 - val_acc: 0.9612\n",
      "Epoch 18/60\n",
      "350\n",
      "400\n",
      "450\n",
      "665s - loss: 0.1028 - acc: 0.9616 - val_loss: 0.0982 - val_acc: 0.9631\n",
      "Epoch 19/60\n",
      "476\n",
      "50\n",
      "100\n",
      "663s - loss: 0.1024 - acc: 0.9618 - val_loss: 0.0994 - val_acc: 0.9621\n",
      "Epoch 20/60\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "666s - loss: 0.0997 - acc: 0.9624 - val_loss: 0.0974 - val_acc: 0.9640\n",
      "Epoch 21/60\n",
      "350\n",
      "400\n",
      "450\n",
      "665s - loss: 0.1001 - acc: 0.9625 - val_loss: 0.0981 - val_acc: 0.9639\n",
      "Epoch 22/60\n",
      "476\n",
      "50\n",
      "100\n",
      "664s - loss: 0.0971 - acc: 0.9640 - val_loss: 0.0935 - val_acc: 0.9638\n",
      "Epoch 23/60\n",
      "150\n",
      "200\n",
      "250\n",
      "665s - loss: 0.0982 - acc: 0.9632 - val_loss: 0.0930 - val_acc: 0.9639\n",
      "Epoch 24/60\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "665s - loss: 0.0970 - acc: 0.9637 - val_loss: 0.0933 - val_acc: 0.9643\n",
      "Epoch 25/60\n",
      "476\n",
      "50\n",
      "100\n",
      "663s - loss: 0.0965 - acc: 0.9637 - val_loss: 0.0951 - val_acc: 0.9637\n",
      "Epoch 26/60\n",
      "150\n",
      "200\n",
      "250\n",
      "664s - loss: 0.0970 - acc: 0.9635 - val_loss: 0.0945 - val_acc: 0.9644\n",
      "Epoch 27/60\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "665s - loss: 0.0975 - acc: 0.9634 - val_loss: 0.0940 - val_acc: 0.9655\n",
      "Epoch 28/60\n",
      "476\n",
      "50\n",
      "100\n",
      "663s - loss: 0.0947 - acc: 0.9644 - val_loss: 0.0942 - val_acc: 0.9640\n",
      "Epoch 29/60\n",
      "150\n",
      "200\n",
      "250\n",
      "665s - loss: 0.0958 - acc: 0.9645 - val_loss: 0.0943 - val_acc: 0.9633\n",
      "Epoch 30/60\n",
      "300\n",
      "350\n",
      "400\n",
      "664s - loss: 0.0943 - acc: 0.9644 - val_loss: 0.0949 - val_acc: 0.9651\n",
      "Epoch 31/60\n",
      "450\n",
      "476\n",
      "50\n",
      "100\n",
      "663s - loss: 0.0981 - acc: 0.9639 - val_loss: 0.0915 - val_acc: 0.9656\n",
      "Epoch 32/60\n",
      "150\n",
      "200\n",
      "250\n",
      "664s - loss: 0.0921 - acc: 0.9655 - val_loss: 0.0934 - val_acc: 0.9644\n",
      "Epoch 33/60\n",
      "300\n",
      "350\n",
      "400\n",
      "664s - loss: 0.0926 - acc: 0.9653 - val_loss: 0.0929 - val_acc: 0.9651\n",
      "Epoch 34/60\n",
      "450\n",
      "476\n",
      "50\n",
      "100\n",
      "663s - loss: 0.0935 - acc: 0.9646 - val_loss: 0.0954 - val_acc: 0.9633\n",
      "Epoch 35/60\n",
      "150\n",
      "200\n",
      "250\n",
      "664s - loss: 0.0932 - acc: 0.9647 - val_loss: 0.0932 - val_acc: 0.9656\n",
      "Epoch 36/60\n",
      "300\n",
      "350\n",
      "400\n",
      "664s - loss: 0.0929 - acc: 0.9651 - val_loss: 0.0929 - val_acc: 0.9640\n",
      "Epoch 37/60\n",
      "450\n",
      "476\n",
      "50\n",
      "100\n",
      "662s - loss: 0.0924 - acc: 0.9654 - val_loss: 0.0964 - val_acc: 0.9633\n",
      "Epoch 38/60\n",
      "150\n",
      "200\n",
      "250\n",
      "665s - loss: 0.0921 - acc: 0.9654 - val_loss: 0.0893 - val_acc: 0.9663\n",
      "Epoch 39/60\n",
      "300\n",
      "350\n",
      "400\n",
      "664s - loss: 0.0912 - acc: 0.9654 - val_loss: 0.0924 - val_acc: 0.9645\n",
      "Epoch 40/60\n",
      "450\n",
      "476\n",
      "50\n",
      "100\n",
      "662s - loss: 0.0914 - acc: 0.9655 - val_loss: 0.0967 - val_acc: 0.9639\n",
      "Epoch 41/60\n",
      "150\n",
      "200\n",
      "250\n",
      "664s - loss: 0.0891 - acc: 0.9661 - val_loss: 0.0903 - val_acc: 0.9658\n",
      "Epoch 42/60\n",
      "300\n",
      "350\n",
      "400\n",
      "664s - loss: 0.0931 - acc: 0.9653 - val_loss: 0.0922 - val_acc: 0.9652\n",
      "Epoch 43/60\n",
      "450\n",
      "476\n",
      "50\n",
      "100\n",
      "662s - loss: 0.0911 - acc: 0.9655 - val_loss: 0.0912 - val_acc: 0.9658\n",
      "Epoch 44/60\n",
      "150\n",
      "200\n",
      "250\n",
      "664s - loss: 0.0909 - acc: 0.9657 - val_loss: 0.0917 - val_acc: 0.9654\n",
      "Epoch 45/60\n",
      "300\n",
      "350\n",
      "400\n",
      "664s - loss: 0.0884 - acc: 0.9669 - val_loss: 0.0885 - val_acc: 0.9667\n",
      "Epoch 46/60\n",
      "450\n",
      "476\n",
      "50\n",
      "100\n",
      "662s - loss: 0.0905 - acc: 0.9659 - val_loss: 0.0893 - val_acc: 0.9658\n",
      "Epoch 47/60\n",
      "150\n",
      "200\n",
      "250\n",
      "664s - loss: 0.0888 - acc: 0.9664 - val_loss: 0.0909 - val_acc: 0.9650\n",
      "Epoch 48/60\n",
      "300\n",
      "350\n",
      "400\n",
      "664s - loss: 0.0897 - acc: 0.9663 - val_loss: 0.0912 - val_acc: 0.9666\n",
      "Epoch 49/60\n",
      "450\n",
      "476\n",
      "50\n",
      "100\n",
      "662s - loss: 0.0903 - acc: 0.9653 - val_loss: 0.0920 - val_acc: 0.9659\n",
      "Epoch 50/60\n",
      "150\n",
      "200\n",
      "250\n",
      "664s - loss: 0.0864 - acc: 0.9676 - val_loss: 0.0914 - val_acc: 0.9652\n",
      "Epoch 51/60\n",
      "300\n",
      "350\n",
      "400\n",
      "664s - loss: 0.0903 - acc: 0.9661 - val_loss: 0.0894 - val_acc: 0.9662\n",
      "Epoch 52/60\n",
      "450\n",
      "476\n",
      "50\n",
      "100\n",
      "662s - loss: 0.0897 - acc: 0.9660 - val_loss: 0.0894 - val_acc: 0.9667\n",
      "Epoch 53/60\n",
      "150\n",
      "200\n",
      "250\n",
      "664s - loss: 0.0881 - acc: 0.9665 - val_loss: 0.0908 - val_acc: 0.9655\n",
      "Epoch 54/60\n",
      "300\n",
      "350\n",
      "400\n",
      "664s - loss: 0.0872 - acc: 0.9667 - val_loss: 0.0892 - val_acc: 0.9665\n",
      "Epoch 55/60\n",
      "450\n",
      "476\n",
      "50\n",
      "662s - loss: 0.0873 - acc: 0.9673 - val_loss: 0.0900 - val_acc: 0.9657\n",
      "Epoch 56/60\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "664s - loss: 0.0859 - acc: 0.9674 - val_loss: 0.0920 - val_acc: 0.9657\n",
      "Epoch 57/60\n",
      "300\n",
      "350\n",
      "400\n",
      "664s - loss: 0.0870 - acc: 0.9671 - val_loss: 0.0910 - val_acc: 0.9665\n",
      "Epoch 58/60\n",
      "450\n",
      "476\n",
      "50\n",
      "662s - loss: 0.0881 - acc: 0.9666 - val_loss: 0.0887 - val_acc: 0.9665\n",
      "Epoch 59/60\n",
      "100\n",
      "150\n",
      "200\n",
      "664s - loss: 0.0845 - acc: 0.9676 - val_loss: 0.0910 - val_acc: 0.9658\n",
      "Epoch 60/60\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "664s - loss: 0.0875 - acc: 0.9668 - val_loss: 0.0907 - val_acc: 0.9658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7888babc18>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit_generator(all_generator(folds[3][0], batch_size),\n",
    "                    samples_per_epoch=10000,\n",
    "                    validation_data=val_generator(folds[3][1], batch_size),\n",
    "                    nb_val_samples=5000,\n",
    "                    nb_epoch=60,\n",
    "                    verbose=2,\n",
    "                    callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('fold4/VGG19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "val_pred = model.predict_generator(val_generator(folds[3][1], batch_size, shuf=0, seed=0), val_samples = len(folds[3][1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pred = val_pred * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "for i in range(8):\n",
    "    val_pred_tmp = model.predict_generator(val_generator(folds[3][1], batch_size, shuf=0, seed=i), val_samples = len(folds[3][1]) )\n",
    "    val_pred = val_pred + val_pred_tmp\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pred = val_pred / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pred = pd.DataFrame(val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pred['id'] = folds[3][1]\n",
    "val_pred = val_pred.sort_values('id')\n",
    "del val_pred['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pred.to_csv(\"fold4/train.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v=pd.read_csv('fold4/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13494"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13494"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_all = []\n",
    "for f, tags, weather in df_train.values:\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1\n",
    "    y_all += [targets]\n",
    "y_all = pd.DataFrame(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40479, 17)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10122"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "y_val = y_all.ix[folds[0][1].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.923640800553\n"
     ]
    }
   ],
   "source": [
    "          \n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "#print(y_valid)\n",
    "#print(p_valid)\n",
    "print(fbeta_score(y_val, np.array(val_pred) > 0.2, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.922365582803\n",
    "0.917041863094\n",
    "0.913685914092\n",
    "0.899545374775\n",
    "0.89285374022\n",
    "0.892722591666\n",
    "0.889811292029\n",
    "0.887170607068\n",
    "0.863779970799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61191"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61191"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61191"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(range(61191))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_generator(tr, batch_size, seed=0):\n",
    "    while(1):\n",
    "        batches = []\n",
    "        cnt = 0\n",
    "        tmp_arr = []\n",
    "        for i in tr:\n",
    "            tmp_arr += [i]\n",
    "            if cnt % batch_size == 0:\n",
    "                batches += [tmp_arr]\n",
    "                tmp_arr = []\n",
    "            cnt += 1\n",
    "        batches += [tmp_arr]\n",
    "        \n",
    "        for i in batches:\n",
    "                \n",
    "            x_test = []\n",
    "\n",
    "            for f in sample_submission.ix[i, 'image_name'].values:\n",
    "                img = cv2.imread('data/test-jpg/{}.jpg'.format(f))\n",
    "                img = cv2.resize(img, (224, 224))\n",
    "                num_rows, num_cols = img.shape[:2]\n",
    "                \n",
    "                rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), (seed % 4)*90, 1)\n",
    "                img = cv2.warpAffine(img, rotation_matrix, (num_cols, num_rows))\n",
    "                \n",
    "                if (seed % 2 == 1):\n",
    "                    img = cv2.flip( img, 0  )\n",
    "                #1\n",
    "                x_test.append(img)\n",
    "\n",
    "            \n",
    "            x_test = np.array(x_test, np.float16) / 255.0\n",
    "            yield x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "test_pred = model.predict_generator(test_generator(range(61191), batch_size, seed=0), val_samples = 61191)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_pred = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_test = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5924"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "for i in range(1,8):\n",
    "    test_pred = model.predict_generator(test_generator(range(61191), batch_size, seed=i), val_samples = 61191 )\n",
    "    result_pred = test_pred + result_pred\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_pred = result_pred / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_pred = pd.DataFrame(result_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_pred.to_csv(\"fold4/test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999967</td>\n",
       "      <td>3.106106e-06</td>\n",
       "      <td>1.177439e-06</td>\n",
       "      <td>5.212919e-03</td>\n",
       "      <td>1.022755e-06</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>5.062488e-04</td>\n",
       "      <td>2.432009e-04</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.003297</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>3.407740e-05</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.997537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999939</td>\n",
       "      <td>5.980464e-05</td>\n",
       "      <td>2.818454e-06</td>\n",
       "      <td>3.326960e-02</td>\n",
       "      <td>2.187197e-05</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>7.766142e-03</td>\n",
       "      <td>3.785155e-03</td>\n",
       "      <td>0.012106</td>\n",
       "      <td>0.016742</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>6.262326e-04</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.998239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.998018</td>\n",
       "      <td>3.454607e-04</td>\n",
       "      <td>2.046244e-03</td>\n",
       "      <td>8.044354e-04</td>\n",
       "      <td>2.254558e-04</td>\n",
       "      <td>0.999746</td>\n",
       "      <td>0.010043</td>\n",
       "      <td>0.035536</td>\n",
       "      <td>5.562823e-04</td>\n",
       "      <td>9.467581e-04</td>\n",
       "      <td>0.017220</td>\n",
       "      <td>0.040024</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.001847</td>\n",
       "      <td>4.917934e-04</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999958</td>\n",
       "      <td>2.117519e-05</td>\n",
       "      <td>2.232488e-06</td>\n",
       "      <td>6.061943e-03</td>\n",
       "      <td>5.304804e-06</td>\n",
       "      <td>0.005796</td>\n",
       "      <td>0.003354</td>\n",
       "      <td>0.005371</td>\n",
       "      <td>1.944432e-03</td>\n",
       "      <td>1.700670e-03</td>\n",
       "      <td>0.134701</td>\n",
       "      <td>0.142489</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>1.017280e-03</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.992230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.361808</td>\n",
       "      <td>5.791064e-06</td>\n",
       "      <td>6.839007e-01</td>\n",
       "      <td>1.370201e-05</td>\n",
       "      <td>1.597645e-06</td>\n",
       "      <td>0.393776</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.015132</td>\n",
       "      <td>7.307442e-06</td>\n",
       "      <td>9.923245e-06</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>1.202656e-05</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.002534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.999986</td>\n",
       "      <td>1.538210e-06</td>\n",
       "      <td>3.735647e-07</td>\n",
       "      <td>9.140233e-04</td>\n",
       "      <td>5.873340e-07</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>1.840009e-04</td>\n",
       "      <td>1.301600e-04</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>2.120440e-05</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.999115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.999282</td>\n",
       "      <td>4.499687e-04</td>\n",
       "      <td>3.573338e-06</td>\n",
       "      <td>3.987618e-04</td>\n",
       "      <td>5.717833e-04</td>\n",
       "      <td>0.329551</td>\n",
       "      <td>0.246420</td>\n",
       "      <td>0.131417</td>\n",
       "      <td>1.550824e-03</td>\n",
       "      <td>3.542325e-04</td>\n",
       "      <td>0.426516</td>\n",
       "      <td>0.857581</td>\n",
       "      <td>0.033066</td>\n",
       "      <td>0.996704</td>\n",
       "      <td>1.001025e-02</td>\n",
       "      <td>0.008540</td>\n",
       "      <td>0.441267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.678870</td>\n",
       "      <td>8.134326e-04</td>\n",
       "      <td>2.296823e-04</td>\n",
       "      <td>8.716126e-05</td>\n",
       "      <td>2.005903e-03</td>\n",
       "      <td>0.014306</td>\n",
       "      <td>0.964268</td>\n",
       "      <td>0.106803</td>\n",
       "      <td>5.682569e-04</td>\n",
       "      <td>8.303207e-05</td>\n",
       "      <td>0.018326</td>\n",
       "      <td>0.343445</td>\n",
       "      <td>0.015184</td>\n",
       "      <td>0.984827</td>\n",
       "      <td>9.150566e-04</td>\n",
       "      <td>0.034154</td>\n",
       "      <td>0.954906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.999983</td>\n",
       "      <td>1.370272e-06</td>\n",
       "      <td>5.577581e-07</td>\n",
       "      <td>4.432974e-04</td>\n",
       "      <td>5.317090e-07</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>1.112486e-04</td>\n",
       "      <td>9.040999e-05</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>1.716028e-05</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.998565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.994316</td>\n",
       "      <td>1.598539e-04</td>\n",
       "      <td>2.877838e-03</td>\n",
       "      <td>3.793644e-04</td>\n",
       "      <td>1.581127e-04</td>\n",
       "      <td>0.030063</td>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.064213</td>\n",
       "      <td>6.814570e-04</td>\n",
       "      <td>1.514508e-03</td>\n",
       "      <td>0.162331</td>\n",
       "      <td>0.859225</td>\n",
       "      <td>0.585079</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>2.259216e-03</td>\n",
       "      <td>0.011323</td>\n",
       "      <td>0.351896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.999875</td>\n",
       "      <td>7.856272e-05</td>\n",
       "      <td>1.869810e-05</td>\n",
       "      <td>3.476651e-04</td>\n",
       "      <td>7.360628e-05</td>\n",
       "      <td>0.998228</td>\n",
       "      <td>0.014126</td>\n",
       "      <td>0.226993</td>\n",
       "      <td>4.015515e-04</td>\n",
       "      <td>2.815766e-04</td>\n",
       "      <td>0.009439</td>\n",
       "      <td>0.037495</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>2.630966e-04</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.002853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.999856</td>\n",
       "      <td>3.351516e-03</td>\n",
       "      <td>1.059800e-05</td>\n",
       "      <td>1.490773e-02</td>\n",
       "      <td>2.558730e-04</td>\n",
       "      <td>0.003413</td>\n",
       "      <td>0.026095</td>\n",
       "      <td>0.028235</td>\n",
       "      <td>1.691920e-02</td>\n",
       "      <td>2.690311e-02</td>\n",
       "      <td>0.723598</td>\n",
       "      <td>0.639615</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.031093</td>\n",
       "      <td>1.062757e-01</td>\n",
       "      <td>0.072501</td>\n",
       "      <td>0.993080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.071089</td>\n",
       "      <td>2.699767e-07</td>\n",
       "      <td>9.425774e-01</td>\n",
       "      <td>9.006841e-07</td>\n",
       "      <td>8.086721e-08</td>\n",
       "      <td>0.016443</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.012944</td>\n",
       "      <td>8.185493e-07</td>\n",
       "      <td>4.367851e-07</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.004365</td>\n",
       "      <td>0.025432</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>7.373139e-07</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.001966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.999877</td>\n",
       "      <td>3.688276e-05</td>\n",
       "      <td>6.145753e-06</td>\n",
       "      <td>6.086219e-04</td>\n",
       "      <td>3.444354e-05</td>\n",
       "      <td>0.005393</td>\n",
       "      <td>0.077665</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>6.819579e-04</td>\n",
       "      <td>7.383208e-04</td>\n",
       "      <td>0.112009</td>\n",
       "      <td>0.949799</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>1.976188e-03</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.994493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.999781</td>\n",
       "      <td>8.120236e-05</td>\n",
       "      <td>2.912215e-05</td>\n",
       "      <td>2.784266e-03</td>\n",
       "      <td>2.156365e-05</td>\n",
       "      <td>0.007024</td>\n",
       "      <td>0.009825</td>\n",
       "      <td>0.016927</td>\n",
       "      <td>2.001904e-03</td>\n",
       "      <td>9.224213e-04</td>\n",
       "      <td>0.362450</td>\n",
       "      <td>0.319400</td>\n",
       "      <td>0.033212</td>\n",
       "      <td>0.004687</td>\n",
       "      <td>2.368983e-03</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>0.953801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.999972</td>\n",
       "      <td>4.916564e-06</td>\n",
       "      <td>1.074837e-06</td>\n",
       "      <td>1.048673e-03</td>\n",
       "      <td>1.982548e-06</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>3.110443e-04</td>\n",
       "      <td>2.571787e-04</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.005889</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>5.542183e-05</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.998570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.998389</td>\n",
       "      <td>1.431545e-04</td>\n",
       "      <td>1.019307e-06</td>\n",
       "      <td>9.271722e-05</td>\n",
       "      <td>1.547988e-04</td>\n",
       "      <td>0.006230</td>\n",
       "      <td>0.745872</td>\n",
       "      <td>0.027492</td>\n",
       "      <td>6.349873e-04</td>\n",
       "      <td>4.931545e-04</td>\n",
       "      <td>0.247159</td>\n",
       "      <td>0.977237</td>\n",
       "      <td>0.003188</td>\n",
       "      <td>0.983720</td>\n",
       "      <td>6.684822e-03</td>\n",
       "      <td>0.006599</td>\n",
       "      <td>0.991455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.999973</td>\n",
       "      <td>6.712968e-06</td>\n",
       "      <td>2.036089e-06</td>\n",
       "      <td>5.739804e-04</td>\n",
       "      <td>4.693825e-06</td>\n",
       "      <td>0.750200</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>2.296602e-04</td>\n",
       "      <td>2.490476e-04</td>\n",
       "      <td>0.003674</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>5.306752e-05</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.254008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.999078</td>\n",
       "      <td>7.190248e-06</td>\n",
       "      <td>2.670492e-04</td>\n",
       "      <td>1.654696e-04</td>\n",
       "      <td>3.692644e-06</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.006873</td>\n",
       "      <td>9.165070e-05</td>\n",
       "      <td>1.092949e-04</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.009436</td>\n",
       "      <td>0.331572</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>3.385883e-05</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.695930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.999831</td>\n",
       "      <td>1.305850e-03</td>\n",
       "      <td>8.036317e-06</td>\n",
       "      <td>1.106443e-02</td>\n",
       "      <td>7.855866e-04</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.606888</td>\n",
       "      <td>0.747794</td>\n",
       "      <td>5.822596e-02</td>\n",
       "      <td>5.557079e-04</td>\n",
       "      <td>0.026586</td>\n",
       "      <td>0.053721</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>1.398870e-03</td>\n",
       "      <td>0.005113</td>\n",
       "      <td>0.995589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.999846</td>\n",
       "      <td>6.878508e-04</td>\n",
       "      <td>1.089343e-05</td>\n",
       "      <td>1.979547e-03</td>\n",
       "      <td>2.119597e-04</td>\n",
       "      <td>0.004259</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.958299</td>\n",
       "      <td>2.609550e-03</td>\n",
       "      <td>1.831324e-04</td>\n",
       "      <td>0.047763</td>\n",
       "      <td>0.312155</td>\n",
       "      <td>0.004559</td>\n",
       "      <td>0.015489</td>\n",
       "      <td>2.239043e-03</td>\n",
       "      <td>0.008556</td>\n",
       "      <td>0.992417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.998077</td>\n",
       "      <td>1.052158e-03</td>\n",
       "      <td>9.426346e-05</td>\n",
       "      <td>1.408917e-03</td>\n",
       "      <td>2.544636e-04</td>\n",
       "      <td>0.008226</td>\n",
       "      <td>0.151411</td>\n",
       "      <td>0.985162</td>\n",
       "      <td>1.088353e-03</td>\n",
       "      <td>6.360449e-05</td>\n",
       "      <td>0.037397</td>\n",
       "      <td>0.214192</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>0.086244</td>\n",
       "      <td>1.958112e-03</td>\n",
       "      <td>0.012831</td>\n",
       "      <td>0.989474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.994631</td>\n",
       "      <td>2.305544e-04</td>\n",
       "      <td>5.538400e-05</td>\n",
       "      <td>1.949885e-04</td>\n",
       "      <td>3.164085e-04</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>0.959490</td>\n",
       "      <td>0.059682</td>\n",
       "      <td>1.153717e-03</td>\n",
       "      <td>2.721530e-04</td>\n",
       "      <td>0.283476</td>\n",
       "      <td>0.993659</td>\n",
       "      <td>0.022695</td>\n",
       "      <td>0.526379</td>\n",
       "      <td>5.850964e-03</td>\n",
       "      <td>0.006570</td>\n",
       "      <td>0.958373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.021969</td>\n",
       "      <td>4.716390e-09</td>\n",
       "      <td>9.808469e-01</td>\n",
       "      <td>2.111699e-08</td>\n",
       "      <td>1.089209e-09</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>2.171473e-08</td>\n",
       "      <td>1.064819e-08</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.870447e-08</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.969079</td>\n",
       "      <td>1.603338e-04</td>\n",
       "      <td>3.165174e-03</td>\n",
       "      <td>8.604564e-05</td>\n",
       "      <td>1.371231e-03</td>\n",
       "      <td>0.999919</td>\n",
       "      <td>0.217248</td>\n",
       "      <td>0.099341</td>\n",
       "      <td>1.714195e-04</td>\n",
       "      <td>3.530525e-04</td>\n",
       "      <td>0.036433</td>\n",
       "      <td>0.930320</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.009238</td>\n",
       "      <td>1.330758e-03</td>\n",
       "      <td>0.006304</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.999985</td>\n",
       "      <td>2.267770e-06</td>\n",
       "      <td>4.244049e-07</td>\n",
       "      <td>1.508066e-03</td>\n",
       "      <td>8.666124e-07</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>2.816993e-04</td>\n",
       "      <td>1.829466e-04</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>3.081585e-05</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.999146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.995655</td>\n",
       "      <td>1.660612e-05</td>\n",
       "      <td>1.089485e-05</td>\n",
       "      <td>5.386593e-06</td>\n",
       "      <td>1.803086e-04</td>\n",
       "      <td>0.997544</td>\n",
       "      <td>0.999364</td>\n",
       "      <td>0.059502</td>\n",
       "      <td>1.873008e-04</td>\n",
       "      <td>9.176655e-06</td>\n",
       "      <td>0.100627</td>\n",
       "      <td>0.998466</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.259698</td>\n",
       "      <td>4.702270e-04</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.003156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.996487</td>\n",
       "      <td>3.592463e-04</td>\n",
       "      <td>1.731960e-04</td>\n",
       "      <td>7.719333e-04</td>\n",
       "      <td>2.240448e-04</td>\n",
       "      <td>0.012767</td>\n",
       "      <td>0.073044</td>\n",
       "      <td>0.023440</td>\n",
       "      <td>8.514746e-04</td>\n",
       "      <td>7.766811e-03</td>\n",
       "      <td>0.146180</td>\n",
       "      <td>0.941820</td>\n",
       "      <td>0.019440</td>\n",
       "      <td>0.022930</td>\n",
       "      <td>1.044770e-02</td>\n",
       "      <td>0.035658</td>\n",
       "      <td>0.964695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.971445</td>\n",
       "      <td>1.258493e-04</td>\n",
       "      <td>4.012210e-04</td>\n",
       "      <td>7.173859e-05</td>\n",
       "      <td>1.587783e-03</td>\n",
       "      <td>0.986829</td>\n",
       "      <td>0.904923</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>3.099827e-04</td>\n",
       "      <td>1.567295e-04</td>\n",
       "      <td>0.082466</td>\n",
       "      <td>0.986966</td>\n",
       "      <td>0.005462</td>\n",
       "      <td>0.048993</td>\n",
       "      <td>1.451951e-03</td>\n",
       "      <td>0.007816</td>\n",
       "      <td>0.009621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.999985</td>\n",
       "      <td>1.165287e-06</td>\n",
       "      <td>4.801989e-07</td>\n",
       "      <td>3.676045e-04</td>\n",
       "      <td>4.549279e-07</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>9.727140e-05</td>\n",
       "      <td>7.996677e-05</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>1.507324e-05</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.998711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61161</th>\n",
       "      <td>0.999957</td>\n",
       "      <td>3.208696e-05</td>\n",
       "      <td>1.716698e-06</td>\n",
       "      <td>3.843901e-03</td>\n",
       "      <td>8.038573e-06</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.004519</td>\n",
       "      <td>0.006189</td>\n",
       "      <td>1.801712e-03</td>\n",
       "      <td>1.688680e-03</td>\n",
       "      <td>0.098412</td>\n",
       "      <td>0.103631</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>1.028786e-03</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>0.997551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61162</th>\n",
       "      <td>0.998761</td>\n",
       "      <td>6.489002e-05</td>\n",
       "      <td>4.828183e-06</td>\n",
       "      <td>6.150446e-05</td>\n",
       "      <td>7.868394e-05</td>\n",
       "      <td>0.011445</td>\n",
       "      <td>0.916545</td>\n",
       "      <td>0.030340</td>\n",
       "      <td>3.524499e-04</td>\n",
       "      <td>1.456435e-04</td>\n",
       "      <td>0.338170</td>\n",
       "      <td>0.996945</td>\n",
       "      <td>0.011069</td>\n",
       "      <td>0.868200</td>\n",
       "      <td>4.964011e-03</td>\n",
       "      <td>0.003803</td>\n",
       "      <td>0.976865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61163</th>\n",
       "      <td>0.999300</td>\n",
       "      <td>1.075226e-04</td>\n",
       "      <td>1.411179e-04</td>\n",
       "      <td>6.423120e-03</td>\n",
       "      <td>5.500939e-05</td>\n",
       "      <td>0.058443</td>\n",
       "      <td>0.005270</td>\n",
       "      <td>0.008186</td>\n",
       "      <td>1.648727e-03</td>\n",
       "      <td>3.262760e-03</td>\n",
       "      <td>0.016991</td>\n",
       "      <td>0.034488</td>\n",
       "      <td>0.010021</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>7.061257e-04</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.913346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61164</th>\n",
       "      <td>0.999938</td>\n",
       "      <td>1.030854e-05</td>\n",
       "      <td>6.149295e-06</td>\n",
       "      <td>8.189619e-04</td>\n",
       "      <td>7.154222e-06</td>\n",
       "      <td>0.289111</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.003376</td>\n",
       "      <td>2.792257e-04</td>\n",
       "      <td>3.606089e-04</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>8.420820e-05</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.726520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61165</th>\n",
       "      <td>0.999659</td>\n",
       "      <td>7.048241e-04</td>\n",
       "      <td>2.068624e-05</td>\n",
       "      <td>1.688479e-02</td>\n",
       "      <td>1.161414e-04</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.022315</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>1.177194e-02</td>\n",
       "      <td>2.090881e-02</td>\n",
       "      <td>0.671940</td>\n",
       "      <td>0.712828</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>0.026375</td>\n",
       "      <td>5.107237e-02</td>\n",
       "      <td>0.029285</td>\n",
       "      <td>0.990935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61166</th>\n",
       "      <td>0.999985</td>\n",
       "      <td>1.322739e-06</td>\n",
       "      <td>4.254490e-07</td>\n",
       "      <td>5.218388e-04</td>\n",
       "      <td>5.181643e-07</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>1.246576e-04</td>\n",
       "      <td>1.014852e-04</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>1.802866e-05</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.998980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61167</th>\n",
       "      <td>0.999901</td>\n",
       "      <td>1.098297e-04</td>\n",
       "      <td>5.610090e-05</td>\n",
       "      <td>5.680132e-04</td>\n",
       "      <td>1.054761e-04</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.006014</td>\n",
       "      <td>0.016558</td>\n",
       "      <td>5.084062e-04</td>\n",
       "      <td>7.975284e-04</td>\n",
       "      <td>0.011210</td>\n",
       "      <td>0.025841</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>2.385268e-04</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61168</th>\n",
       "      <td>0.999981</td>\n",
       "      <td>3.058670e-06</td>\n",
       "      <td>5.650337e-07</td>\n",
       "      <td>2.761142e-03</td>\n",
       "      <td>1.104483e-06</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>4.334358e-04</td>\n",
       "      <td>2.546448e-04</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.003725</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>3.835941e-05</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.998837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61169</th>\n",
       "      <td>0.999900</td>\n",
       "      <td>2.509452e-05</td>\n",
       "      <td>6.233103e-06</td>\n",
       "      <td>9.495432e-03</td>\n",
       "      <td>9.660926e-06</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>1.582034e-03</td>\n",
       "      <td>1.392484e-03</td>\n",
       "      <td>0.008860</td>\n",
       "      <td>0.016998</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>2.774077e-04</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.995234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61170</th>\n",
       "      <td>0.999981</td>\n",
       "      <td>2.290810e-06</td>\n",
       "      <td>5.720761e-07</td>\n",
       "      <td>1.240500e-03</td>\n",
       "      <td>8.849601e-07</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>2.496493e-04</td>\n",
       "      <td>1.611552e-04</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.003506</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>2.936473e-05</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.998770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61171</th>\n",
       "      <td>0.999374</td>\n",
       "      <td>1.628796e-04</td>\n",
       "      <td>2.865769e-05</td>\n",
       "      <td>9.291368e-04</td>\n",
       "      <td>6.934589e-05</td>\n",
       "      <td>0.008868</td>\n",
       "      <td>0.068133</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>9.157860e-04</td>\n",
       "      <td>5.410908e-03</td>\n",
       "      <td>0.308702</td>\n",
       "      <td>0.979834</td>\n",
       "      <td>0.005015</td>\n",
       "      <td>0.031144</td>\n",
       "      <td>1.588224e-02</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>0.985425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61172</th>\n",
       "      <td>0.996569</td>\n",
       "      <td>2.368998e-03</td>\n",
       "      <td>8.984202e-05</td>\n",
       "      <td>5.827612e-03</td>\n",
       "      <td>1.856989e-03</td>\n",
       "      <td>0.023427</td>\n",
       "      <td>0.971727</td>\n",
       "      <td>0.131306</td>\n",
       "      <td>2.410950e-02</td>\n",
       "      <td>2.993789e-03</td>\n",
       "      <td>0.225830</td>\n",
       "      <td>0.785463</td>\n",
       "      <td>0.006503</td>\n",
       "      <td>0.249677</td>\n",
       "      <td>1.062861e-02</td>\n",
       "      <td>0.028026</td>\n",
       "      <td>0.976465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61173</th>\n",
       "      <td>0.999953</td>\n",
       "      <td>4.500354e-05</td>\n",
       "      <td>1.629930e-06</td>\n",
       "      <td>3.804479e-02</td>\n",
       "      <td>1.473421e-05</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>7.313328e-03</td>\n",
       "      <td>3.750455e-03</td>\n",
       "      <td>0.009671</td>\n",
       "      <td>0.013727</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.001915</td>\n",
       "      <td>4.221095e-04</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.998629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61174</th>\n",
       "      <td>0.999985</td>\n",
       "      <td>1.364887e-06</td>\n",
       "      <td>4.228582e-07</td>\n",
       "      <td>5.108836e-04</td>\n",
       "      <td>5.387782e-07</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>1.255990e-04</td>\n",
       "      <td>1.025489e-04</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.003076</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>1.850787e-05</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.999006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61175</th>\n",
       "      <td>0.999984</td>\n",
       "      <td>1.252274e-06</td>\n",
       "      <td>4.985066e-07</td>\n",
       "      <td>5.265188e-04</td>\n",
       "      <td>4.788780e-07</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>1.172302e-04</td>\n",
       "      <td>9.239617e-05</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>1.637074e-05</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.998748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61176</th>\n",
       "      <td>0.999967</td>\n",
       "      <td>3.725411e-06</td>\n",
       "      <td>1.594743e-06</td>\n",
       "      <td>4.352764e-04</td>\n",
       "      <td>1.675731e-06</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.002517</td>\n",
       "      <td>1.888194e-04</td>\n",
       "      <td>1.915578e-04</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.005881</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>4.477100e-05</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.997247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61177</th>\n",
       "      <td>0.998971</td>\n",
       "      <td>5.196242e-05</td>\n",
       "      <td>8.685133e-06</td>\n",
       "      <td>4.545992e-05</td>\n",
       "      <td>6.160387e-05</td>\n",
       "      <td>0.008885</td>\n",
       "      <td>0.988668</td>\n",
       "      <td>0.037649</td>\n",
       "      <td>2.904862e-04</td>\n",
       "      <td>6.397196e-05</td>\n",
       "      <td>0.173240</td>\n",
       "      <td>0.998716</td>\n",
       "      <td>0.004368</td>\n",
       "      <td>0.234360</td>\n",
       "      <td>2.583760e-03</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.987478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61178</th>\n",
       "      <td>0.999983</td>\n",
       "      <td>2.853712e-06</td>\n",
       "      <td>5.331539e-07</td>\n",
       "      <td>9.846068e-04</td>\n",
       "      <td>1.176949e-06</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>2.456980e-04</td>\n",
       "      <td>1.898870e-04</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>3.619038e-05</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.999042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61179</th>\n",
       "      <td>0.967658</td>\n",
       "      <td>2.165107e-05</td>\n",
       "      <td>3.323147e-05</td>\n",
       "      <td>1.520967e-05</td>\n",
       "      <td>6.682293e-05</td>\n",
       "      <td>0.008043</td>\n",
       "      <td>0.995119</td>\n",
       "      <td>0.042820</td>\n",
       "      <td>1.253204e-04</td>\n",
       "      <td>1.508803e-05</td>\n",
       "      <td>0.099131</td>\n",
       "      <td>0.998481</td>\n",
       "      <td>0.011725</td>\n",
       "      <td>0.215425</td>\n",
       "      <td>6.995893e-04</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.984653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61180</th>\n",
       "      <td>0.971657</td>\n",
       "      <td>7.901911e-04</td>\n",
       "      <td>1.074627e-04</td>\n",
       "      <td>5.251727e-04</td>\n",
       "      <td>1.252494e-03</td>\n",
       "      <td>0.010270</td>\n",
       "      <td>0.875115</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>1.697843e-03</td>\n",
       "      <td>1.426749e-03</td>\n",
       "      <td>0.107879</td>\n",
       "      <td>0.944701</td>\n",
       "      <td>0.007169</td>\n",
       "      <td>0.207448</td>\n",
       "      <td>6.266179e-03</td>\n",
       "      <td>0.052776</td>\n",
       "      <td>0.984010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61181</th>\n",
       "      <td>0.999968</td>\n",
       "      <td>1.868225e-05</td>\n",
       "      <td>7.268863e-07</td>\n",
       "      <td>7.299666e-02</td>\n",
       "      <td>5.543006e-06</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>5.957303e-03</td>\n",
       "      <td>1.792227e-03</td>\n",
       "      <td>0.004750</td>\n",
       "      <td>0.008176</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>1.839389e-04</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.999195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61182</th>\n",
       "      <td>0.999728</td>\n",
       "      <td>2.195526e-04</td>\n",
       "      <td>5.963997e-06</td>\n",
       "      <td>1.541036e-03</td>\n",
       "      <td>1.298093e-04</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.795494</td>\n",
       "      <td>0.057612</td>\n",
       "      <td>3.310861e-03</td>\n",
       "      <td>5.266647e-04</td>\n",
       "      <td>0.252606</td>\n",
       "      <td>0.903223</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.031329</td>\n",
       "      <td>4.539633e-03</td>\n",
       "      <td>0.008787</td>\n",
       "      <td>0.994294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61183</th>\n",
       "      <td>0.999983</td>\n",
       "      <td>1.314784e-06</td>\n",
       "      <td>4.877958e-07</td>\n",
       "      <td>7.543983e-04</td>\n",
       "      <td>4.836272e-07</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>1.438530e-04</td>\n",
       "      <td>1.038783e-04</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>1.679979e-05</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.998666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61184</th>\n",
       "      <td>0.999776</td>\n",
       "      <td>2.753890e-04</td>\n",
       "      <td>1.757719e-05</td>\n",
       "      <td>1.189101e-03</td>\n",
       "      <td>6.407840e-05</td>\n",
       "      <td>0.004353</td>\n",
       "      <td>0.046901</td>\n",
       "      <td>0.990612</td>\n",
       "      <td>8.266161e-04</td>\n",
       "      <td>3.164687e-05</td>\n",
       "      <td>0.016643</td>\n",
       "      <td>0.070762</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>5.486817e-04</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.993749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61185</th>\n",
       "      <td>0.367675</td>\n",
       "      <td>4.666716e-04</td>\n",
       "      <td>3.028131e-01</td>\n",
       "      <td>9.570892e-04</td>\n",
       "      <td>1.780296e-04</td>\n",
       "      <td>0.536505</td>\n",
       "      <td>0.017615</td>\n",
       "      <td>0.518209</td>\n",
       "      <td>2.418692e-04</td>\n",
       "      <td>2.225157e-04</td>\n",
       "      <td>0.010972</td>\n",
       "      <td>0.142598</td>\n",
       "      <td>0.026429</td>\n",
       "      <td>0.007438</td>\n",
       "      <td>6.147469e-04</td>\n",
       "      <td>0.006756</td>\n",
       "      <td>0.031748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61186</th>\n",
       "      <td>0.114158</td>\n",
       "      <td>4.420554e-07</td>\n",
       "      <td>8.985660e-01</td>\n",
       "      <td>1.026719e-06</td>\n",
       "      <td>9.962171e-08</td>\n",
       "      <td>0.182640</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.007532</td>\n",
       "      <td>6.455858e-07</td>\n",
       "      <td>7.946539e-07</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.004857</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>1.086101e-06</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.001119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61187</th>\n",
       "      <td>0.999979</td>\n",
       "      <td>1.375536e-04</td>\n",
       "      <td>6.377340e-07</td>\n",
       "      <td>2.949559e-03</td>\n",
       "      <td>2.205703e-05</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.044437</td>\n",
       "      <td>0.995056</td>\n",
       "      <td>2.013767e-03</td>\n",
       "      <td>2.395875e-05</td>\n",
       "      <td>0.010967</td>\n",
       "      <td>0.037366</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>2.148819e-04</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.999090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61188</th>\n",
       "      <td>0.999920</td>\n",
       "      <td>1.877433e-04</td>\n",
       "      <td>3.129968e-06</td>\n",
       "      <td>4.903665e-03</td>\n",
       "      <td>8.204202e-05</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.161574</td>\n",
       "      <td>0.313742</td>\n",
       "      <td>7.259411e-03</td>\n",
       "      <td>8.331224e-04</td>\n",
       "      <td>0.012076</td>\n",
       "      <td>0.031136</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.005572</td>\n",
       "      <td>5.132243e-04</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.998055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61189</th>\n",
       "      <td>0.023268</td>\n",
       "      <td>2.174352e-09</td>\n",
       "      <td>9.781073e-01</td>\n",
       "      <td>1.369258e-08</td>\n",
       "      <td>4.540361e-10</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>1.254945e-08</td>\n",
       "      <td>6.256986e-09</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>9.037250e-09</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61190</th>\n",
       "      <td>0.741350</td>\n",
       "      <td>1.033791e-03</td>\n",
       "      <td>3.420757e-04</td>\n",
       "      <td>1.382809e-04</td>\n",
       "      <td>2.182033e-03</td>\n",
       "      <td>0.027739</td>\n",
       "      <td>0.939396</td>\n",
       "      <td>0.116504</td>\n",
       "      <td>7.860148e-04</td>\n",
       "      <td>1.151946e-04</td>\n",
       "      <td>0.018411</td>\n",
       "      <td>0.214129</td>\n",
       "      <td>0.016812</td>\n",
       "      <td>0.982339</td>\n",
       "      <td>9.127743e-04</td>\n",
       "      <td>0.031675</td>\n",
       "      <td>0.906828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61191 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0             1             2             3             4   \\\n",
       "0      0.999967  3.106106e-06  1.177439e-06  5.212919e-03  1.022755e-06   \n",
       "1      0.999939  5.980464e-05  2.818454e-06  3.326960e-02  2.187197e-05   \n",
       "2      0.998018  3.454607e-04  2.046244e-03  8.044354e-04  2.254558e-04   \n",
       "3      0.999958  2.117519e-05  2.232488e-06  6.061943e-03  5.304804e-06   \n",
       "4      0.361808  5.791064e-06  6.839007e-01  1.370201e-05  1.597645e-06   \n",
       "5      0.999986  1.538210e-06  3.735647e-07  9.140233e-04  5.873340e-07   \n",
       "6      0.999282  4.499687e-04  3.573338e-06  3.987618e-04  5.717833e-04   \n",
       "7      0.678870  8.134326e-04  2.296823e-04  8.716126e-05  2.005903e-03   \n",
       "8      0.999983  1.370272e-06  5.577581e-07  4.432974e-04  5.317090e-07   \n",
       "9      0.994316  1.598539e-04  2.877838e-03  3.793644e-04  1.581127e-04   \n",
       "10     0.999875  7.856272e-05  1.869810e-05  3.476651e-04  7.360628e-05   \n",
       "11     0.999856  3.351516e-03  1.059800e-05  1.490773e-02  2.558730e-04   \n",
       "12     0.071089  2.699767e-07  9.425774e-01  9.006841e-07  8.086721e-08   \n",
       "13     0.999877  3.688276e-05  6.145753e-06  6.086219e-04  3.444354e-05   \n",
       "14     0.999781  8.120236e-05  2.912215e-05  2.784266e-03  2.156365e-05   \n",
       "15     0.999972  4.916564e-06  1.074837e-06  1.048673e-03  1.982548e-06   \n",
       "16     0.998389  1.431545e-04  1.019307e-06  9.271722e-05  1.547988e-04   \n",
       "17     0.999973  6.712968e-06  2.036089e-06  5.739804e-04  4.693825e-06   \n",
       "18     0.999078  7.190248e-06  2.670492e-04  1.654696e-04  3.692644e-06   \n",
       "19     0.999831  1.305850e-03  8.036317e-06  1.106443e-02  7.855866e-04   \n",
       "20     0.999846  6.878508e-04  1.089343e-05  1.979547e-03  2.119597e-04   \n",
       "21     0.998077  1.052158e-03  9.426346e-05  1.408917e-03  2.544636e-04   \n",
       "22     0.994631  2.305544e-04  5.538400e-05  1.949885e-04  3.164085e-04   \n",
       "23     0.021969  4.716390e-09  9.808469e-01  2.111699e-08  1.089209e-09   \n",
       "24     0.969079  1.603338e-04  3.165174e-03  8.604564e-05  1.371231e-03   \n",
       "25     0.999985  2.267770e-06  4.244049e-07  1.508066e-03  8.666124e-07   \n",
       "26     0.995655  1.660612e-05  1.089485e-05  5.386593e-06  1.803086e-04   \n",
       "27     0.996487  3.592463e-04  1.731960e-04  7.719333e-04  2.240448e-04   \n",
       "28     0.971445  1.258493e-04  4.012210e-04  7.173859e-05  1.587783e-03   \n",
       "29     0.999985  1.165287e-06  4.801989e-07  3.676045e-04  4.549279e-07   \n",
       "...         ...           ...           ...           ...           ...   \n",
       "61161  0.999957  3.208696e-05  1.716698e-06  3.843901e-03  8.038573e-06   \n",
       "61162  0.998761  6.489002e-05  4.828183e-06  6.150446e-05  7.868394e-05   \n",
       "61163  0.999300  1.075226e-04  1.411179e-04  6.423120e-03  5.500939e-05   \n",
       "61164  0.999938  1.030854e-05  6.149295e-06  8.189619e-04  7.154222e-06   \n",
       "61165  0.999659  7.048241e-04  2.068624e-05  1.688479e-02  1.161414e-04   \n",
       "61166  0.999985  1.322739e-06  4.254490e-07  5.218388e-04  5.181643e-07   \n",
       "61167  0.999901  1.098297e-04  5.610090e-05  5.680132e-04  1.054761e-04   \n",
       "61168  0.999981  3.058670e-06  5.650337e-07  2.761142e-03  1.104483e-06   \n",
       "61169  0.999900  2.509452e-05  6.233103e-06  9.495432e-03  9.660926e-06   \n",
       "61170  0.999981  2.290810e-06  5.720761e-07  1.240500e-03  8.849601e-07   \n",
       "61171  0.999374  1.628796e-04  2.865769e-05  9.291368e-04  6.934589e-05   \n",
       "61172  0.996569  2.368998e-03  8.984202e-05  5.827612e-03  1.856989e-03   \n",
       "61173  0.999953  4.500354e-05  1.629930e-06  3.804479e-02  1.473421e-05   \n",
       "61174  0.999985  1.364887e-06  4.228582e-07  5.108836e-04  5.387782e-07   \n",
       "61175  0.999984  1.252274e-06  4.985066e-07  5.265188e-04  4.788780e-07   \n",
       "61176  0.999967  3.725411e-06  1.594743e-06  4.352764e-04  1.675731e-06   \n",
       "61177  0.998971  5.196242e-05  8.685133e-06  4.545992e-05  6.160387e-05   \n",
       "61178  0.999983  2.853712e-06  5.331539e-07  9.846068e-04  1.176949e-06   \n",
       "61179  0.967658  2.165107e-05  3.323147e-05  1.520967e-05  6.682293e-05   \n",
       "61180  0.971657  7.901911e-04  1.074627e-04  5.251727e-04  1.252494e-03   \n",
       "61181  0.999968  1.868225e-05  7.268863e-07  7.299666e-02  5.543006e-06   \n",
       "61182  0.999728  2.195526e-04  5.963997e-06  1.541036e-03  1.298093e-04   \n",
       "61183  0.999983  1.314784e-06  4.877958e-07  7.543983e-04  4.836272e-07   \n",
       "61184  0.999776  2.753890e-04  1.757719e-05  1.189101e-03  6.407840e-05   \n",
       "61185  0.367675  4.666716e-04  3.028131e-01  9.570892e-04  1.780296e-04   \n",
       "61186  0.114158  4.420554e-07  8.985660e-01  1.026719e-06  9.962171e-08   \n",
       "61187  0.999979  1.375536e-04  6.377340e-07  2.949559e-03  2.205703e-05   \n",
       "61188  0.999920  1.877433e-04  3.129968e-06  4.903665e-03  8.204202e-05   \n",
       "61189  0.023268  2.174352e-09  9.781073e-01  1.369258e-08  4.540361e-10   \n",
       "61190  0.741350  1.033791e-03  3.420757e-04  1.382809e-04  2.182033e-03   \n",
       "\n",
       "             5         6         7             8             9         10  \\\n",
       "0      0.000925  0.000966  0.001429  5.062488e-04  2.432009e-04  0.001663   \n",
       "1      0.002569  0.004343  0.004395  7.766142e-03  3.785155e-03  0.012106   \n",
       "2      0.999746  0.010043  0.035536  5.562823e-04  9.467581e-04  0.017220   \n",
       "3      0.005796  0.003354  0.005371  1.944432e-03  1.700670e-03  0.134701   \n",
       "4      0.393776  0.002045  0.015132  7.307442e-06  9.923245e-06  0.001527   \n",
       "5      0.000666  0.000774  0.001212  1.840009e-04  1.301600e-04  0.001492   \n",
       "6      0.329551  0.246420  0.131417  1.550824e-03  3.542325e-04  0.426516   \n",
       "7      0.014306  0.964268  0.106803  5.682569e-04  8.303207e-05  0.018326   \n",
       "8      0.000692  0.000841  0.001376  1.112486e-04  9.040999e-05  0.001394   \n",
       "9      0.030063  0.044678  0.064213  6.814570e-04  1.514508e-03  0.162331   \n",
       "10     0.998228  0.014126  0.226993  4.015515e-04  2.815766e-04  0.009439   \n",
       "11     0.003413  0.026095  0.028235  1.691920e-02  2.690311e-02  0.723598   \n",
       "12     0.016443  0.001158  0.012944  8.185493e-07  4.367851e-07  0.000264   \n",
       "13     0.005393  0.077665  0.013841  6.819579e-04  7.383208e-04  0.112009   \n",
       "14     0.007024  0.009825  0.016927  2.001904e-03  9.224213e-04  0.362450   \n",
       "15     0.001138  0.001580  0.002353  3.110443e-04  2.571787e-04  0.002878   \n",
       "16     0.006230  0.745872  0.027492  6.349873e-04  4.931545e-04  0.247159   \n",
       "17     0.750200  0.001764  0.003196  2.296602e-04  2.490476e-04  0.003674   \n",
       "18     0.004489  0.003012  0.006873  9.165070e-05  1.092949e-04  0.003639   \n",
       "19     0.003606  0.606888  0.747794  5.822596e-02  5.557079e-04  0.026586   \n",
       "20     0.004259  0.119048  0.958299  2.609550e-03  1.831324e-04  0.047763   \n",
       "21     0.008226  0.151411  0.985162  1.088353e-03  6.360449e-05  0.037397   \n",
       "22     0.022355  0.959490  0.059682  1.153717e-03  2.721530e-04  0.283476   \n",
       "23     0.002111  0.000142  0.002027  2.171473e-08  1.064819e-08  0.000024   \n",
       "24     0.999919  0.217248  0.099341  1.714195e-04  3.530525e-04  0.036433   \n",
       "25     0.000753  0.000898  0.001332  2.816993e-04  1.829466e-04  0.001780   \n",
       "26     0.997544  0.999364  0.059502  1.873008e-04  9.176655e-06  0.100627   \n",
       "27     0.012767  0.073044  0.023440  8.514746e-04  7.766811e-03  0.146180   \n",
       "28     0.986829  0.904923  0.053600  3.099827e-04  1.567295e-04  0.082466   \n",
       "29     0.000650  0.000782  0.001316  9.727140e-05  7.996677e-05  0.001301   \n",
       "...         ...       ...       ...           ...           ...       ...   \n",
       "61161  0.001673  0.004519  0.006189  1.801712e-03  1.688680e-03  0.098412   \n",
       "61162  0.011445  0.916545  0.030340  3.524499e-04  1.456435e-04  0.338170   \n",
       "61163  0.058443  0.005270  0.008186  1.648727e-03  3.262760e-03  0.016991   \n",
       "61164  0.289111  0.001805  0.003376  2.792257e-04  3.606089e-04  0.004193   \n",
       "61165  0.005535  0.022315  0.019900  1.177194e-02  2.090881e-02  0.671940   \n",
       "61166  0.000680  0.000761  0.001267  1.246576e-04  1.014852e-04  0.001411   \n",
       "61167  0.999990  0.006014  0.016558  5.084062e-04  7.975284e-04  0.011210   \n",
       "61168  0.000781  0.001027  0.001422  4.334358e-04  2.546448e-04  0.001952   \n",
       "61169  0.003672  0.002876  0.003752  1.582034e-03  1.392484e-03  0.008860   \n",
       "61170  0.000796  0.001022  0.001583  2.496493e-04  1.611552e-04  0.001693   \n",
       "61171  0.008868  0.068133  0.014479  9.157860e-04  5.410908e-03  0.308702   \n",
       "61172  0.023427  0.971727  0.131306  2.410950e-02  2.993789e-03  0.225830   \n",
       "61173  0.001721  0.003597  0.003589  7.313328e-03  3.750455e-03  0.009671   \n",
       "61174  0.000690  0.000780  0.001288  1.255990e-04  1.025489e-04  0.001446   \n",
       "61175  0.000679  0.000757  0.001268  1.172302e-04  9.239617e-05  0.001317   \n",
       "61176  0.001188  0.001573  0.002517  1.888194e-04  1.915578e-04  0.002471   \n",
       "61177  0.008885  0.988668  0.037649  2.904862e-04  6.397196e-05  0.173240   \n",
       "61178  0.000861  0.001062  0.001575  2.456980e-04  1.898870e-04  0.002038   \n",
       "61179  0.008043  0.995119  0.042820  1.253204e-04  1.508803e-05  0.099131   \n",
       "61180  0.010270  0.875115  0.059200  1.697843e-03  1.426749e-03  0.107879   \n",
       "61181  0.001140  0.001955  0.002148  5.957303e-03  1.792227e-03  0.004750   \n",
       "61182  0.005292  0.795494  0.057612  3.310861e-03  5.266647e-04  0.252606   \n",
       "61183  0.000650  0.000757  0.001225  1.438530e-04  1.038783e-04  0.001320   \n",
       "61184  0.004353  0.046901  0.990612  8.266161e-04  3.164687e-05  0.016643   \n",
       "61185  0.536505  0.017615  0.518209  2.418692e-04  2.225157e-04  0.010972   \n",
       "61186  0.182640  0.000777  0.007532  6.455858e-07  7.946539e-07  0.000384   \n",
       "61187  0.001293  0.044437  0.995056  2.013767e-03  2.395875e-05  0.010967   \n",
       "61188  0.002369  0.161574  0.313742  7.259411e-03  8.331224e-04  0.012076   \n",
       "61189  0.001479  0.000118  0.001825  1.254945e-08  6.256986e-09  0.000017   \n",
       "61190  0.027739  0.939396  0.116504  7.860148e-04  1.151946e-04  0.018411   \n",
       "\n",
       "             11        12        13            14        15        16  \n",
       "0      0.003297  0.000782  0.000320  3.407740e-05  0.000085  0.997537  \n",
       "1      0.016742  0.000277  0.002726  6.262326e-04  0.000842  0.998239  \n",
       "2      0.040024  0.000310  0.001847  4.917934e-04  0.001674  0.000099  \n",
       "3      0.142489  0.000648  0.001855  1.017280e-03  0.001212  0.992230  \n",
       "4      0.012225  0.003225  0.000380  1.202656e-05  0.000192  0.002534  \n",
       "5      0.003057  0.000110  0.000227  2.120440e-05  0.000087  0.999115  \n",
       "6      0.857581  0.033066  0.996704  1.001025e-02  0.008540  0.441267  \n",
       "7      0.343445  0.015184  0.984827  9.150566e-04  0.034154  0.954906  \n",
       "8      0.003040  0.000264  0.000196  1.716028e-05  0.000089  0.998565  \n",
       "9      0.859225  0.585079  0.008042  2.259216e-03  0.011323  0.351896  \n",
       "10     0.037495  0.000066  0.002141  2.630966e-04  0.001177  0.002853  \n",
       "11     0.639615  0.002170  0.031093  1.062757e-01  0.072501  0.993080  \n",
       "12     0.004365  0.025432  0.000074  7.373139e-07  0.000032  0.001966  \n",
       "13     0.949799  0.002093  0.010810  1.976188e-03  0.001844  0.994493  \n",
       "14     0.319400  0.033212  0.004687  2.368983e-03  0.004854  0.953801  \n",
       "15     0.005889  0.000214  0.000480  5.542183e-05  0.000226  0.998570  \n",
       "16     0.977237  0.003188  0.983720  6.684822e-03  0.006599  0.991455  \n",
       "17     0.007931  0.000076  0.000499  5.306752e-05  0.000183  0.254008  \n",
       "18     0.009436  0.331572  0.000301  3.385883e-05  0.000261  0.695930  \n",
       "19     0.053721  0.001562  0.012954  1.398870e-03  0.005113  0.995589  \n",
       "20     0.312155  0.004559  0.015489  2.239043e-03  0.008556  0.992417  \n",
       "21     0.214192  0.005178  0.086244  1.958112e-03  0.012831  0.989474  \n",
       "22     0.993659  0.022695  0.526379  5.850964e-03  0.006570  0.958373  \n",
       "23     0.000524  0.008923  0.000005  1.870447e-08  0.000002  0.000676  \n",
       "24     0.930320  0.000900  0.009238  1.330758e-03  0.006304  0.000029  \n",
       "25     0.003535  0.000107  0.000317  3.081585e-05  0.000107  0.999146  \n",
       "26     0.998466  0.000278  0.259698  4.702270e-04  0.001664  0.003156  \n",
       "27     0.941820  0.019440  0.022930  1.044770e-02  0.035658  0.964695  \n",
       "28     0.986966  0.005462  0.048993  1.451951e-03  0.007816  0.009621  \n",
       "29     0.002864  0.000216  0.000176  1.507324e-05  0.000083  0.998711  \n",
       "...         ...       ...       ...           ...       ...       ...  \n",
       "61161  0.103631  0.000545  0.002337  1.028786e-03  0.001923  0.997551  \n",
       "61162  0.996945  0.011069  0.868200  4.964011e-03  0.003803  0.976865  \n",
       "61163  0.034488  0.010021  0.002184  7.061257e-04  0.001646  0.913346  \n",
       "61164  0.009441  0.000310  0.000570  8.420820e-05  0.000251  0.726520  \n",
       "61165  0.712828  0.002888  0.026375  5.107237e-02  0.029285  0.990935  \n",
       "61166  0.003016  0.000136  0.000204  1.802866e-05  0.000088  0.998980  \n",
       "61167  0.025841  0.000029  0.000869  2.385268e-04  0.000689  0.000012  \n",
       "61168  0.003725  0.000213  0.000354  3.835941e-05  0.000114  0.998837  \n",
       "61169  0.016998  0.001096  0.001378  2.774077e-04  0.000516  0.995234  \n",
       "61170  0.003506  0.000230  0.000303  2.936473e-05  0.000107  0.998770  \n",
       "61171  0.979834  0.005015  0.031144  1.588224e-02  0.009120  0.985425  \n",
       "61172  0.785463  0.006503  0.249677  1.062861e-02  0.028026  0.976465  \n",
       "61173  0.013727  0.000205  0.001915  4.221095e-04  0.000643  0.998629  \n",
       "61174  0.003076  0.000127  0.000209  1.850787e-05  0.000091  0.999006  \n",
       "61175  0.002864  0.000207  0.000187  1.637074e-05  0.000081  0.998748  \n",
       "61176  0.005881  0.000659  0.000322  4.477100e-05  0.000207  0.997247  \n",
       "61177  0.998716  0.004368  0.234360  2.583760e-03  0.002264  0.987478  \n",
       "61178  0.003992  0.000118  0.000350  3.619038e-05  0.000142  0.999042  \n",
       "61179  0.998481  0.011725  0.215425  6.995893e-04  0.001538  0.984653  \n",
       "61180  0.944701  0.007169  0.207448  6.266179e-03  0.052776  0.984010  \n",
       "61181  0.008176  0.000104  0.001228  1.839389e-04  0.000265  0.999195  \n",
       "61182  0.903223  0.001919  0.031329  4.539633e-03  0.008787  0.994294  \n",
       "61183  0.002810  0.000247  0.000188  1.679979e-05  0.000077  0.998666  \n",
       "61184  0.070762  0.002500  0.011071  5.486817e-04  0.003319  0.993749  \n",
       "61185  0.142598  0.026429  0.007438  6.147469e-04  0.006756  0.031748  \n",
       "61186  0.004857  0.001834  0.000109  1.086101e-06  0.000039  0.001119  \n",
       "61187  0.037366  0.000234  0.005746  2.148819e-04  0.001411  0.999090  \n",
       "61188  0.031136  0.000605  0.005572  5.132243e-04  0.002316  0.998055  \n",
       "61189  0.000365  0.009636  0.000003  9.037250e-09  0.000001  0.000876  \n",
       "61190  0.214129  0.016812  0.982339  9.127743e-04  0.031675  0.906828  \n",
       "\n",
       "[61191 rows x 17 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_test = np.array(p_test) > 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True],\n",
       "       [ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True],\n",
       "       [ True, False, False, False, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agriculture': 11,\n",
       " 'artisinal_mine': 1,\n",
       " 'bare_ground': 15,\n",
       " 'blooming': 3,\n",
       " 'blow_down': 9,\n",
       " 'clear': 16,\n",
       " 'cloudy': 2,\n",
       " 'conventional_mine': 4,\n",
       " 'cultivation': 10,\n",
       " 'habitation': 13,\n",
       " 'haze': 12,\n",
       " 'partly_cloudy': 5,\n",
       " 'primary': 0,\n",
       " 'road': 6,\n",
       " 'selective_logging': 8,\n",
       " 'slash_burn': 14,\n",
       " 'water': 7}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = np.array(['primary', 'artisinal_mine', 'cloudy','blooming','conventional_mine',\n",
    "                       'partly_cloudy','road','water','selective_logging','blow_down',\n",
    "                       'cultivation','agriculture','haze','habitation',\n",
    "                       'slash_burn','bare_ground','clear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(len(p_test)):\n",
    "    res += [' '.join(categories[p_test[i]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61191"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission['tags'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>primary partly_cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>primary cultivation agriculture clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>primary cloudy partly_cloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                   tags\n",
       "0     test_0                          primary clear\n",
       "1     test_1                          primary clear\n",
       "2     test_2                  primary partly_cloudy\n",
       "3     test_3  primary cultivation agriculture clear\n",
       "4     test_4           primary cloudy partly_cloudy"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>primary partly_cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>primary partly_cloudy cultivation agriculture ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>primary cloudy partly_cloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                               tags\n",
       "0     test_0                                      primary clear\n",
       "1     test_1                                      primary clear\n",
       "2     test_2                              primary partly_cloudy\n",
       "3     test_3  primary partly_cloudy cultivation agriculture ...\n",
       "4     test_4                       primary cloudy partly_cloudy"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61186</th>\n",
       "      <td>file_9995</td>\n",
       "      <td>primary partly_cloudy road water agriculture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61187</th>\n",
       "      <td>file_9996</td>\n",
       "      <td>primary road water cultivation agriculture hab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61188</th>\n",
       "      <td>file_9997</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61189</th>\n",
       "      <td>file_9998</td>\n",
       "      <td>primary cloudy haze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61190</th>\n",
       "      <td>file_9999</td>\n",
       "      <td>primary partly_cloudy road water agriculture h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_name                                               tags\n",
       "61186  file_9995       primary partly_cloudy road water agriculture\n",
       "61187  file_9996  primary road water cultivation agriculture hab...\n",
       "61188  file_9997                                      primary clear\n",
       "61189  file_9998                                primary cloudy haze\n",
       "61190  file_9999  primary partly_cloudy road water agriculture h..."
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"predictions/tmp13.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
