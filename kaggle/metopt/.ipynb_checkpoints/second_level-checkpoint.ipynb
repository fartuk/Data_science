{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fattahov.ao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "files = pd.read_csv('ef.csv').ix[:,0].tolist()\n",
    "tmp_files = pd.read_csv('ef.csv')\n",
    "tmp_files.columns = ['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1 = pd.read_csv('preds/1_train.csv',header=None)\n",
    "p2 = pd.read_csv('preds/2_train.csv',header=None)\n",
    "p3 = pd.read_csv('preds/3_train.csv',header=None)\n",
    "p4 = pd.read_csv('preds/4_train.csv',header=None)\n",
    "p5 = pd.read_csv('preds/5_train.csv',header=None)\n",
    "p6 = pd.read_csv('preds/6_train.csv',header=None)\n",
    "\n",
    "p3 = p3[:6905]\n",
    "p4 = p4[:6905]\n",
    "p5 = p5[:6905]\n",
    "p6 = p6[:6905]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Стратифицированное разбиение на фолды\n",
    "def skf(a):\n",
    "    n = len(a)\n",
    "    b = [0, 0]\n",
    "    for i in a:\n",
    "        b[i] += 1\n",
    "    bb = []\n",
    "    for i in range(5):\n",
    "        bb += [[0, 0]]\n",
    "    for i in range(2):\n",
    "        for j in range(b[i] % 5):\n",
    "            bb[j][i] = b[i] // 5 + 1\n",
    "        for j in range(b[i] % 5, 5):\n",
    "            bb[j][i] = b[i] // 5;\n",
    " \n",
    "    cur = [0, 0]\n",
    "    c = []\n",
    "    for i in range(5):\n",
    "        c += [[0, 0]]\n",
    "    ans = []\n",
    "    for i in range(5):\n",
    "        ans += [[[], []]]\n",
    "    for i in range(n):\n",
    "        if c[cur[a[i]]][a[i]] < bb[cur[a[i]]][a[i]]:\n",
    "            for j in range(5):\n",
    "                if (cur[a[i]] == j):\n",
    "                    ans[j][0].append(i)\n",
    "                else:\n",
    "                    ans[j][1].append(i)\n",
    "            c[cur[a[i]]][a[i]] += 1\n",
    "            if (c[cur[a[i]]][a[i]] == bb[cur[a[i]]][a[i]]):\n",
    "                cur[a[i]] += 1;\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "classes = []\n",
    "y = []\n",
    "for i in files:\n",
    "    if 'indoor' in i:\n",
    "        classes += [[0, 1]]\n",
    "        y += [1]\n",
    "    else:\n",
    "        classes += [[1, 0]]\n",
    "        y += [0]\n",
    "classes = np.array(classes)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = pd.concat([p1, p2, p3, p4, p5, p6], axis=1)\n",
    "preds.columns = ['p1', 'p2', 'p3', 'p4', 'p5', 'p6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.726661</td>\n",
       "      <td>0.872270</td>\n",
       "      <td>0.992406</td>\n",
       "      <td>0.977511</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>0.908865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999764</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.201203</td>\n",
       "      <td>0.109141</td>\n",
       "      <td>0.060366</td>\n",
       "      <td>0.118175</td>\n",
       "      <td>0.004036</td>\n",
       "      <td>0.031256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.938829</td>\n",
       "      <td>0.831030</td>\n",
       "      <td>0.868279</td>\n",
       "      <td>0.896412</td>\n",
       "      <td>0.997449</td>\n",
       "      <td>0.897891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.425932</td>\n",
       "      <td>0.145346</td>\n",
       "      <td>0.860063</td>\n",
       "      <td>0.231303</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.538335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.657997</td>\n",
       "      <td>0.246363</td>\n",
       "      <td>0.677175</td>\n",
       "      <td>0.359080</td>\n",
       "      <td>0.810799</td>\n",
       "      <td>0.584259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.990974</td>\n",
       "      <td>0.952459</td>\n",
       "      <td>0.973173</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>0.959905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.993630</td>\n",
       "      <td>0.995189</td>\n",
       "      <td>0.992105</td>\n",
       "      <td>0.994251</td>\n",
       "      <td>0.999830</td>\n",
       "      <td>0.984658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.364095</td>\n",
       "      <td>0.180749</td>\n",
       "      <td>0.683647</td>\n",
       "      <td>0.457735</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.138814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.998441</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.999980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.967060</td>\n",
       "      <td>0.975291</td>\n",
       "      <td>0.997188</td>\n",
       "      <td>0.982715</td>\n",
       "      <td>0.999421</td>\n",
       "      <td>0.966143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.023472</td>\n",
       "      <td>0.010249</td>\n",
       "      <td>0.060166</td>\n",
       "      <td>0.018545</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.256081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.996123</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>0.997521</td>\n",
       "      <td>0.999899</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.993925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.245024</td>\n",
       "      <td>0.118910</td>\n",
       "      <td>0.022774</td>\n",
       "      <td>0.047218</td>\n",
       "      <td>0.044708</td>\n",
       "      <td>0.108958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.001086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.032149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.997702</td>\n",
       "      <td>0.998445</td>\n",
       "      <td>0.999807</td>\n",
       "      <td>0.996087</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.990962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.992804</td>\n",
       "      <td>0.995677</td>\n",
       "      <td>0.991286</td>\n",
       "      <td>0.985288</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.984744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.999592</td>\n",
       "      <td>0.999898</td>\n",
       "      <td>0.999928</td>\n",
       "      <td>0.999621</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.998951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.987168</td>\n",
       "      <td>0.993691</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.991912</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.996416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          p1        p2        p3        p4        p5        p6\n",
       "0   0.726661  0.872270  0.992406  0.977511  0.999973  0.908865\n",
       "1   0.999976  0.999976  0.999764  0.999903  0.999999  0.999893\n",
       "2   0.201203  0.109141  0.060366  0.118175  0.004036  0.031256\n",
       "3   0.938829  0.831030  0.868279  0.896412  0.997449  0.897891\n",
       "4   0.425932  0.145346  0.860063  0.231303  0.001202  0.538335\n",
       "5   0.657997  0.246363  0.677175  0.359080  0.810799  0.584259\n",
       "6   0.990974  0.952459  0.973173  0.962963  0.999762  0.959905\n",
       "7   0.993630  0.995189  0.992105  0.994251  0.999830  0.984658\n",
       "8   0.364095  0.180749  0.683647  0.457735  0.004145  0.138814\n",
       "9   0.999866  0.999968  0.998441  0.999961  0.999910  0.999980\n",
       "10  0.967060  0.975291  0.997188  0.982715  0.999421  0.966143\n",
       "11  0.023472  0.010249  0.060166  0.018545  0.001773  0.256081\n",
       "12  0.996123  0.999918  0.997521  0.999899  0.999966  0.993925\n",
       "13  0.245024  0.118910  0.022774  0.047218  0.044708  0.108958\n",
       "14  0.001015  0.000204  0.000395  0.000262  0.002636  0.001086\n",
       "15  0.000410  0.000751  0.000749  0.001322  0.003242  0.032149\n",
       "16  0.997702  0.998445  0.999807  0.996087  0.999999  0.990962\n",
       "17  0.992804  0.995677  0.991286  0.985288  0.999989  0.984744\n",
       "18  0.999592  0.999898  0.999928  0.999621  0.999962  0.998951\n",
       "19  0.987168  0.993691  0.993407  0.991912  0.999999  0.996416"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from scipy.stats import logistic\n",
    " \n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "def logloss(true_label, predicted, eps=1e-15):\n",
    "    p = np.clip(predicted, eps, 1 - eps)\n",
    "    if true_label == 1:\n",
    "        return -np.log(p)\n",
    "    else:\n",
    "        return -np.log(1 - p)\n",
    "    \n",
    "def log_loss(y_true, y_pred):\n",
    "    los = 0\n",
    "    for i in range(len(y_true)):\n",
    "\n",
    "\n",
    "        los += logloss(y_true[i][1], y_pred[i][1])\n",
    "            \n",
    "    return los / len(y_true)    \n",
    "    \n",
    "def log_loss1(y_true, y_pred):\n",
    "    los = 0\n",
    "    for i in range(len(y_true)):\n",
    "\n",
    "        los += logloss(y_true[i], y_pred[i])\n",
    "            \n",
    "    return los / len(y_true)     \n",
    "\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return logistic.cdf(x)\n",
    "\"\"\"\n",
    "def softmax(x):\n",
    "    e = np.exp(x - np.max(x))  # prevent overflow\n",
    "    if e.ndim == 1:\n",
    "        return e / np.sum(e, axis=0)\n",
    "    else:  \n",
    "        return e / np.array([np.sum(e, axis=1)]).T  # ndim = 2\n",
    "\"\"\"\n",
    "\n",
    "def softmax(X, copy=True):\n",
    "\n",
    "    if copy:\n",
    "        X = np.copy(X)\n",
    "    max_prob = np.max(X, axis=1).reshape((-1, 1))\n",
    "    X -= max_prob\n",
    "    np.exp(X, X)\n",
    "    sum_prob = np.sum(X, axis=1).reshape((-1, 1))\n",
    "    X /= sum_prob\n",
    "    return X\n",
    "\n",
    "class LogisticRegression_(object):\n",
    "    def __init__(self, X, y, X_val, y_val):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.W = np.zeros((X.shape[1], y.shape[1]))  # initialize W 0\n",
    "        self.b = np.zeros(y.shape[1])          # initialize bias 0\n",
    "        self.losses_train = []\n",
    "        self.losses_valid = []\n",
    "\n",
    "        # self.params = [self.W, self.b]\n",
    "\n",
    "    def fit(self, n_epochs=1000, lr=0.0001, L2_reg=0.001):\n",
    "        for i in range(n_epochs):\n",
    "            # p_y_given_x = sigmoid(numpy.dot(self.x, self.W) + self.b)\n",
    "            p_y_given_x = softmax(np.dot(self.X, self.W) + self.b)\n",
    "            d_y = self.y - p_y_given_x\n",
    "\n",
    "            self.W += lr * np.dot(self.X.T, d_y) - lr * L2_reg * self.W\n",
    "            self.b += lr * np.mean(d_y, axis=0)\n",
    "            #print(softmax(np.dot(self.X, self.W) + self.b))\n",
    "\n",
    "            #print(softmax(np.dot(self.X_val, self.W) + self.b))\n",
    "            if i % 500 == 0:\n",
    "                loss = log_loss(self.y, softmax(np.dot(self.X, self.W) + self.b))\n",
    "                val_loss = log_loss(self.y_val, softmax(np.dot(self.X_val, self.W) + self.b))\n",
    "                #loss = self.cross_entropy(val=False)\n",
    "                #val_loss = self.cross_entropy(val=True)\n",
    "            \n",
    "                self.losses_train += [loss]\n",
    "                self.losses_valid += [val_loss]\n",
    "                print('Train_loss=', loss,' Val_loss=', val_loss)\n",
    "\n",
    "    def cross_entropy(self, val=False):\n",
    "        if val:\n",
    "            X = self.X_val\n",
    "            y = self.y_val\n",
    "        else:\n",
    "            X = self.X\n",
    "            y = self.y\n",
    "            \n",
    "        # sigmoid_activation = sigmoid(numpy.dot(self.x, self.W) + self.b)\n",
    "        sigmoid_activation = softmax(np.dot(X, self.W) + self.b)\n",
    "\n",
    "        cross_entropy = - np.mean(np.sum(y * np.log(sigmoid_activation) + \n",
    "                                               (1 - y) * np.log(1 - sigmoid_activation),axis=1))\n",
    "        return cross_entropy\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # return sigmoid(np.dot(x, self.W) + self.b)\n",
    "        return softmax(np.dot(X, self.W) + self.b)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # return sigmoid(np.dot(x, self.W) + self.b)\n",
    "        preds = []\n",
    "        probas = softmax(np.dot(X, self.W) + self.b)\n",
    "        for i in probas:\n",
    "            preds += [i == i.max()]\n",
    "        return np.array(preds).astype(int)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = skf(y)\n",
    "    \n",
    "folds = []    \n",
    "for i in tmp:\n",
    "    folds += [[i[1], i[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6905, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss= 1.73871491486  Val_loss= 1.78600457519\n",
      "Train_loss= 0.25601013909  Val_loss= 0.257772060073\n",
      "Train_loss= 0.235745704159  Val_loss= 0.238628413829\n",
      "Train_loss= 0.218778307295  Val_loss= 0.222546240253\n",
      "Train_loss= 0.204472773512  Val_loss= 0.209035265711\n",
      "Train_loss= 0.192337855468  Val_loss= 0.197627604378\n",
      "Train_loss= 0.181975293019  Val_loss= 0.187932601015\n",
      "Train_loss= 0.17306429986  Val_loss= 0.179634769983\n",
      "Train_loss= 0.165347531793  Val_loss= 0.172481587377\n",
      "Train_loss= 0.158618641242  Val_loss= 0.16627118567\n",
      "Train_loss= 0.152711814181  Val_loss= 0.160841836905\n",
      "Train_loss= 0.147493276375  Val_loss= 0.156063412496\n",
      "Train_loss= 0.14285453786  Val_loss= 0.151830599358\n",
      "Train_loss= 0.138707077525  Val_loss= 0.148057558402\n",
      "Train_loss= 0.134978178204  Val_loss= 0.14467371252\n",
      "Train_loss= 0.131607661652  Val_loss= 0.141620373805\n",
      "Train_loss= 0.128545324582  Val_loss= 0.138847920889\n",
      "1.19962869325\n",
      "0.186365962744\n",
      "Train_loss= 1.75912508089  Val_loss= 1.68199724049\n",
      "Train_loss= 0.261402056087  Val_loss= 0.267454715299\n",
      "Train_loss= 0.241836531236  Val_loss= 0.246136391916\n",
      "Train_loss= 0.225352052637  Val_loss= 0.228183215756\n",
      "Train_loss= 0.211330940885  Val_loss= 0.212889417559\n",
      "Train_loss= 0.199332650546  Val_loss= 0.19977669559\n",
      "Train_loss= 0.188999566553  Val_loss= 0.188465040918\n",
      "Train_loss= 0.180042679917  Val_loss= 0.178647462305\n",
      "Train_loss= 0.172229095655  Val_loss= 0.17007566308\n",
      "Train_loss= 0.165371083815  Val_loss= 0.16254865237\n",
      "Train_loss= 0.159316925045  Val_loss= 0.155903334322\n",
      "Train_loss= 0.153943493807  Val_loss= 0.150006860335\n",
      "Train_loss= 0.149150361235  Val_loss= 0.144750517903\n",
      "Train_loss= 0.144855155165  Val_loss= 0.14004490063\n",
      "Train_loss= 0.140989928858  Val_loss= 0.135816116628\n",
      "Train_loss= 0.137498326795  Val_loss= 0.132002828396\n",
      "Train_loss= 0.13433337875  Val_loss= 0.128553962596\n",
      "1.22462115055\n",
      "0.188297267826\n",
      "Train_loss= 1.75156688335  Val_loss= 1.73700383809\n",
      "Train_loss= 0.25784603375  Val_loss= 0.249971535108\n",
      "Train_loss= 0.237652626693  Val_loss= 0.230851812882\n",
      "Train_loss= 0.220760118744  Val_loss= 0.21487103601\n",
      "Train_loss= 0.206537765571  Val_loss= 0.201436711666\n",
      "Train_loss= 0.194489947641  Val_loss= 0.190070863114\n",
      "Train_loss= 0.18421513948  Val_loss= 0.180388478198\n",
      "Train_loss= 0.175390606911  Val_loss= 0.172080939305\n",
      "Train_loss= 0.167757984848  Val_loss= 0.164901637071\n",
      "Train_loss= 0.161110498375  Val_loss= 0.158653495201\n",
      "Train_loss= 0.155282274243  Val_loss= 0.15317860852\n",
      "Train_loss= 0.15013970551  Val_loss= 0.148349902057\n",
      "Train_loss= 0.14557461203  Val_loss= 0.144064540239\n",
      "Train_loss= 0.141498880104  Val_loss= 0.140238767895\n",
      "Train_loss= 0.137840280337  Val_loss= 0.136803883955\n",
      "Train_loss= 0.134539205993  Val_loss= 0.133703091173\n",
      "Train_loss= 0.131546122554  Val_loss= 0.130889008809\n",
      "1.27553076093\n",
      "0.19217117641\n",
      "Train_loss= 1.79167060445  Val_loss= 1.52830087492\n",
      "Train_loss= 0.260740551425  Val_loss= 0.242952212746\n",
      "Train_loss= 0.240912998543  Val_loss= 0.2232870406\n",
      "Train_loss= 0.224330706223  Val_loss= 0.206661391833\n",
      "Train_loss= 0.210366674446  Val_loss= 0.19259146002\n",
      "Train_loss= 0.198534282107  Val_loss= 0.180626873929\n",
      "Train_loss= 0.18844050382  Val_loss= 0.170385916483\n",
      "Train_loss= 0.179769664516  Val_loss= 0.161558887901\n",
      "Train_loss= 0.172269069783  Val_loss= 0.153896901185\n",
      "Train_loss= 0.165736440953  Val_loss= 0.147200177532\n",
      "Train_loss= 0.160009451462  Val_loss= 0.141308014375\n",
      "Train_loss= 0.154957291859  Val_loss= 0.136090625205\n",
      "Train_loss= 0.150473997974  Val_loss= 0.13144264943\n",
      "Train_loss= 0.14647322678  Val_loss= 0.127278046797\n",
      "Train_loss= 0.142884183654  Val_loss= 0.123526098938\n",
      "Train_loss= 0.139648449105  Val_loss= 0.120128277856\n",
      "Train_loss= 0.136717501303  Val_loss= 0.117035783879\n",
      "0.951086297192\n",
      "0.1659404468\n",
      "Train_loss= 1.68709441021  Val_loss= 1.99707039037\n",
      "Train_loss= 0.250565490437  Val_loss= 0.28898931946\n",
      "Train_loss= 0.230941578618  Val_loss= 0.267852263842\n",
      "Train_loss= 0.214467779272  Val_loss= 0.249956604043\n",
      "Train_loss= 0.200482679389  Val_loss= 0.234660961737\n",
      "Train_loss= 0.188539893413  Val_loss= 0.221495121155\n",
      "Train_loss= 0.178279662707  Val_loss= 0.210082738796\n",
      "Train_loss= 0.169411495509  Val_loss= 0.200118850776\n",
      "Train_loss= 0.161701456095  Val_loss= 0.191354991009\n",
      "Train_loss= 0.154961463729  Val_loss= 0.183586081029\n",
      "Train_loss= 0.14904084034  Val_loss= 0.176636981867\n",
      "Train_loss= 0.143821786617  Val_loss= 0.170343108824\n",
      "Train_loss= 0.139241132144  Val_loss= 0.164521321529\n",
      "Train_loss= 0.135429152936  Val_loss= 0.159726956352\n",
      "Train_loss= 0.132114001208  Val_loss= 0.156094851821\n",
      "Train_loss= 0.129143416021  Val_loss= 0.152840537488\n",
      "Train_loss= 0.126470147734  Val_loss= 0.149907111501\n",
      "1.50171084521\n",
      "0.208514414057\n",
      "logreg: 1.231 +- 0.176\n",
      "rmse: 0.188 +- 0.014\n"
     ]
    }
   ],
   "source": [
    "pred_train = np.zeros(len(tr))\n",
    "est = []\n",
    "f_ll,f_rmse = [],[]\n",
    "\n",
    "for itr, ite in folds:\n",
    "    ypred = []\n",
    "    tmp_tr = tr[itr]\n",
    "    tmp_classes = classes[itr]\n",
    "    classifier = LogisticRegression_(tmp_tr, tmp_classes, tr[ite], classes[ite])\n",
    "    classifier.fit(n_epochs=8100, lr=0.002, L2_reg=0.001)\n",
    "\n",
    "    ypred = classifier.predict(tr[ite])[:, 1]\n",
    "\n",
    "    \n",
    "    pred_train[ite] = ypred \n",
    "\n",
    "    print(log_loss1(y[ite], pred_train[ite]))\n",
    "    print(rmse(y[ite], pred_train[ite]))\n",
    "    \n",
    "    f_ll.append(log_loss1(y[ite], pred_train[ite]))\n",
    "    f_rmse.append(rmse(y[ite], pred_train[ite]))\n",
    "\n",
    "    est.append(classifier)\n",
    "    \n",
    "print('logreg: {:.3f} +- {:.3f}'.format(np.mean(f_ll), np.std(f_ll)))\n",
    "print('rmse: {:.3f} +- {:.3f}'.format(np.mean(f_rmse), np.std(f_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fattahov.ao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n",
      "/home/fattahov.ao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/fattahov.ao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  after removing the cwd from sys.path.\n",
      "/home/fattahov.ao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"\n",
      "/home/fattahov.ao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "p1 = pd.read_csv('preds/1.csv')\n",
    "p2 = pd.read_csv('preds/2.csv').ix[:,1]\n",
    "p3 = pd.read_csv('preds/3.csv').ix[:,1]\n",
    "p4 = pd.read_csv('preds/4.csv').ix[:,1]\n",
    "p5 = pd.read_csv('preds/5.csv').ix[:,1]\n",
    "p6 = pd.read_csv('preds/6.csv').ix[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_test = pd.concat([p1, p2, p3, p4, p5, p6], axis=1)\n",
    "#preds.columns = ['p1', 'p2', 'p3', 'p4', 'p5', 'p6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = preds_test.id\n",
    "del preds_test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>res</th>\n",
       "      <th>res</th>\n",
       "      <th>res</th>\n",
       "      <th>res</th>\n",
       "      <th>res</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>0.042007</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.059628</td>\n",
       "      <td>0.010672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.873999</td>\n",
       "      <td>0.979739</td>\n",
       "      <td>0.987742</td>\n",
       "      <td>0.978227</td>\n",
       "      <td>0.999935</td>\n",
       "      <td>0.945789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.729980</td>\n",
       "      <td>0.755818</td>\n",
       "      <td>0.828963</td>\n",
       "      <td>0.822823</td>\n",
       "      <td>0.996606</td>\n",
       "      <td>0.527153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.620298</td>\n",
       "      <td>0.504142</td>\n",
       "      <td>0.742050</td>\n",
       "      <td>0.437726</td>\n",
       "      <td>0.710074</td>\n",
       "      <td>0.548276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999550</td>\n",
       "      <td>0.999407</td>\n",
       "      <td>0.981607</td>\n",
       "      <td>0.994913</td>\n",
       "      <td>0.999952</td>\n",
       "      <td>0.972804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.173062</td>\n",
       "      <td>0.314176</td>\n",
       "      <td>0.275407</td>\n",
       "      <td>0.386468</td>\n",
       "      <td>0.073598</td>\n",
       "      <td>0.323531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.912085</td>\n",
       "      <td>0.862033</td>\n",
       "      <td>0.976372</td>\n",
       "      <td>0.913806</td>\n",
       "      <td>0.999524</td>\n",
       "      <td>0.965701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999862</td>\n",
       "      <td>0.999959</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.986881</td>\n",
       "      <td>0.994741</td>\n",
       "      <td>0.987167</td>\n",
       "      <td>0.996171</td>\n",
       "      <td>0.999834</td>\n",
       "      <td>0.962648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.039438</td>\n",
       "      <td>0.012841</td>\n",
       "      <td>0.118860</td>\n",
       "      <td>0.102356</td>\n",
       "      <td>0.600754</td>\n",
       "      <td>0.185040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        res       res       res       res       res       res\n",
       "0  0.002582  0.003858  0.042007  0.004032  0.059628  0.010672\n",
       "1  0.873999  0.979739  0.987742  0.978227  0.999935  0.945789\n",
       "2  0.729980  0.755818  0.828963  0.822823  0.996606  0.527153\n",
       "3  0.620298  0.504142  0.742050  0.437726  0.710074  0.548276\n",
       "4  0.999550  0.999407  0.981607  0.994913  0.999952  0.972804\n",
       "5  0.173062  0.314176  0.275407  0.386468  0.073598  0.323531\n",
       "6  0.912085  0.862033  0.976372  0.913806  0.999524  0.965701\n",
       "7  0.999951  0.999995  0.999862  0.999959  0.999993  0.999803\n",
       "8  0.986881  0.994741  0.987167  0.996171  0.999834  0.962648\n",
       "9  0.039438  0.012841  0.118860  0.102356  0.600754  0.185040"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te = np.array(preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Усреднение \n",
    "res = est[0].predict_proba(te)\n",
    "\n",
    "for i in range(1,5):\n",
    "    res += est[i].predict_proba(te)\n",
    "\n",
    "res = res / 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(idx)\n",
    "tmp.columns = ['id']\n",
    "tmp['res'] = res[:,1]\n",
    "tmp.to_csv('preds/st_188.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>223</td>\n",
       "      <td>0.146623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1878</td>\n",
       "      <td>0.986377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>409</td>\n",
       "      <td>0.986877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1993</td>\n",
       "      <td>0.857029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>783</td>\n",
       "      <td>0.984741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>900</td>\n",
       "      <td>0.282334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2508</td>\n",
       "      <td>0.981243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1848</td>\n",
       "      <td>0.984503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1285</td>\n",
       "      <td>0.985135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2889</td>\n",
       "      <td>0.722239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2134</td>\n",
       "      <td>0.114739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>160</td>\n",
       "      <td>0.985989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>815</td>\n",
       "      <td>0.979611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.984513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>363</td>\n",
       "      <td>0.108854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1570</td>\n",
       "      <td>0.985780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1435</td>\n",
       "      <td>0.124993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>246</td>\n",
       "      <td>0.112241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1108</td>\n",
       "      <td>0.984652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1955</td>\n",
       "      <td>0.984747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2826</td>\n",
       "      <td>0.125578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1836</td>\n",
       "      <td>0.984211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>646</td>\n",
       "      <td>0.984555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2006</td>\n",
       "      <td>0.981983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>559</td>\n",
       "      <td>0.987528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>903</td>\n",
       "      <td>0.984429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2367</td>\n",
       "      <td>0.984491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1754</td>\n",
       "      <td>0.983071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1289</td>\n",
       "      <td>0.142616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2847</td>\n",
       "      <td>0.121845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>2178</td>\n",
       "      <td>0.389518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931</th>\n",
       "      <td>1966</td>\n",
       "      <td>0.119487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2932</th>\n",
       "      <td>1743</td>\n",
       "      <td>0.086223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>792</td>\n",
       "      <td>0.984502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2934</th>\n",
       "      <td>509</td>\n",
       "      <td>0.985401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>1793</td>\n",
       "      <td>0.944339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>1617</td>\n",
       "      <td>0.984234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>2771</td>\n",
       "      <td>0.984542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>1554</td>\n",
       "      <td>0.116276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>1755</td>\n",
       "      <td>0.148584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>666</td>\n",
       "      <td>0.117644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>248</td>\n",
       "      <td>0.114589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>2158</td>\n",
       "      <td>0.106131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>2950</td>\n",
       "      <td>0.993967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2944</th>\n",
       "      <td>1315</td>\n",
       "      <td>0.984531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>300</td>\n",
       "      <td>0.168054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>2211</td>\n",
       "      <td>0.108096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947</th>\n",
       "      <td>2903</td>\n",
       "      <td>0.984503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2948</th>\n",
       "      <td>28</td>\n",
       "      <td>0.111539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>1734</td>\n",
       "      <td>0.984494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2950</th>\n",
       "      <td>49</td>\n",
       "      <td>0.984359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951</th>\n",
       "      <td>1800</td>\n",
       "      <td>0.113477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>607</td>\n",
       "      <td>0.984495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2953</th>\n",
       "      <td>2004</td>\n",
       "      <td>0.984631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>1050</td>\n",
       "      <td>0.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>2191</td>\n",
       "      <td>0.161505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>475</td>\n",
       "      <td>0.984452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>1431</td>\n",
       "      <td>0.982944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>507</td>\n",
       "      <td>0.983167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>541</td>\n",
       "      <td>0.130750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2960 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       res\n",
       "0      223  0.146623\n",
       "1     1878  0.986377\n",
       "2      409  0.986877\n",
       "3     1993  0.857029\n",
       "4      783  0.984741\n",
       "5      900  0.282334\n",
       "6     2508  0.981243\n",
       "7     1848  0.984503\n",
       "8     1285  0.985135\n",
       "9     2889  0.722239\n",
       "10    2134  0.114739\n",
       "11     160  0.985989\n",
       "12     815  0.979611\n",
       "13       1  0.984513\n",
       "14     363  0.108854\n",
       "15    1570  0.985780\n",
       "16    1435  0.124993\n",
       "17     246  0.112241\n",
       "18    1108  0.984652\n",
       "19    1955  0.984747\n",
       "20    2826  0.125578\n",
       "21    1836  0.984211\n",
       "22     646  0.984555\n",
       "23    2006  0.981983\n",
       "24     559  0.987528\n",
       "25     903  0.984429\n",
       "26    2367  0.984491\n",
       "27    1754  0.983071\n",
       "28    1289  0.142616\n",
       "29    2847  0.121845\n",
       "...    ...       ...\n",
       "2930  2178  0.389518\n",
       "2931  1966  0.119487\n",
       "2932  1743  0.086223\n",
       "2933   792  0.984502\n",
       "2934   509  0.985401\n",
       "2935  1793  0.944339\n",
       "2936  1617  0.984234\n",
       "2937  2771  0.984542\n",
       "2938  1554  0.116276\n",
       "2939  1755  0.148584\n",
       "2940   666  0.117644\n",
       "2941   248  0.114589\n",
       "2942  2158  0.106131\n",
       "2943  2950  0.993967\n",
       "2944  1315  0.984531\n",
       "2945   300  0.168054\n",
       "2946  2211  0.108096\n",
       "2947  2903  0.984503\n",
       "2948    28  0.111539\n",
       "2949  1734  0.984494\n",
       "2950    49  0.984359\n",
       "2951  1800  0.113477\n",
       "2952   607  0.984495\n",
       "2953  2004  0.984631\n",
       "2954  1050  0.118600\n",
       "2955  2191  0.161505\n",
       "2956   475  0.984452\n",
       "2957  1431  0.982944\n",
       "2958   507  0.983167\n",
       "2959   541  0.130750\n",
       "\n",
       "[2960 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
