{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "K.set_session(tf.Session(config=config))\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "MAX_PARAGRAPH_LENGTH = 200\n",
    "MAX_QUESTION_LENGTH = 30\n",
    "\n",
    "X_train = pd.read_csv('../train_task_b.csv')\n",
    "\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, split=\" \", char_level=False)\n",
    "tokenizer.fit_on_texts(X_train.paragraph.tolist() + X_train.question.tolist())\n",
    "\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(X_train.paragraph.tolist())\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(X_train.question.tolist())\n",
    "train_sequences_3 = tokenizer.texts_to_sequences(X_train.answer.tolist())\n",
    "\n",
    "train_sequences_1 = pad_sequences(train_sequences_1, maxlen=MAX_PARAGRAPH_LENGTH)\n",
    "train_sequences_2 = pad_sequences(train_sequences_2, maxlen=MAX_QUESTION_LENGTH)\n",
    "#train_sequences_3 = pad_sequences(train_sequences_3, maxlen=MAX_QUESTION_LENGTH)\n",
    "\n",
    "\n",
    "def find(arr, sub_arr):\n",
    "    for i in range(len(arr) - len(sub_arr)):\n",
    "        flag = 1\n",
    "        for j in range(len(sub_arr)):\n",
    "            if arr[i + j] != sub_arr[j]:\n",
    "                flag = 0\n",
    "        if flag == 1:\n",
    "            return i, i + len(sub_arr)\n",
    "        \n",
    "        \n",
    "start_end = np.zeros([len(train_sequences_1),2])\n",
    "for i in range(len(train_sequences_1)):\n",
    "    start_end[i] = find(train_sequences_1[i], train_sequences_3[i])\n",
    "    \n",
    "    \n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#word2vec = KeyedVectors.load_word2vec_format('../ruwikiruscorpora_0_300_20.bin', binary=True)\n",
    "word2vec = KeyedVectors.load_word2vec_format('../news_0_300_2.bin', binary=True)\n",
    "#word2vec = KeyedVectors.load_word2vec_format('../ruscorpora_1_600_2.bin', binary=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139609/139609 [00:42<00:00, 3252.08it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_vect(p):\n",
    "    vect = []\n",
    "    for i in list(p.KNOWN_GRAMMEMES):\n",
    "        if i in p:\n",
    "            vect += [1]\n",
    "        else:\n",
    "            vect += [0]\n",
    "    return np.array(vect)\n",
    "\n",
    "from tqdm import tqdm\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "cnt = 0 \n",
    "for word, i in tqdm(word_index.items()):\n",
    "    p = morph.parse(word)[0]\n",
    "    vect = get_vect(p.tag)\n",
    "    word = p.normalized[0]\n",
    "    for part_sp in p.tag.PARTS_OF_SPEECH:\n",
    "        if word+'_'+part_sp in word2vec.vocab:\n",
    "            #embedding_matrix[i] = np.concatenate((word2vec.word_vec(word+'_'+part_sp), vect))\n",
    "            embedding_matrix[i] = word2vec.word_vec(word+'_'+part_sp)\n",
    "            cnt += 1\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer1 = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_PARAGRAPH_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "embedding_layer2 = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_QUESTION_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "embedding_layer3 = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=1,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sequences_1 = train_sequences_1[40000:]\n",
    "train_sequences_1 = train_sequences_1[:40000]\n",
    "\n",
    "val_sequences_2 = train_sequences_2[40000:]\n",
    "train_sequences_2 = train_sequences_2[:40000]\n",
    "\n",
    "val_sequences_3 = train_sequences_3[40000:]\n",
    "train_sequences_3 = train_sequences_3[:40000]\n",
    "\n",
    "start_end_val = start_end[40000:]\n",
    "start_end_train = start_end[:40000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.layers import add\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers.wrappers import Bidirectional \n",
    "from keras.layers import multiply, average\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "INPUT_DIM = 2\n",
    "TIME_STEPS = 20\n",
    "# if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "APPLY_ATTENTION_BEFORE_LSTM = False\n",
    " \n",
    "\n",
    "def attention_3d_block230(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, 230))(a)\n",
    "    a = Dense(230, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec230')(a)\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul230', mode='mul')\n",
    "    return output_attention_mul\n",
    "\n",
    "def attention_3d_block200(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, 200))(a)\n",
    "    a = Dense(200, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec200')(a)\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul200', mode='mul')\n",
    "    return output_attention_mul\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice(x):\n",
    "    return x[:, :200, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice1(x):\n",
    "    return x[:, 200:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice2(x):\n",
    "    return x[:, :20, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(x, i):\n",
    "    return x[:, i, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "from keras.layers import multiply, Multiply\n",
    "LSTM_DIM=90\n",
    "\n",
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(LSTM_DIM, self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):        \n",
    "        #return tf.multiply(x, self.kernel)\n",
    "        return x*self.kernel\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_sqrt(x):\n",
    "    return K.sqrt(x)\n",
    "\n",
    "def K_mul(x):\n",
    "    return x[0]*x[1]\n",
    "\n",
    "def K_cos(x):\n",
    "    return x[0] / (x[1] + x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import multiply, average,Multiply\n",
    "from keras.layers import dot, merge\n",
    "\n",
    "LSTM_DIM=90\n",
    "W_DIM = 70\n",
    "\n",
    "\n",
    "lstm_layer1_1 = LSTM(LSTM_DIM, dropout=0.5, recurrent_dropout=0.5,return_sequences=True, go_backwards=False)\n",
    "lstm_layer1_1_rev = LSTM(LSTM_DIM, dropout=0.5, recurrent_dropout=0.5,return_sequences=True, go_backwards=True)\n",
    "\n",
    "lstm_layer1_2 = Bidirectional(LSTM(200, dropout=0.5, recurrent_dropout=0.5,return_sequences=True))\n",
    "lstm_layer1_3 = Bidirectional(LSTM(200, dropout=0.5, recurrent_dropout=0.4,return_sequences=True))\n",
    "\n",
    "lstm_layer2_1 = LSTM(LSTM_DIM, dropout=0.5, recurrent_dropout=0.5,return_sequences=True, go_backwards=False)\n",
    "lstm_layer2_1_rev = LSTM(LSTM_DIM, dropout=0.5, recurrent_dropout=0.5,return_sequences=True, go_backwards=True)\n",
    "\n",
    "lstm_layer2_1_ = LSTM(LSTM_DIM, dropout=0.5, recurrent_dropout=0.5,return_sequences=False, go_backwards=False)\n",
    "lstm_layer2_1_rev_ = LSTM(LSTM_DIM, dropout=0.5, recurrent_dropout=0.5,return_sequences=False, go_backwards=True)\n",
    "#lstm_layer2_2 = LSTM(200, dropout=0.2, recurrent_dropout=0.2,return_sequences=False)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_PARAGRAPH_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer1(sequence_1_input)\n",
    "p = lstm_layer1_1(embedded_sequences_1)\n",
    "p_rev = lstm_layer1_1_rev(embedded_sequences_1)\n",
    "\n",
    "p1 = Reshape((200, LSTM_DIM, 1))(p)\n",
    "p1 = concatenate([p1]*W_DIM, axis=3)\n",
    "\n",
    "p2 = Reshape((200, LSTM_DIM, 1))(p_rev)\n",
    "p2 = concatenate([p2]*W_DIM, axis=3)\n",
    "\n",
    "p3 = Reshape((200, LSTM_DIM, 1))(p)\n",
    "p3 = concatenate([p3]*W_DIM, axis=3)\n",
    "p3 = Reshape((200, LSTM_DIM, W_DIM, 1))(p3)\n",
    "p3 = concatenate([p3]*30, axis=4)\n",
    "p3 = Reshape((200, 30, LSTM_DIM,W_DIM))(p3)\n",
    "\n",
    "p4 = Reshape((200, LSTM_DIM, 1))(p_rev)\n",
    "p4 = concatenate([p4]*W_DIM, axis=3)\n",
    "p4 = Reshape((200, LSTM_DIM, W_DIM, 1))(p4)\n",
    "p4 = concatenate([p4]*30, axis=4)\n",
    "p4 = Reshape((200, 30, LSTM_DIM,W_DIM))(p4)\n",
    "\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_QUESTION_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer2(sequence_2_input)\n",
    "q_ = lstm_layer2_1_(embedded_sequences_2)\n",
    "q_rev_ = lstm_layer2_1_rev_(embedded_sequences_2)\n",
    "q = lstm_layer2_1(embedded_sequences_2)\n",
    "q_rev = lstm_layer2_1_rev(embedded_sequences_2)\n",
    "\n",
    "q1 = Reshape((1, LSTM_DIM))(q_)\n",
    "q1 = concatenate([q1]*200, axis=1)\n",
    "q1 = Reshape((200, LSTM_DIM, 1))(q1)\n",
    "q1 = concatenate([q1]*W_DIM, axis=3)\n",
    "\n",
    "q2 = Reshape((1, LSTM_DIM))(q_rev_)\n",
    "q2 = concatenate([q2]*200, axis=1)\n",
    "q2 = Reshape((200, LSTM_DIM, 1))(q2)\n",
    "q2 = concatenate([q2]*W_DIM, axis=3)\n",
    "\n",
    "q3 = Reshape((30, LSTM_DIM))(q)\n",
    "q3 = concatenate([q3]*200, axis=1)\n",
    "q3 = Reshape((200, 30, LSTM_DIM, 1))(q3)\n",
    "q3 = concatenate([q3]*W_DIM, axis=4)\n",
    "\n",
    "q4 = Reshape((30, LSTM_DIM))(q_rev)\n",
    "q4 = concatenate([q4]*200, axis=1)\n",
    "q4 = Reshape((200, 30, LSTM_DIM, 1))(q4)\n",
    "q4 = concatenate([q4]*W_DIM, axis=4)\n",
    "\n",
    "#########################################\n",
    "W1 = MyLayer(W_DIM)\n",
    "p1 = W1(p1)\n",
    "q1 = W1(q1)\n",
    "\n",
    "m1 = Lambda(K_mul)([p1, q1])\n",
    "m1 = Lambda(lambda x: K.sum(x, axis=2), output_shape=(200, W_DIM))(m1)\n",
    "\n",
    "p1_norm = Lambda(K_mul)([p1,p1])\n",
    "p1_norm = Lambda(lambda x: K.sum(x, axis=2), output_shape=(200, W_DIM))(p1_norm)\n",
    "p1_norm = Lambda(K_sqrt)(p1_norm)\n",
    "\n",
    "q1_norm = Lambda(K_mul)([q1,q1])\n",
    "q1_norm = Lambda(lambda x: K.sum(x, axis=2), output_shape=(200, W_DIM))(q1_norm)\n",
    "q1_norm = Lambda(K_sqrt)(q1_norm)\n",
    "\n",
    "m1 = Lambda(K_cos)([m1, p1_norm, q1_norm])\n",
    "\n",
    "#########################################\n",
    "W2 = MyLayer(W_DIM)\n",
    "p2 = W2(p2)\n",
    "q2 = W2(q2)\n",
    "\n",
    "m2 = Lambda(K_mul)([p2, q2])\n",
    "m2 = Lambda(lambda x: K.sum(x, axis=2), output_shape=(200, W_DIM))(m2)\n",
    "\n",
    "p2_norm = Lambda(K_mul)([p2,p2])\n",
    "p2_norm = Lambda(lambda x: K.sum(x, axis=2), output_shape=(200, W_DIM))(p2_norm)\n",
    "p2_norm = Lambda(K_sqrt)(p2_norm)\n",
    "\n",
    "q2_norm = Lambda(K_mul)([q2,q2])\n",
    "q2_norm = Lambda(lambda x: K.sum(x, axis=2), output_shape=(200, W_DIM))(q2_norm)\n",
    "q2_norm = Lambda(K_sqrt)(q2_norm)\n",
    "\n",
    "m2 = Lambda(K_cos)([m2, p2_norm, q2_norm])\n",
    "'''\n",
    "#########################################\n",
    "\n",
    "W3 = MyLayer(W_DIM)\n",
    "p3 = W3(p3)\n",
    "q3 = W3(q3)\n",
    "\n",
    "m3 = Lambda(K_mul)([p3, q3])\n",
    "m3 = Lambda(lambda x: K.sum(x, axis=3), output_shape=(200, 30, W_DIM))(m3)\n",
    "\n",
    "p3_norm = Lambda(K_mul)([p3,p3])\n",
    "p3_norm = Lambda(lambda x: K.sum(x, axis=3), output_shape=(200, 30, W_DIM))(p3_norm)\n",
    "p3_norm = Lambda(K_sqrt)(p3_norm)\n",
    "\n",
    "q3_norm = Lambda(K_mul)([q3,q3])\n",
    "q3_norm = Lambda(lambda x: K.sum(x, axis=3), output_shape=(200, 30, W_DIM))(q3_norm)\n",
    "q3_norm = Lambda(K_sqrt)(q3_norm)\n",
    "\n",
    "m3 = Lambda(K_cos)([m3, p3_norm, q3_norm])\n",
    "m3 = Lambda(lambda x: K.max(x, axis=2), output_shape=(200, W_DIM))(m3)\n",
    "#########################################\n",
    "W4 = MyLayer(W_DIM)\n",
    "p4 = W4(p4)\n",
    "q4 = W4(q4)\n",
    "\n",
    "m4 = Lambda(K_mul)([p4, q4])\n",
    "m4 = Lambda(lambda x: K.sum(x, axis=3), output_shape=(200, 30, W_DIM))(m4)\n",
    "\n",
    "p4_norm = Lambda(K_mul)([p4,p4])\n",
    "p4_norm = Lambda(lambda x: K.sum(x, axis=3), output_shape=(200, 30, W_DIM))(p4_norm)\n",
    "p4_norm = Lambda(K_sqrt)(p4_norm)\n",
    "\n",
    "q4_norm = Lambda(K_mul)([q4,q4])\n",
    "q4_norm = Lambda(lambda x: K.sum(x, axis=3), output_shape=(200, 30, W_DIM))(q4_norm)\n",
    "q4_norm = Lambda(K_sqrt)(q4_norm)\n",
    "\n",
    "m4 = Lambda(K_cos)([m4, p4_norm, q4_norm])\n",
    "m4 = Lambda(lambda x: K.max(x, axis=2), output_shape=(200, W_DIM))(m4)\n",
    "#########################################\n",
    "\n",
    "W5 = MyLayer(W_DIM)\n",
    "p5 = W3(p3)\n",
    "q5 = W3(q3)\n",
    "\n",
    "m5 = Lambda(K_mul)([p5, q5])\n",
    "m5 = Lambda(lambda x: K.sum(x, axis=3), output_shape=(200, 30, W_DIM))(m5)\n",
    "\n",
    "p5_norm = Lambda(K_mul)([p5,p5])\n",
    "p5_norm = Lambda(lambda x: K.sum(x, axis=3), output_shape=(200, 30, W_DIM))(p5_norm)\n",
    "p5_norm = Lambda(K_sqrt)(p5_norm)\n",
    "\n",
    "q5_norm = Lambda(K_mul)([q5,q5])\n",
    "q5_norm = Lambda(lambda x: K.sum(x, axis=3), output_shape=(200, 30, W_DIM))(q5_norm)\n",
    "q5_norm = Lambda(K_sqrt)(q5_norm)\n",
    "\n",
    "m5 = Lambda(K_cos)([m5, p5_norm, q5_norm])\n",
    "m5 = Lambda(lambda x: K.mean(x, axis=2), output_shape=(200, W_DIM))(m5)\n",
    "#########################################\n",
    "W6 = MyLayer(W_DIM)\n",
    "p6 = W4(p4)\n",
    "q6 = W4(q4)\n",
    "\n",
    "m6 = Lambda(K_mul)([p6, q6])\n",
    "m6 = Lambda(lambda x: K.sum(x, axis=3), output_shape=(200, 30, W_DIM))(m6)\n",
    "\n",
    "p6_norm = Lambda(K_mul)([p6,p6])\n",
    "p6_norm = Lambda(lambda x: K.sum(x, axis=3), output_shape=(200, 30, W_DIM))(p6_norm)\n",
    "p6_norm = Lambda(K_sqrt)(p6_norm)\n",
    "\n",
    "q6_norm = Lambda(K_mul)([q6,q6])\n",
    "q6_norm = Lambda(lambda x: K.sum(x, axis=3), output_shape=(200, 30, W_DIM))(q6_norm)\n",
    "q6_norm = Lambda(K_sqrt)(q6_norm)\n",
    "\n",
    "m6 = Lambda(K_cos)([m6, p6_norm, q6_norm])\n",
    "m6 = Lambda(lambda x: K.mean(x, axis=2), output_shape=(200, W_DIM))(m6)\n",
    "#########################################\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "merged = concatenate([p,m1,m2],axis=2)\n",
    "merged = lstm_layer1_2(merged)\n",
    "merged = lstm_layer1_3(merged)\n",
    "#merged = attention_3d_block200(merged)\n",
    "merged = Dense(240, activation='relu')(merged)\n",
    "merged = Dropout(0.1)(merged)\n",
    "merged = Dense(120, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda_38/Max:0' shape=(?, 200, 70) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda_68/Sqrt:0' shape=(?, 200, 30, 70) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_46/concat:0' shape=(?, 200, 30, 90, 70) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda_60/Sqrt:0' shape=(?, 200, 70) dtype=float32>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "sgd = keras.optimizers.SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False)\n",
    "\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "        outputs=[preds])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer=adam,\n",
    "        metrics=['acc'])\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('IBM.h5', save_best_only=True, save_weights_only=True, monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('IBM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_generator(batch_size, tr1, tr2, st_end):\n",
    "    while(1):\n",
    "        idx = np.array(range(tr1.shape[0]))\n",
    "        np.random.shuffle(idx)\n",
    "        x_train1 = np.zeros([batch_size, MAX_PARAGRAPH_LENGTH])\n",
    "        x_train2 = np.zeros([batch_size, 30])\n",
    "        #x_train3 = np.zeros([batch_size, 30])\n",
    "\n",
    "        y_train = np.zeros([batch_size,200,1])\n",
    "\n",
    "        curr_batch_size = 0\n",
    "        for i in range(batch_size):\n",
    "            y = []\n",
    "            for j in range(200):\n",
    "                y += [j == st_end[idx[i]][0]]\n",
    "                #y += [[j == st_end[idx[i]][0], j == st_end[idx[i]][1]- 1]]\n",
    "                #y += [j >= st_end[idx[i]][0] and j < st_end[idx[i]][1]]\n",
    "            x_train1[i] = tr1[idx[i]]\n",
    "            x_train2[i] = tr2[idx[i]]\n",
    "            y_train[i] = np.array(y).reshape(200,1)\n",
    "\n",
    "        yield ([x_train1, x_train2], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afattahov/.local/lib/python3.5/site-packages/ipykernel_launcher.py:8: UserWarning: The semantics of the Keras 2 argument  `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Update your method calls accordingly.\n",
      "  \n",
      "/home/afattahov/.local/lib/python3.5/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., verbose=1, callbacks=[<keras.ca..., steps_per_epoch=100, validation_data=<generator..., epochs=40, validation_steps=100)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "100/100 [==============================] - 195s - loss: 0.2959 - acc: 0.9951 - val_loss: 0.2837 - val_acc: 0.9952\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - 191s - loss: 0.2723 - acc: 0.9954 - val_loss: 0.2616 - val_acc: 0.9952\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - 192s - loss: 0.2518 - acc: 0.9952 - val_loss: 0.2418 - val_acc: 0.9954\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - 200s - loss: 0.1758 - acc: 0.9952 - val_loss: 0.1701 - val_acc: 0.9951\n",
      "Epoch 9/40\n",
      " 69/100 [===================>..........] - ETA: 51s - loss: 0.1661 - acc: 0.9953"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "model.fit_generator(all_generator(batch_size, train_sequences_1,train_sequences_2,start_end_train),\n",
    "                    samples_per_epoch=100,\n",
    "                    validation_data=all_generator(batch_size,val_sequences_1,val_sequences_2,start_end_val),\n",
    "                    nb_val_samples=100,\n",
    "                    nb_epoch=40,\n",
    "                    verbose=1,\n",
    "                    callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('b2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([x_train1,x_train2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 200, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = len(val_sequences_1)\n",
    "batch = 20\n",
    "x_train1 = np.zeros([batch,MAX_PARAGRAPH_LENGTH])\n",
    "x_train2 = np.zeros([batch,30])\n",
    "st_end = np.zeros([batch,2])\n",
    "\n",
    "for i in range(batch):\n",
    "    x_train1[i] = val_sequences_1[i]\n",
    "    x_train2[i] = val_sequences_2[i]\n",
    "    st_end[i] = start_end_val[i]\n",
    "\n",
    "    \n",
    "    \n",
    "pred = model.predict([x_train1,x_train2])    \n",
    "    \n",
    "real_arr = []\n",
    "for i in range(batch):\n",
    "    real = []\n",
    "    for j in range(200):\n",
    "        real += [float(j >= st_end[i][0] and j < st_end[i][1]) / 5.0]\n",
    "        #real += [float(j == st_end[i][1]-1) / 5.0]\n",
    "\n",
    "    tmp = real\n",
    "    real_arr += [tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "for i in range(batch):\n",
    "    #plt.plot(range(200), pred2[i].reshape(200))\n",
    "    plt.plot(range(200), pred[i].reshape(200))\n",
    "    plt.plot(range(200), real_arr[i])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = pred > 0.09\n",
    "\n",
    "predict = []\n",
    "for i in range(len(val_sequences_1)):\n",
    "    predict += [val_sequences_1[i][mask[i]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3182,     5,  3324, 15426,     2, 25770,  3836,  3665, 33643,\n",
       "       38453,  1475,   962,   463,     3, 38478,  5317,  1336,  8667,\n",
       "           2, 13692], dtype=int32)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(prediction, ground_truth):\n",
    "    if len(prediction) == 0:\n",
    "        return 0\n",
    "    if len(ground_truth) == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * len(list(set(prediction) & set(ground_truth))) / len(prediction)\n",
    "    recall = 1.0 * len(list(set(prediction) & set(ground_truth))) / len(ground_truth)\n",
    "    if precision + recall != 0:\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46153846153846156"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(tmp, tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35714285714285715"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(predict[0], val_sequences_3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = []\n",
    "for i in range(len(predict)):\n",
    "    f1 += [f1_score(predict[i], val_sequences_3[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.094258449787767276"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(f1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
